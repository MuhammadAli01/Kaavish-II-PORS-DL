{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"train","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"ZhSsDYSf2L20","colab_type":"code","outputId":"25c0f9ec-d660-4c9f-ada3-5d5b23b82e82","executionInfo":{"status":"ok","timestamp":1589192274956,"user_tz":-300,"elapsed":1110,"user":{"displayName":"Muhammad Ali","photoUrl":"","userId":"15673831022739340207"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","os.chdir('/content/drive/My Drive/Deep Fashion Retrieval/deep-fashion-retrieval')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P-JtZQgArAF9","colab_type":"text"},"source":["# Training with 26 most relevant categories\n","\n"]},{"cell_type":"code","metadata":{"id":"uoTUVg6grWZ0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":236},"outputId":"e68b2245-67c8-4f72-d1e2-4e9b1aa60bf4"},"source":["# From scratch. Freeze=True. LR=0.05\n","! python train.py"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:211: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n","  \"please use transforms.Resize instead.\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:211: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n","  \"please use transforms.Resize instead.\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:698: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n","  \"please use transforms.RandomResizedCrop instead.\")\n","Loading model model_5_1500.pth.tar\n","Test() called at batch_idx: 0\n","/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jydikJ4fdXdf","colab_type":"code","outputId":"253d2624-ba17-4fef-8bea-4eda61707948","executionInfo":{"status":"ok","timestamp":1584862857505,"user_tz":-300,"elapsed":669079,"user":{"displayName":"Muhammad Ali","photoUrl":"","userId":"15673831022739340207"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# From scratch. Freeze=True. LR=0.03\n","! python train.py"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n","  \"please use transforms.Resize instead.\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:704: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n","  \"please use transforms.RandomResizedCrop instead.\")\n","Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n","100% 97.8M/97.8M [00:00<00:00, 148MB/s]\n","/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n","train.py:132: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(data, volatile=True), Variable(target)\n","\n","Test set: Average loss: 3.5452, Accuracy: 1/480 (0%)\n","\n","Train Epoch: 1 [0/110534 (0%)]\tAll Loss: 4.4143\tTriple Loss(1): 0.5338\tClassification Loss: 3.3468\n","Train Epoch: 1 [160/110534 (0%)]\tAll Loss: 4.8570\tTriple Loss(1): 0.9563\tClassification Loss: 2.9443\n","Train Epoch: 1 [320/110534 (0%)]\tAll Loss: 3.8361\tTriple Loss(1): 0.6547\tClassification Loss: 2.5268\n","Train Epoch: 1 [480/110534 (0%)]\tAll Loss: 3.2431\tTriple Loss(1): 0.3742\tClassification Loss: 2.4946\n","Train Epoch: 1 [640/110534 (1%)]\tAll Loss: 3.0285\tTriple Loss(1): 0.2627\tClassification Loss: 2.5032\n","Train Epoch: 1 [800/110534 (1%)]\tAll Loss: 3.6233\tTriple Loss(1): 0.4795\tClassification Loss: 2.6644\n","Train Epoch: 1 [960/110534 (1%)]\tAll Loss: 2.2063\tTriple Loss(0): 0.0000\tClassification Loss: 2.2063\n","Train Epoch: 1 [1120/110534 (1%)]\tAll Loss: 3.6590\tTriple Loss(1): 0.6184\tClassification Loss: 2.4222\n","Train Epoch: 1 [1280/110534 (1%)]\tAll Loss: 2.2368\tTriple Loss(0): 0.0000\tClassification Loss: 2.2368\n","Train Epoch: 1 [1440/110534 (1%)]\tAll Loss: 2.1369\tTriple Loss(0): 0.0000\tClassification Loss: 2.1369\n","\n","Test set: Average loss: 2.2800, Accuracy: 206/480 (43%)\n","\n","Train Epoch: 1 [1600/110534 (1%)]\tAll Loss: 2.3407\tTriple Loss(1): 0.1900\tClassification Loss: 1.9608\n","Train Epoch: 1 [1760/110534 (2%)]\tAll Loss: 2.4829\tTriple Loss(1): 0.3140\tClassification Loss: 1.8550\n","Train Epoch: 1 [1920/110534 (2%)]\tAll Loss: 2.7557\tTriple Loss(1): 0.3909\tClassification Loss: 1.9739\n","Train Epoch: 1 [2080/110534 (2%)]\tAll Loss: 3.0795\tTriple Loss(1): 0.2800\tClassification Loss: 2.5196\n","Train Epoch: 1 [2240/110534 (2%)]\tAll Loss: 3.8502\tTriple Loss(1): 0.7043\tClassification Loss: 2.4416\n","Train Epoch: 1 [2400/110534 (2%)]\tAll Loss: 2.6917\tTriple Loss(1): 0.2999\tClassification Loss: 2.0920\n","Train Epoch: 1 [2560/110534 (2%)]\tAll Loss: 2.8151\tTriple Loss(1): 0.4473\tClassification Loss: 1.9205\n","Train Epoch: 1 [2720/110534 (2%)]\tAll Loss: 2.3904\tTriple Loss(1): 0.3243\tClassification Loss: 1.7419\n","Train Epoch: 1 [2880/110534 (3%)]\tAll Loss: 2.3757\tTriple Loss(0): 0.0000\tClassification Loss: 2.3757\n","Train Epoch: 1 [3040/110534 (3%)]\tAll Loss: 2.3911\tTriple Loss(1): 0.2836\tClassification Loss: 1.8239\n","\n","Test set: Average loss: 2.0714, Accuracy: 225/480 (47%)\n","\n","Train Epoch: 1 [3200/110534 (3%)]\tAll Loss: 2.8504\tTriple Loss(1): 0.4525\tClassification Loss: 1.9454\n","Train Epoch: 1 [3360/110534 (3%)]\tAll Loss: 2.6078\tTriple Loss(1): 0.3052\tClassification Loss: 1.9974\n","Train Epoch: 1 [3520/110534 (3%)]\tAll Loss: 2.6495\tTriple Loss(1): 0.1701\tClassification Loss: 2.3092\n","Train Epoch: 1 [3680/110534 (3%)]\tAll Loss: 3.4935\tTriple Loss(1): 0.5687\tClassification Loss: 2.3560\n","Train Epoch: 1 [3840/110534 (3%)]\tAll Loss: 3.4968\tTriple Loss(1): 0.4765\tClassification Loss: 2.5438\n","Train Epoch: 1 [4000/110534 (4%)]\tAll Loss: 2.4679\tTriple Loss(1): 0.3846\tClassification Loss: 1.6987\n","Train Epoch: 1 [4160/110534 (4%)]\tAll Loss: 2.3861\tTriple Loss(1): 0.3178\tClassification Loss: 1.7506\n","Train Epoch: 1 [4320/110534 (4%)]\tAll Loss: 2.8354\tTriple Loss(1): 0.4780\tClassification Loss: 1.8795\n","Train Epoch: 1 [4480/110534 (4%)]\tAll Loss: 3.0071\tTriple Loss(1): 0.4195\tClassification Loss: 2.1682\n","Train Epoch: 1 [4640/110534 (4%)]\tAll Loss: 4.1486\tTriple Loss(1): 1.0343\tClassification Loss: 2.0800\n","\n","Test set: Average loss: 1.9922, Accuracy: 229/480 (48%)\n","\n","Train Epoch: 1 [4800/110534 (4%)]\tAll Loss: 2.5574\tTriple Loss(1): 0.1688\tClassification Loss: 2.2198\n","Train Epoch: 1 [4960/110534 (4%)]\tAll Loss: 2.3217\tTriple Loss(1): 0.3467\tClassification Loss: 1.6283\n","Train Epoch: 1 [5120/110534 (5%)]\tAll Loss: 2.9598\tTriple Loss(1): 0.3316\tClassification Loss: 2.2966\n","Train Epoch: 1 [5280/110534 (5%)]\tAll Loss: 4.6341\tTriple Loss(0): 1.5001\tClassification Loss: 1.6338\n","Train Epoch: 1 [5440/110534 (5%)]\tAll Loss: 2.5342\tTriple Loss(1): 0.2661\tClassification Loss: 2.0020\n","Train Epoch: 1 [5600/110534 (5%)]\tAll Loss: 2.3360\tTriple Loss(1): 0.4771\tClassification Loss: 1.3817\n","Train Epoch: 1 [5760/110534 (5%)]\tAll Loss: 1.6619\tTriple Loss(0): 0.0000\tClassification Loss: 1.6619\n","Train Epoch: 1 [5920/110534 (5%)]\tAll Loss: 2.4459\tTriple Loss(0): 0.2560\tClassification Loss: 1.9338\n","Train Epoch: 1 [6080/110534 (6%)]\tAll Loss: 2.4839\tTriple Loss(1): 0.4388\tClassification Loss: 1.6063\n","Train Epoch: 1 [6240/110534 (6%)]\tAll Loss: 1.8234\tTriple Loss(0): 0.0000\tClassification Loss: 1.8234\n","\n","Test set: Average loss: 1.9265, Accuracy: 239/480 (50%)\n","\n","Train Epoch: 1 [6400/110534 (6%)]\tAll Loss: 2.3763\tTriple Loss(1): 0.2911\tClassification Loss: 1.7940\n","Train Epoch: 1 [6560/110534 (6%)]\tAll Loss: 2.1931\tTriple Loss(1): 0.1881\tClassification Loss: 1.8168\n","Train Epoch: 1 [6720/110534 (6%)]\tAll Loss: 2.5772\tTriple Loss(0): 0.0000\tClassification Loss: 2.5772\n","Train Epoch: 1 [6880/110534 (6%)]\tAll Loss: 2.7580\tTriple Loss(1): 0.6271\tClassification Loss: 1.5039\n","Train Epoch: 1 [7040/110534 (6%)]\tAll Loss: 3.5511\tTriple Loss(0): 0.9675\tClassification Loss: 1.6161\n","Train Epoch: 1 [7200/110534 (7%)]\tAll Loss: 2.0347\tTriple Loss(0): 0.0000\tClassification Loss: 2.0347\n","Train Epoch: 1 [7360/110534 (7%)]\tAll Loss: 2.9461\tTriple Loss(1): 0.5774\tClassification Loss: 1.7912\n","Train Epoch: 1 [7520/110534 (7%)]\tAll Loss: 3.0092\tTriple Loss(1): 0.5100\tClassification Loss: 1.9893\n","Train Epoch: 1 [7680/110534 (7%)]\tAll Loss: 3.5980\tTriple Loss(1): 0.7644\tClassification Loss: 2.0692\n","Train Epoch: 1 [7840/110534 (7%)]\tAll Loss: 2.3077\tTriple Loss(1): 0.2990\tClassification Loss: 1.7097\n","\n","Test set: Average loss: 1.8852, Accuracy: 252/480 (52%)\n","\n","Train Epoch: 1 [8000/110534 (7%)]\tAll Loss: 1.9967\tTriple Loss(0): 0.0000\tClassification Loss: 1.9967\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_500.pth.tar\n","Train Epoch: 1 [8160/110534 (7%)]\tAll Loss: 3.0926\tTriple Loss(1): 0.4673\tClassification Loss: 2.1579\n","Train Epoch: 1 [8320/110534 (8%)]\tAll Loss: 2.6853\tTriple Loss(1): 0.3060\tClassification Loss: 2.0733\n","Train Epoch: 1 [8480/110534 (8%)]\tAll Loss: 2.2640\tTriple Loss(1): 0.2606\tClassification Loss: 1.7427\n","Train Epoch: 1 [8640/110534 (8%)]\tAll Loss: 2.0339\tTriple Loss(1): 0.2316\tClassification Loss: 1.5706\n","Train Epoch: 1 [8800/110534 (8%)]\tAll Loss: 1.9069\tTriple Loss(0): 0.0000\tClassification Loss: 1.9069\n","Train Epoch: 1 [8960/110534 (8%)]\tAll Loss: 2.6992\tTriple Loss(1): 0.3536\tClassification Loss: 1.9920\n","Train Epoch: 1 [9120/110534 (8%)]\tAll Loss: 2.4718\tTriple Loss(1): 0.2210\tClassification Loss: 2.0298\n","Train Epoch: 1 [9280/110534 (8%)]\tAll Loss: 2.5370\tTriple Loss(1): 0.2682\tClassification Loss: 2.0005\n","Train Epoch: 1 [9440/110534 (9%)]\tAll Loss: 2.2933\tTriple Loss(1): 0.1734\tClassification Loss: 1.9466\n","\n","Test set: Average loss: 1.8597, Accuracy: 228/480 (48%)\n","\n","Train Epoch: 1 [9600/110534 (9%)]\tAll Loss: 2.9287\tTriple Loss(1): 0.4999\tClassification Loss: 1.9288\n","Train Epoch: 1 [9760/110534 (9%)]\tAll Loss: 3.0796\tTriple Loss(1): 0.4296\tClassification Loss: 2.2205\n","Train Epoch: 1 [9920/110534 (9%)]\tAll Loss: 2.4734\tTriple Loss(1): 0.2608\tClassification Loss: 1.9517\n","Train Epoch: 1 [10080/110534 (9%)]\tAll Loss: 3.4928\tTriple Loss(1): 1.0061\tClassification Loss: 1.4806\n","Train Epoch: 1 [10240/110534 (9%)]\tAll Loss: 6.2052\tTriple Loss(0): 2.0305\tClassification Loss: 2.1442\n","Train Epoch: 1 [10400/110534 (9%)]\tAll Loss: 2.0504\tTriple Loss(0): 0.0000\tClassification Loss: 2.0504\n","Train Epoch: 1 [10560/110534 (10%)]\tAll Loss: 2.8594\tTriple Loss(1): 0.5061\tClassification Loss: 1.8472\n","Train Epoch: 1 [10720/110534 (10%)]\tAll Loss: 1.5039\tTriple Loss(0): 0.0000\tClassification Loss: 1.5039\n","Train Epoch: 1 [10880/110534 (10%)]\tAll Loss: 2.0504\tTriple Loss(1): 0.2160\tClassification Loss: 1.6184\n","Train Epoch: 1 [11040/110534 (10%)]\tAll Loss: 3.0014\tTriple Loss(1): 0.3453\tClassification Loss: 2.3109\n","\n","Test set: Average loss: 1.8295, Accuracy: 243/480 (51%)\n","\n","Train Epoch: 1 [11200/110534 (10%)]\tAll Loss: 2.9490\tTriple Loss(1): 0.3590\tClassification Loss: 2.2310\n","Train Epoch: 1 [11360/110534 (10%)]\tAll Loss: 2.1170\tTriple Loss(1): 0.1954\tClassification Loss: 1.7263\n","Train Epoch: 1 [11520/110534 (10%)]\tAll Loss: 2.8297\tTriple Loss(1): 0.2046\tClassification Loss: 2.4205\n","Train Epoch: 1 [11680/110534 (11%)]\tAll Loss: 1.8521\tTriple Loss(1): 0.1200\tClassification Loss: 1.6122\n","Train Epoch: 1 [11840/110534 (11%)]\tAll Loss: 3.6049\tTriple Loss(1): 0.3911\tClassification Loss: 2.8227\n","Train Epoch: 1 [12000/110534 (11%)]\tAll Loss: 1.9043\tTriple Loss(0): 0.0000\tClassification Loss: 1.9043\n","Train Epoch: 1 [12160/110534 (11%)]\tAll Loss: 1.6506\tTriple Loss(1): 0.1432\tClassification Loss: 1.3641\n","Train Epoch: 1 [12320/110534 (11%)]\tAll Loss: 2.2761\tTriple Loss(1): 0.2815\tClassification Loss: 1.7131\n","Train Epoch: 1 [12480/110534 (11%)]\tAll Loss: 5.0387\tTriple Loss(0): 1.7876\tClassification Loss: 1.4636\n","Train Epoch: 1 [12640/110534 (11%)]\tAll Loss: 2.5517\tTriple Loss(1): 0.3920\tClassification Loss: 1.7677\n","\n","Test set: Average loss: 1.8183, Accuracy: 245/480 (51%)\n","\n","Train Epoch: 1 [12800/110534 (12%)]\tAll Loss: 2.4414\tTriple Loss(1): 0.2819\tClassification Loss: 1.8775\n","Train Epoch: 1 [12960/110534 (12%)]\tAll Loss: 2.1514\tTriple Loss(1): 0.0485\tClassification Loss: 2.0543\n","Train Epoch: 1 [13120/110534 (12%)]\tAll Loss: 2.2425\tTriple Loss(1): 0.1573\tClassification Loss: 1.9279\n","Train Epoch: 1 [13280/110534 (12%)]\tAll Loss: 2.5091\tTriple Loss(1): 0.1249\tClassification Loss: 2.2593\n","Train Epoch: 1 [13440/110534 (12%)]\tAll Loss: 2.1102\tTriple Loss(1): 0.3051\tClassification Loss: 1.5000\n","Train Epoch: 1 [13600/110534 (12%)]\tAll Loss: 1.7166\tTriple Loss(1): 0.0955\tClassification Loss: 1.5256\n","Train Epoch: 1 [13760/110534 (12%)]\tAll Loss: 8.0505\tTriple Loss(0): 3.2184\tClassification Loss: 1.6137\n","Train Epoch: 1 [13920/110534 (13%)]\tAll Loss: 3.5639\tTriple Loss(1): 0.8611\tClassification Loss: 1.8417\n","Train Epoch: 1 [14080/110534 (13%)]\tAll Loss: 1.9234\tTriple Loss(1): 0.2552\tClassification Loss: 1.4129\n","Train Epoch: 1 [14240/110534 (13%)]\tAll Loss: 2.1892\tTriple Loss(1): 0.2002\tClassification Loss: 1.7888\n","\n","Test set: Average loss: 1.7885, Accuracy: 246/480 (51%)\n","\n","Train Epoch: 1 [14400/110534 (13%)]\tAll Loss: 2.0378\tTriple Loss(0): 0.0000\tClassification Loss: 2.0378\n","Train Epoch: 1 [14560/110534 (13%)]\tAll Loss: 1.0318\tTriple Loss(0): 0.0000\tClassification Loss: 1.0318\n","Train Epoch: 1 [14720/110534 (13%)]\tAll Loss: 2.0860\tTriple Loss(1): 0.0986\tClassification Loss: 1.8889\n","Train Epoch: 1 [14880/110534 (13%)]\tAll Loss: 2.9208\tTriple Loss(1): 0.4581\tClassification Loss: 2.0046\n","Train Epoch: 1 [15040/110534 (14%)]\tAll Loss: 2.2589\tTriple Loss(1): 0.3053\tClassification Loss: 1.6483\n","Train Epoch: 1 [15200/110534 (14%)]\tAll Loss: 2.7933\tTriple Loss(1): 0.2593\tClassification Loss: 2.2747\n","Train Epoch: 1 [15360/110534 (14%)]\tAll Loss: 2.4159\tTriple Loss(1): 0.3888\tClassification Loss: 1.6382\n","Train Epoch: 1 [15520/110534 (14%)]\tAll Loss: 2.1064\tTriple Loss(1): 0.0597\tClassification Loss: 1.9870\n","Train Epoch: 1 [15680/110534 (14%)]\tAll Loss: 2.8460\tTriple Loss(1): 0.3473\tClassification Loss: 2.1513\n","Train Epoch: 1 [15840/110534 (14%)]\tAll Loss: 2.1004\tTriple Loss(1): 0.2522\tClassification Loss: 1.5960\n","\n","Test set: Average loss: 1.7783, Accuracy: 242/480 (50%)\n","\n","Train Epoch: 1 [16000/110534 (14%)]\tAll Loss: 1.6371\tTriple Loss(0): 0.0000\tClassification Loss: 1.6371\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_1000.pth.tar\n","Train Epoch: 1 [16160/110534 (15%)]\tAll Loss: 1.9434\tTriple Loss(1): 0.2312\tClassification Loss: 1.4809\n","Train Epoch: 1 [16320/110534 (15%)]\tAll Loss: 1.5733\tTriple Loss(1): 0.2481\tClassification Loss: 1.0770\n","Train Epoch: 1 [16480/110534 (15%)]\tAll Loss: 2.3331\tTriple Loss(1): 0.2781\tClassification Loss: 1.7768\n","Train Epoch: 1 [16640/110534 (15%)]\tAll Loss: 2.7166\tTriple Loss(1): 0.3664\tClassification Loss: 1.9838\n","Train Epoch: 1 [16800/110534 (15%)]\tAll Loss: 2.5864\tTriple Loss(1): 0.4622\tClassification Loss: 1.6621\n","Train Epoch: 1 [16960/110534 (15%)]\tAll Loss: 2.4156\tTriple Loss(1): 0.0963\tClassification Loss: 2.2229\n","Train Epoch: 1 [17120/110534 (15%)]\tAll Loss: 1.8829\tTriple Loss(1): 0.1674\tClassification Loss: 1.5480\n","Train Epoch: 1 [17280/110534 (16%)]\tAll Loss: 2.5703\tTriple Loss(1): 0.3205\tClassification Loss: 1.9293\n","Train Epoch: 1 [17440/110534 (16%)]\tAll Loss: 3.1227\tTriple Loss(1): 0.7767\tClassification Loss: 1.5692\n","\n","Test set: Average loss: 1.7602, Accuracy: 245/480 (51%)\n","\n","Train Epoch: 1 [17600/110534 (16%)]\tAll Loss: 2.0807\tTriple Loss(1): 0.3185\tClassification Loss: 1.4438\n","Train Epoch: 1 [17760/110534 (16%)]\tAll Loss: 3.0359\tTriple Loss(1): 0.5214\tClassification Loss: 1.9931\n","Train Epoch: 1 [17920/110534 (16%)]\tAll Loss: 2.3871\tTriple Loss(1): 0.1580\tClassification Loss: 2.0710\n","Train Epoch: 1 [18080/110534 (16%)]\tAll Loss: 1.8947\tTriple Loss(1): 0.2218\tClassification Loss: 1.4511\n","Train Epoch: 1 [18240/110534 (17%)]\tAll Loss: 2.1521\tTriple Loss(1): 0.1642\tClassification Loss: 1.8237\n","Train Epoch: 1 [18400/110534 (17%)]\tAll Loss: 2.2392\tTriple Loss(1): 0.2023\tClassification Loss: 1.8346\n","Train Epoch: 1 [18560/110534 (17%)]\tAll Loss: 3.1183\tTriple Loss(1): 0.3890\tClassification Loss: 2.3403\n","Train Epoch: 1 [18720/110534 (17%)]\tAll Loss: 3.0667\tTriple Loss(1): 0.4273\tClassification Loss: 2.2120\n","Train Epoch: 1 [18880/110534 (17%)]\tAll Loss: 1.8367\tTriple Loss(1): 0.2237\tClassification Loss: 1.3893\n","Train Epoch: 1 [19040/110534 (17%)]\tAll Loss: 1.0950\tTriple Loss(0): 0.0000\tClassification Loss: 1.0950\n","\n","Test set: Average loss: 1.7434, Accuracy: 248/480 (52%)\n","\n","Train Epoch: 1 [19200/110534 (17%)]\tAll Loss: 2.1484\tTriple Loss(0): 0.0000\tClassification Loss: 2.1484\n","Train Epoch: 1 [19360/110534 (18%)]\tAll Loss: 2.6654\tTriple Loss(1): 0.4447\tClassification Loss: 1.7760\n","Train Epoch: 1 [19520/110534 (18%)]\tAll Loss: 2.2299\tTriple Loss(1): 0.3237\tClassification Loss: 1.5824\n","Train Epoch: 1 [19680/110534 (18%)]\tAll Loss: 2.5088\tTriple Loss(1): 0.6103\tClassification Loss: 1.2883\n","Train Epoch: 1 [19840/110534 (18%)]\tAll Loss: 3.3303\tTriple Loss(1): 0.6626\tClassification Loss: 2.0051\n","Train Epoch: 1 [20000/110534 (18%)]\tAll Loss: 3.6643\tTriple Loss(0): 0.9478\tClassification Loss: 1.7687\n","Train Epoch: 1 [20160/110534 (18%)]\tAll Loss: 1.8298\tTriple Loss(1): 0.0287\tClassification Loss: 1.7725\n","Train Epoch: 1 [20320/110534 (18%)]\tAll Loss: 3.0570\tTriple Loss(1): 0.4036\tClassification Loss: 2.2498\n","Train Epoch: 1 [20480/110534 (19%)]\tAll Loss: 2.4646\tTriple Loss(1): 0.3680\tClassification Loss: 1.7286\n","Train Epoch: 1 [20640/110534 (19%)]\tAll Loss: 1.7394\tTriple Loss(1): 0.1270\tClassification Loss: 1.4853\n","\n","Test set: Average loss: 1.7373, Accuracy: 239/480 (50%)\n","\n","Train Epoch: 1 [20800/110534 (19%)]\tAll Loss: 2.7553\tTriple Loss(1): 0.4483\tClassification Loss: 1.8586\n","Train Epoch: 1 [20960/110534 (19%)]\tAll Loss: 1.8203\tTriple Loss(1): 0.1722\tClassification Loss: 1.4759\n","Train Epoch: 1 [21120/110534 (19%)]\tAll Loss: 2.1848\tTriple Loss(1): 0.4604\tClassification Loss: 1.2641\n","Train Epoch: 1 [21280/110534 (19%)]\tAll Loss: 1.8239\tTriple Loss(0): 0.0000\tClassification Loss: 1.8239\n","Train Epoch: 1 [21440/110534 (19%)]\tAll Loss: 1.7775\tTriple Loss(1): 0.2220\tClassification Loss: 1.3335\n","Train Epoch: 1 [21600/110534 (20%)]\tAll Loss: 2.8766\tTriple Loss(1): 0.2649\tClassification Loss: 2.3468\n","Train Epoch: 1 [21760/110534 (20%)]\tAll Loss: 2.3836\tTriple Loss(1): 0.5205\tClassification Loss: 1.3427\n","Train Epoch: 1 [21920/110534 (20%)]\tAll Loss: 1.7578\tTriple Loss(1): 0.3559\tClassification Loss: 1.0459\n","Train Epoch: 1 [22080/110534 (20%)]\tAll Loss: 2.1123\tTriple Loss(1): 0.1146\tClassification Loss: 1.8831\n","Train Epoch: 1 [22240/110534 (20%)]\tAll Loss: 1.7298\tTriple Loss(0): 0.0000\tClassification Loss: 1.7298\n","\n","Test set: Average loss: 1.7323, Accuracy: 264/480 (55%)\n","\n","Train Epoch: 1 [22400/110534 (20%)]\tAll Loss: 8.0036\tTriple Loss(0): 3.1385\tClassification Loss: 1.7266\n","Train Epoch: 1 [22560/110534 (20%)]\tAll Loss: 2.8988\tTriple Loss(1): 0.4594\tClassification Loss: 1.9800\n","Train Epoch: 1 [22720/110534 (21%)]\tAll Loss: 2.1298\tTriple Loss(1): 0.0789\tClassification Loss: 1.9721\n","Train Epoch: 1 [22880/110534 (21%)]\tAll Loss: 2.0278\tTriple Loss(1): 0.3648\tClassification Loss: 1.2982\n","Train Epoch: 1 [23040/110534 (21%)]\tAll Loss: 2.2615\tTriple Loss(1): 0.4366\tClassification Loss: 1.3884\n","Train Epoch: 1 [23200/110534 (21%)]\tAll Loss: 3.1041\tTriple Loss(1): 0.5182\tClassification Loss: 2.0678\n","Train Epoch: 1 [23360/110534 (21%)]\tAll Loss: 1.6819\tTriple Loss(1): 0.1326\tClassification Loss: 1.4167\n","Train Epoch: 1 [23520/110534 (21%)]\tAll Loss: 2.1603\tTriple Loss(0): 0.0000\tClassification Loss: 2.1603\n","Train Epoch: 1 [23680/110534 (21%)]\tAll Loss: 2.3423\tTriple Loss(1): 0.1152\tClassification Loss: 2.1119\n","Train Epoch: 1 [23840/110534 (22%)]\tAll Loss: 1.9530\tTriple Loss(1): 0.2547\tClassification Loss: 1.4435\n","\n","Test set: Average loss: 1.7395, Accuracy: 251/480 (52%)\n","\n","Train Epoch: 1 [24000/110534 (22%)]\tAll Loss: 1.3366\tTriple Loss(0): 0.0000\tClassification Loss: 1.3366\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_1500.pth.tar\n","Train Epoch: 1 [24160/110534 (22%)]\tAll Loss: 1.8430\tTriple Loss(1): 0.3033\tClassification Loss: 1.2365\n","Train Epoch: 1 [24320/110534 (22%)]\tAll Loss: 2.3736\tTriple Loss(1): 0.4286\tClassification Loss: 1.5164\n","Train Epoch: 1 [24480/110534 (22%)]\tAll Loss: 1.8925\tTriple Loss(0): 0.0000\tClassification Loss: 1.8925\n","Train Epoch: 1 [24640/110534 (22%)]\tAll Loss: 2.8441\tTriple Loss(1): 0.5133\tClassification Loss: 1.8175\n","Train Epoch: 1 [24800/110534 (22%)]\tAll Loss: 1.8592\tTriple Loss(1): 0.0860\tClassification Loss: 1.6871\n","Train Epoch: 1 [24960/110534 (23%)]\tAll Loss: 2.1841\tTriple Loss(1): 0.1015\tClassification Loss: 1.9811\n","Train Epoch: 1 [25120/110534 (23%)]\tAll Loss: 1.2537\tTriple Loss(0): 0.0000\tClassification Loss: 1.2537\n","Train Epoch: 1 [25280/110534 (23%)]\tAll Loss: 1.8744\tTriple Loss(1): 0.0427\tClassification Loss: 1.7889\n","Train Epoch: 1 [25440/110534 (23%)]\tAll Loss: 2.2895\tTriple Loss(1): 0.3152\tClassification Loss: 1.6590\n","\n","Test set: Average loss: 1.7374, Accuracy: 257/480 (54%)\n","\n","Train Epoch: 1 [25600/110534 (23%)]\tAll Loss: 2.6009\tTriple Loss(1): 0.4634\tClassification Loss: 1.6742\n","Train Epoch: 1 [25760/110534 (23%)]\tAll Loss: 2.8563\tTriple Loss(1): 0.6885\tClassification Loss: 1.4794\n","Train Epoch: 1 [25920/110534 (23%)]\tAll Loss: 1.3957\tTriple Loss(1): 0.1384\tClassification Loss: 1.1188\n","Train Epoch: 1 [26080/110534 (24%)]\tAll Loss: 1.4907\tTriple Loss(1): 0.2092\tClassification Loss: 1.0722\n","Train Epoch: 1 [26240/110534 (24%)]\tAll Loss: 2.1105\tTriple Loss(1): 0.4151\tClassification Loss: 1.2802\n","Train Epoch: 1 [26400/110534 (24%)]\tAll Loss: 2.7323\tTriple Loss(1): 0.4274\tClassification Loss: 1.8775\n","Train Epoch: 1 [26560/110534 (24%)]\tAll Loss: 2.7351\tTriple Loss(1): 0.2566\tClassification Loss: 2.2218\n","Train Epoch: 1 [26720/110534 (24%)]\tAll Loss: 2.9102\tTriple Loss(1): 0.3529\tClassification Loss: 2.2045\n","Train Epoch: 1 [26880/110534 (24%)]\tAll Loss: 2.0216\tTriple Loss(1): 0.1385\tClassification Loss: 1.7446\n","Train Epoch: 1 [27040/110534 (24%)]\tAll Loss: 2.0369\tTriple Loss(1): 0.3247\tClassification Loss: 1.3875\n","\n","Test set: Average loss: 1.7470, Accuracy: 251/480 (52%)\n","\n","Train Epoch: 1 [27200/110534 (25%)]\tAll Loss: 0.9674\tTriple Loss(0): 0.0000\tClassification Loss: 0.9674\n","Train Epoch: 1 [27360/110534 (25%)]\tAll Loss: 2.3373\tTriple Loss(1): 0.4883\tClassification Loss: 1.3606\n","Train Epoch: 1 [27520/110534 (25%)]\tAll Loss: 1.2385\tTriple Loss(0): 0.0000\tClassification Loss: 1.2385\n","Train Epoch: 1 [27680/110534 (25%)]\tAll Loss: 2.3396\tTriple Loss(1): 0.3618\tClassification Loss: 1.6160\n","Train Epoch: 1 [27840/110534 (25%)]\tAll Loss: 2.1223\tTriple Loss(1): 0.3141\tClassification Loss: 1.4941\n","Train Epoch: 1 [28000/110534 (25%)]\tAll Loss: 1.9073\tTriple Loss(1): 0.1975\tClassification Loss: 1.5122\n","Train Epoch: 1 [28160/110534 (25%)]\tAll Loss: 1.6015\tTriple Loss(0): 0.0000\tClassification Loss: 1.6015\n","Train Epoch: 1 [28320/110534 (26%)]\tAll Loss: 2.0224\tTriple Loss(1): 0.1162\tClassification Loss: 1.7900\n","Train Epoch: 1 [28480/110534 (26%)]\tAll Loss: 2.5083\tTriple Loss(1): 0.4921\tClassification Loss: 1.5242\n","Train Epoch: 1 [28640/110534 (26%)]\tAll Loss: 2.3169\tTriple Loss(1): 0.2816\tClassification Loss: 1.7536\n","\n","Test set: Average loss: 1.7344, Accuracy: 258/480 (54%)\n","\n","Train Epoch: 1 [28800/110534 (26%)]\tAll Loss: 1.8546\tTriple Loss(1): 0.2070\tClassification Loss: 1.4406\n","Train Epoch: 1 [28960/110534 (26%)]\tAll Loss: 1.8001\tTriple Loss(1): 0.0989\tClassification Loss: 1.6023\n","Train Epoch: 1 [29120/110534 (26%)]\tAll Loss: 2.7016\tTriple Loss(1): 0.4069\tClassification Loss: 1.8878\n","Train Epoch: 1 [29280/110534 (26%)]\tAll Loss: 2.4843\tTriple Loss(1): 0.1820\tClassification Loss: 2.1203\n","Train Epoch: 1 [29440/110534 (27%)]\tAll Loss: 1.8073\tTriple Loss(1): 0.0000\tClassification Loss: 1.8073\n","Train Epoch: 1 [29600/110534 (27%)]\tAll Loss: 2.0742\tTriple Loss(1): 0.3113\tClassification Loss: 1.4516\n","Train Epoch: 1 [29760/110534 (27%)]\tAll Loss: 1.8755\tTriple Loss(0): 0.0000\tClassification Loss: 1.8755\n","Train Epoch: 1 [29920/110534 (27%)]\tAll Loss: 3.3407\tTriple Loss(1): 0.6700\tClassification Loss: 2.0008\n","Train Epoch: 1 [30080/110534 (27%)]\tAll Loss: 1.9335\tTriple Loss(1): 0.0527\tClassification Loss: 1.8282\n","Train Epoch: 1 [30240/110534 (27%)]\tAll Loss: 2.1114\tTriple Loss(1): 0.2574\tClassification Loss: 1.5965\n","\n","Test set: Average loss: 1.7287, Accuracy: 256/480 (53%)\n","\n","Train Epoch: 1 [30400/110534 (28%)]\tAll Loss: 2.7049\tTriple Loss(1): 0.5395\tClassification Loss: 1.6259\n","Train Epoch: 1 [30560/110534 (28%)]\tAll Loss: 2.0938\tTriple Loss(1): 0.2513\tClassification Loss: 1.5913\n","Train Epoch: 1 [30720/110534 (28%)]\tAll Loss: 1.4494\tTriple Loss(1): 0.0843\tClassification Loss: 1.2808\n","Train Epoch: 1 [30880/110534 (28%)]\tAll Loss: 2.2945\tTriple Loss(1): 0.2136\tClassification Loss: 1.8673\n","Train Epoch: 1 [31040/110534 (28%)]\tAll Loss: 2.4796\tTriple Loss(1): 0.1866\tClassification Loss: 2.1064\n","Train Epoch: 1 [31200/110534 (28%)]\tAll Loss: 2.4242\tTriple Loss(1): 0.4059\tClassification Loss: 1.6124\n","Train Epoch: 1 [31360/110534 (28%)]\tAll Loss: 1.5287\tTriple Loss(0): 0.0000\tClassification Loss: 1.5287\n","Train Epoch: 1 [31520/110534 (29%)]\tAll Loss: 2.6023\tTriple Loss(1): 0.4070\tClassification Loss: 1.7883\n","Train Epoch: 1 [31680/110534 (29%)]\tAll Loss: 1.3797\tTriple Loss(1): 0.0000\tClassification Loss: 1.3797\n","Train Epoch: 1 [31840/110534 (29%)]\tAll Loss: 2.7918\tTriple Loss(1): 0.4757\tClassification Loss: 1.8403\n","\n","Test set: Average loss: 1.7016, Accuracy: 254/480 (53%)\n","\n","Train Epoch: 1 [32000/110534 (29%)]\tAll Loss: 2.8496\tTriple Loss(1): 0.6106\tClassification Loss: 1.6285\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_2000.pth.tar\n","Train Epoch: 1 [32160/110534 (29%)]\tAll Loss: 1.7050\tTriple Loss(0): 0.0000\tClassification Loss: 1.7050\n","Train Epoch: 1 [32320/110534 (29%)]\tAll Loss: 2.2530\tTriple Loss(1): 0.1865\tClassification Loss: 1.8801\n","Train Epoch: 1 [32480/110534 (29%)]\tAll Loss: 2.7750\tTriple Loss(1): 0.4948\tClassification Loss: 1.7854\n","Train Epoch: 1 [32640/110534 (30%)]\tAll Loss: 1.8560\tTriple Loss(1): 0.1776\tClassification Loss: 1.5008\n","Train Epoch: 1 [32800/110534 (30%)]\tAll Loss: 2.1720\tTriple Loss(1): 0.2456\tClassification Loss: 1.6807\n","Train Epoch: 1 [32960/110534 (30%)]\tAll Loss: 1.7658\tTriple Loss(1): 0.1871\tClassification Loss: 1.3916\n","Train Epoch: 1 [33120/110534 (30%)]\tAll Loss: 1.6251\tTriple Loss(1): 0.2502\tClassification Loss: 1.1247\n","Train Epoch: 1 [33280/110534 (30%)]\tAll Loss: 1.9298\tTriple Loss(1): 0.3302\tClassification Loss: 1.2694\n","Train Epoch: 1 [33440/110534 (30%)]\tAll Loss: 1.7600\tTriple Loss(0): 0.0000\tClassification Loss: 1.7600\n","\n","Test set: Average loss: 1.6812, Accuracy: 260/480 (54%)\n","\n","Train Epoch: 1 [33600/110534 (30%)]\tAll Loss: 2.1115\tTriple Loss(1): 0.1022\tClassification Loss: 1.9072\n","Train Epoch: 1 [33760/110534 (31%)]\tAll Loss: 1.6384\tTriple Loss(1): 0.1103\tClassification Loss: 1.4179\n","Train Epoch: 1 [33920/110534 (31%)]\tAll Loss: 2.3777\tTriple Loss(1): 0.3523\tClassification Loss: 1.6731\n","Train Epoch: 1 [34080/110534 (31%)]\tAll Loss: 2.1798\tTriple Loss(0): 0.2187\tClassification Loss: 1.7424\n","Train Epoch: 1 [34240/110534 (31%)]\tAll Loss: 1.9237\tTriple Loss(1): 0.0864\tClassification Loss: 1.7509\n","Train Epoch: 1 [34400/110534 (31%)]\tAll Loss: 1.9648\tTriple Loss(1): 0.4296\tClassification Loss: 1.1056\n","Train Epoch: 1 [34560/110534 (31%)]\tAll Loss: 1.8077\tTriple Loss(1): 0.0875\tClassification Loss: 1.6326\n","Train Epoch: 1 [34720/110534 (31%)]\tAll Loss: 3.0203\tTriple Loss(0): 0.4860\tClassification Loss: 2.0483\n","Train Epoch: 1 [34880/110534 (32%)]\tAll Loss: 1.6015\tTriple Loss(1): 0.1746\tClassification Loss: 1.2523\n","Train Epoch: 1 [35040/110534 (32%)]\tAll Loss: 1.7310\tTriple Loss(1): 0.0656\tClassification Loss: 1.5998\n","\n","Test set: Average loss: 1.6872, Accuracy: 258/480 (54%)\n","\n","Train Epoch: 1 [35200/110534 (32%)]\tAll Loss: 2.1691\tTriple Loss(1): 0.1048\tClassification Loss: 1.9595\n","Train Epoch: 1 [35360/110534 (32%)]\tAll Loss: 1.7157\tTriple Loss(1): 0.2708\tClassification Loss: 1.1740\n","Train Epoch: 1 [35520/110534 (32%)]\tAll Loss: 2.3784\tTriple Loss(1): 0.2022\tClassification Loss: 1.9740\n","Train Epoch: 1 [35680/110534 (32%)]\tAll Loss: 2.1333\tTriple Loss(1): 0.1404\tClassification Loss: 1.8526\n","Train Epoch: 1 [35840/110534 (32%)]\tAll Loss: 1.9367\tTriple Loss(0): 0.0000\tClassification Loss: 1.9367\n","Train Epoch: 1 [36000/110534 (33%)]\tAll Loss: 2.9708\tTriple Loss(1): 0.2544\tClassification Loss: 2.4620\n","Train Epoch: 1 [36160/110534 (33%)]\tAll Loss: 2.2738\tTriple Loss(1): 0.1293\tClassification Loss: 2.0152\n","Train Epoch: 1 [36320/110534 (33%)]\tAll Loss: 2.1244\tTriple Loss(1): 0.2160\tClassification Loss: 1.6924\n","Train Epoch: 1 [36480/110534 (33%)]\tAll Loss: 1.8173\tTriple Loss(1): 0.1794\tClassification Loss: 1.4585\n","Train Epoch: 1 [36640/110534 (33%)]\tAll Loss: 2.4315\tTriple Loss(1): 0.2460\tClassification Loss: 1.9395\n","\n","Test set: Average loss: 1.6844, Accuracy: 258/480 (54%)\n","\n","Train Epoch: 1 [36800/110534 (33%)]\tAll Loss: 2.3965\tTriple Loss(1): 0.4675\tClassification Loss: 1.4614\n","Train Epoch: 1 [36960/110534 (33%)]\tAll Loss: 2.1950\tTriple Loss(1): 0.2446\tClassification Loss: 1.7058\n","Train Epoch: 1 [37120/110534 (34%)]\tAll Loss: 2.7443\tTriple Loss(1): 0.4009\tClassification Loss: 1.9425\n","Train Epoch: 1 [37280/110534 (34%)]\tAll Loss: 2.8279\tTriple Loss(1): 0.4127\tClassification Loss: 2.0024\n","Train Epoch: 1 [37440/110534 (34%)]\tAll Loss: 2.6237\tTriple Loss(1): 0.4364\tClassification Loss: 1.7510\n","Train Epoch: 1 [37600/110534 (34%)]\tAll Loss: 1.8871\tTriple Loss(1): 0.3736\tClassification Loss: 1.1399\n","Train Epoch: 1 [37760/110534 (34%)]\tAll Loss: 2.7415\tTriple Loss(1): 0.2519\tClassification Loss: 2.2377\n","Train Epoch: 1 [37920/110534 (34%)]\tAll Loss: 2.3877\tTriple Loss(1): 0.3520\tClassification Loss: 1.6837\n","Train Epoch: 1 [38080/110534 (34%)]\tAll Loss: 1.7944\tTriple Loss(0): 0.0000\tClassification Loss: 1.7944\n","Train Epoch: 1 [38240/110534 (35%)]\tAll Loss: 2.1240\tTriple Loss(1): 0.3511\tClassification Loss: 1.4218\n","\n","Test set: Average loss: 1.6652, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 1 [38400/110534 (35%)]\tAll Loss: 1.5204\tTriple Loss(1): 0.0600\tClassification Loss: 1.4004\n","Train Epoch: 1 [38560/110534 (35%)]\tAll Loss: 1.9964\tTriple Loss(1): 0.2744\tClassification Loss: 1.4476\n","Train Epoch: 1 [38720/110534 (35%)]\tAll Loss: 2.6177\tTriple Loss(1): 0.2678\tClassification Loss: 2.0820\n","Train Epoch: 1 [38880/110534 (35%)]\tAll Loss: 2.1037\tTriple Loss(1): 0.3539\tClassification Loss: 1.3958\n","Train Epoch: 1 [39040/110534 (35%)]\tAll Loss: 2.3214\tTriple Loss(1): 0.3418\tClassification Loss: 1.6378\n","Train Epoch: 1 [39200/110534 (35%)]\tAll Loss: 2.7963\tTriple Loss(1): 0.1761\tClassification Loss: 2.4441\n","Train Epoch: 1 [39360/110534 (36%)]\tAll Loss: 1.8231\tTriple Loss(0): 0.0000\tClassification Loss: 1.8231\n","Train Epoch: 1 [39520/110534 (36%)]\tAll Loss: 3.1425\tTriple Loss(1): 0.4884\tClassification Loss: 2.1656\n","Train Epoch: 1 [39680/110534 (36%)]\tAll Loss: 1.3206\tTriple Loss(0): 0.0000\tClassification Loss: 1.3206\n","Train Epoch: 1 [39840/110534 (36%)]\tAll Loss: 1.0592\tTriple Loss(0): 0.0000\tClassification Loss: 1.0592\n","\n","Test set: Average loss: 1.6835, Accuracy: 256/480 (53%)\n","\n","Train Epoch: 1 [40000/110534 (36%)]\tAll Loss: 1.9001\tTriple Loss(1): 0.1591\tClassification Loss: 1.5819\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_2500.pth.tar\n","Train Epoch: 1 [40160/110534 (36%)]\tAll Loss: 3.9110\tTriple Loss(0): 1.3810\tClassification Loss: 1.1490\n","Train Epoch: 1 [40320/110534 (36%)]\tAll Loss: 2.6015\tTriple Loss(1): 0.6803\tClassification Loss: 1.2410\n","Train Epoch: 1 [40480/110534 (37%)]\tAll Loss: 2.2713\tTriple Loss(1): 0.2127\tClassification Loss: 1.8459\n","Train Epoch: 1 [40640/110534 (37%)]\tAll Loss: 2.1313\tTriple Loss(1): 0.2875\tClassification Loss: 1.5562\n","Train Epoch: 1 [40800/110534 (37%)]\tAll Loss: 2.7172\tTriple Loss(1): 0.4240\tClassification Loss: 1.8691\n","Train Epoch: 1 [40960/110534 (37%)]\tAll Loss: 2.0453\tTriple Loss(1): 0.1075\tClassification Loss: 1.8304\n","Train Epoch: 1 [41120/110534 (37%)]\tAll Loss: 1.8145\tTriple Loss(1): 0.0701\tClassification Loss: 1.6742\n","Train Epoch: 1 [41280/110534 (37%)]\tAll Loss: 1.9494\tTriple Loss(0): 0.0000\tClassification Loss: 1.9494\n","Train Epoch: 1 [41440/110534 (37%)]\tAll Loss: 2.0465\tTriple Loss(1): 0.1451\tClassification Loss: 1.7563\n","\n","Test set: Average loss: 1.7037, Accuracy: 254/480 (53%)\n","\n","Train Epoch: 1 [41600/110534 (38%)]\tAll Loss: 2.0117\tTriple Loss(1): 0.1471\tClassification Loss: 1.7174\n","Train Epoch: 1 [41760/110534 (38%)]\tAll Loss: 1.2673\tTriple Loss(0): 0.0000\tClassification Loss: 1.2673\n","Train Epoch: 1 [41920/110534 (38%)]\tAll Loss: 1.9234\tTriple Loss(1): 0.0891\tClassification Loss: 1.7452\n","Train Epoch: 1 [42080/110534 (38%)]\tAll Loss: 1.8756\tTriple Loss(1): 0.1567\tClassification Loss: 1.5622\n","Train Epoch: 1 [42240/110534 (38%)]\tAll Loss: 1.5800\tTriple Loss(0): 0.0000\tClassification Loss: 1.5800\n","Train Epoch: 1 [42400/110534 (38%)]\tAll Loss: 1.9033\tTriple Loss(0): 0.0000\tClassification Loss: 1.9033\n","Train Epoch: 1 [42560/110534 (39%)]\tAll Loss: 2.8083\tTriple Loss(1): 0.2004\tClassification Loss: 2.4074\n","Train Epoch: 1 [42720/110534 (39%)]\tAll Loss: 2.4092\tTriple Loss(1): 0.3933\tClassification Loss: 1.6226\n","Train Epoch: 1 [42880/110534 (39%)]\tAll Loss: 1.9786\tTriple Loss(1): 0.4109\tClassification Loss: 1.1568\n","Train Epoch: 1 [43040/110534 (39%)]\tAll Loss: 2.6589\tTriple Loss(1): 0.4070\tClassification Loss: 1.8448\n","\n","Test set: Average loss: 1.6721, Accuracy: 268/480 (56%)\n","\n","Train Epoch: 1 [43200/110534 (39%)]\tAll Loss: 2.7395\tTriple Loss(1): 0.2995\tClassification Loss: 2.1404\n","Train Epoch: 1 [43360/110534 (39%)]\tAll Loss: 2.2771\tTriple Loss(1): 0.3378\tClassification Loss: 1.6015\n","Train Epoch: 1 [43520/110534 (39%)]\tAll Loss: 5.8147\tTriple Loss(0): 1.8893\tClassification Loss: 2.0360\n","Train Epoch: 1 [43680/110534 (40%)]\tAll Loss: 1.8281\tTriple Loss(0): 0.0000\tClassification Loss: 1.8281\n","Train Epoch: 1 [43840/110534 (40%)]\tAll Loss: 2.9911\tTriple Loss(1): 0.4145\tClassification Loss: 2.1621\n","Train Epoch: 1 [44000/110534 (40%)]\tAll Loss: 1.2749\tTriple Loss(1): 0.0271\tClassification Loss: 1.2206\n","Train Epoch: 1 [44160/110534 (40%)]\tAll Loss: 1.9735\tTriple Loss(0): 0.0000\tClassification Loss: 1.9735\n","Train Epoch: 1 [44320/110534 (40%)]\tAll Loss: 2.0241\tTriple Loss(1): 0.1365\tClassification Loss: 1.7511\n","Train Epoch: 1 [44480/110534 (40%)]\tAll Loss: 3.3794\tTriple Loss(1): 0.9619\tClassification Loss: 1.4555\n","Train Epoch: 1 [44640/110534 (40%)]\tAll Loss: 2.7754\tTriple Loss(1): 0.5273\tClassification Loss: 1.7209\n","\n","Test set: Average loss: 1.6759, Accuracy: 262/480 (55%)\n","\n","Train Epoch: 1 [44800/110534 (41%)]\tAll Loss: 1.8406\tTriple Loss(1): 0.0655\tClassification Loss: 1.7096\n","Train Epoch: 1 [44960/110534 (41%)]\tAll Loss: 2.4416\tTriple Loss(1): 0.2229\tClassification Loss: 1.9958\n","Train Epoch: 1 [45120/110534 (41%)]\tAll Loss: 2.6244\tTriple Loss(1): 0.3847\tClassification Loss: 1.8550\n","Train Epoch: 1 [45280/110534 (41%)]\tAll Loss: 2.1538\tTriple Loss(1): 0.2667\tClassification Loss: 1.6204\n","Train Epoch: 1 [45440/110534 (41%)]\tAll Loss: 2.5297\tTriple Loss(1): 0.3624\tClassification Loss: 1.8049\n","Train Epoch: 1 [45600/110534 (41%)]\tAll Loss: 2.4316\tTriple Loss(1): 0.0828\tClassification Loss: 2.2661\n","Train Epoch: 1 [45760/110534 (41%)]\tAll Loss: 2.4020\tTriple Loss(1): 0.1672\tClassification Loss: 2.0677\n","Train Epoch: 1 [45920/110534 (42%)]\tAll Loss: 2.9869\tTriple Loss(1): 0.7682\tClassification Loss: 1.4505\n","Train Epoch: 1 [46080/110534 (42%)]\tAll Loss: 1.6336\tTriple Loss(1): 0.1570\tClassification Loss: 1.3196\n","Train Epoch: 1 [46240/110534 (42%)]\tAll Loss: 1.6570\tTriple Loss(1): 0.0784\tClassification Loss: 1.5001\n","\n","Test set: Average loss: 1.6631, Accuracy: 260/480 (54%)\n","\n","Train Epoch: 1 [46400/110534 (42%)]\tAll Loss: 2.0499\tTriple Loss(1): 0.2161\tClassification Loss: 1.6176\n","Train Epoch: 1 [46560/110534 (42%)]\tAll Loss: 2.8014\tTriple Loss(1): 0.4793\tClassification Loss: 1.8428\n","Train Epoch: 1 [46720/110534 (42%)]\tAll Loss: 2.0991\tTriple Loss(1): 0.1912\tClassification Loss: 1.7166\n","Train Epoch: 1 [46880/110534 (42%)]\tAll Loss: 2.1935\tTriple Loss(1): 0.3623\tClassification Loss: 1.4689\n","Train Epoch: 1 [47040/110534 (43%)]\tAll Loss: 3.2774\tTriple Loss(1): 0.7869\tClassification Loss: 1.7035\n","Train Epoch: 1 [47200/110534 (43%)]\tAll Loss: 2.4117\tTriple Loss(1): 0.2790\tClassification Loss: 1.8538\n","Train Epoch: 1 [47360/110534 (43%)]\tAll Loss: 3.6778\tTriple Loss(1): 0.4898\tClassification Loss: 2.6982\n","Train Epoch: 1 [47520/110534 (43%)]\tAll Loss: 2.2172\tTriple Loss(0): 0.0000\tClassification Loss: 2.2172\n","Train Epoch: 1 [47680/110534 (43%)]\tAll Loss: 1.2244\tTriple Loss(0): 0.0000\tClassification Loss: 1.2244\n","Train Epoch: 1 [47840/110534 (43%)]\tAll Loss: 1.9322\tTriple Loss(1): 0.2470\tClassification Loss: 1.4382\n","\n","Test set: Average loss: 1.6978, Accuracy: 264/480 (55%)\n","\n","Train Epoch: 1 [48000/110534 (43%)]\tAll Loss: 2.3713\tTriple Loss(1): 0.2886\tClassification Loss: 1.7942\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_3000.pth.tar\n","Train Epoch: 1 [48160/110534 (44%)]\tAll Loss: 1.4578\tTriple Loss(0): 0.0000\tClassification Loss: 1.4578\n","Train Epoch: 1 [48320/110534 (44%)]\tAll Loss: 1.6336\tTriple Loss(1): 0.2186\tClassification Loss: 1.1963\n","Train Epoch: 1 [48480/110534 (44%)]\tAll Loss: 2.3242\tTriple Loss(1): 0.1432\tClassification Loss: 2.0378\n","Train Epoch: 1 [48640/110534 (44%)]\tAll Loss: 2.1099\tTriple Loss(1): 0.3013\tClassification Loss: 1.5073\n","Train Epoch: 1 [48800/110534 (44%)]\tAll Loss: 2.1901\tTriple Loss(1): 0.1239\tClassification Loss: 1.9422\n","Train Epoch: 1 [48960/110534 (44%)]\tAll Loss: 2.0225\tTriple Loss(1): 0.1874\tClassification Loss: 1.6477\n","Train Epoch: 1 [49120/110534 (44%)]\tAll Loss: 1.9272\tTriple Loss(1): 0.2077\tClassification Loss: 1.5118\n","Train Epoch: 1 [49280/110534 (45%)]\tAll Loss: 2.4594\tTriple Loss(0): 0.0000\tClassification Loss: 2.4594\n","Train Epoch: 1 [49440/110534 (45%)]\tAll Loss: 2.8511\tTriple Loss(1): 0.4788\tClassification Loss: 1.8936\n","\n","Test set: Average loss: 1.6825, Accuracy: 265/480 (55%)\n","\n","Train Epoch: 1 [49600/110534 (45%)]\tAll Loss: 2.1997\tTriple Loss(1): 0.2990\tClassification Loss: 1.6017\n","Train Epoch: 1 [49760/110534 (45%)]\tAll Loss: 2.1875\tTriple Loss(0): 0.0000\tClassification Loss: 2.1875\n","Train Epoch: 1 [49920/110534 (45%)]\tAll Loss: 1.5093\tTriple Loss(1): 0.1610\tClassification Loss: 1.1874\n","Train Epoch: 1 [50080/110534 (45%)]\tAll Loss: 2.3034\tTriple Loss(1): 0.0975\tClassification Loss: 2.1084\n","Train Epoch: 1 [50240/110534 (45%)]\tAll Loss: 3.2061\tTriple Loss(1): 0.7579\tClassification Loss: 1.6902\n","Train Epoch: 1 [50400/110534 (46%)]\tAll Loss: 2.2060\tTriple Loss(1): 0.3240\tClassification Loss: 1.5581\n","Train Epoch: 1 [50560/110534 (46%)]\tAll Loss: 2.2236\tTriple Loss(1): 0.2350\tClassification Loss: 1.7537\n","Train Epoch: 1 [50720/110534 (46%)]\tAll Loss: 2.5736\tTriple Loss(1): 0.1968\tClassification Loss: 2.1800\n","Train Epoch: 1 [50880/110534 (46%)]\tAll Loss: 1.2685\tTriple Loss(0): 0.0000\tClassification Loss: 1.2685\n","Train Epoch: 1 [51040/110534 (46%)]\tAll Loss: 1.6777\tTriple Loss(0): 0.0000\tClassification Loss: 1.6777\n","\n","Test set: Average loss: 1.6988, Accuracy: 257/480 (54%)\n","\n","Train Epoch: 1 [51200/110534 (46%)]\tAll Loss: 2.4098\tTriple Loss(1): 0.4714\tClassification Loss: 1.4670\n","Train Epoch: 1 [51360/110534 (46%)]\tAll Loss: 2.4801\tTriple Loss(1): 0.1653\tClassification Loss: 2.1495\n","Train Epoch: 1 [51520/110534 (47%)]\tAll Loss: 1.3963\tTriple Loss(0): 0.0000\tClassification Loss: 1.3963\n","Train Epoch: 1 [51680/110534 (47%)]\tAll Loss: 1.8958\tTriple Loss(1): 0.0082\tClassification Loss: 1.8794\n","Train Epoch: 1 [51840/110534 (47%)]\tAll Loss: 2.0692\tTriple Loss(1): 0.0904\tClassification Loss: 1.8883\n","Train Epoch: 1 [52000/110534 (47%)]\tAll Loss: 2.0395\tTriple Loss(0): 0.0000\tClassification Loss: 2.0395\n","Train Epoch: 1 [52160/110534 (47%)]\tAll Loss: 2.6840\tTriple Loss(1): 0.1743\tClassification Loss: 2.3353\n","Train Epoch: 1 [52320/110534 (47%)]\tAll Loss: 1.7882\tTriple Loss(1): 0.3951\tClassification Loss: 0.9979\n","Train Epoch: 1 [52480/110534 (47%)]\tAll Loss: 4.6125\tTriple Loss(0): 1.3091\tClassification Loss: 1.9942\n","Train Epoch: 1 [52640/110534 (48%)]\tAll Loss: 2.7230\tTriple Loss(1): 0.4872\tClassification Loss: 1.7485\n","\n","Test set: Average loss: 1.6785, Accuracy: 254/480 (53%)\n","\n","Train Epoch: 1 [52800/110534 (48%)]\tAll Loss: 2.0589\tTriple Loss(0): 0.0000\tClassification Loss: 2.0589\n","Train Epoch: 1 [52960/110534 (48%)]\tAll Loss: 1.6605\tTriple Loss(1): 0.0565\tClassification Loss: 1.5475\n","Train Epoch: 1 [53120/110534 (48%)]\tAll Loss: 1.7958\tTriple Loss(1): 0.0000\tClassification Loss: 1.7958\n","Train Epoch: 1 [53280/110534 (48%)]\tAll Loss: 1.3486\tTriple Loss(1): 0.1788\tClassification Loss: 0.9911\n","Train Epoch: 1 [53440/110534 (48%)]\tAll Loss: 2.1211\tTriple Loss(0): 0.0000\tClassification Loss: 2.1211\n","Train Epoch: 1 [53600/110534 (48%)]\tAll Loss: 2.4411\tTriple Loss(1): 0.4609\tClassification Loss: 1.5194\n","Train Epoch: 1 [53760/110534 (49%)]\tAll Loss: 2.5613\tTriple Loss(1): 0.0945\tClassification Loss: 2.3723\n","Train Epoch: 1 [53920/110534 (49%)]\tAll Loss: 3.3960\tTriple Loss(1): 0.6378\tClassification Loss: 2.1205\n","Train Epoch: 1 [54080/110534 (49%)]\tAll Loss: 2.0700\tTriple Loss(1): 0.2435\tClassification Loss: 1.5830\n","Train Epoch: 1 [54240/110534 (49%)]\tAll Loss: 1.7183\tTriple Loss(1): 0.1080\tClassification Loss: 1.5023\n","\n","Test set: Average loss: 1.6812, Accuracy: 263/480 (55%)\n","\n","Train Epoch: 1 [54400/110534 (49%)]\tAll Loss: 3.3269\tTriple Loss(1): 0.5658\tClassification Loss: 2.1953\n","Train Epoch: 1 [54560/110534 (49%)]\tAll Loss: 1.9592\tTriple Loss(1): 0.2024\tClassification Loss: 1.5544\n","Train Epoch: 1 [54720/110534 (50%)]\tAll Loss: 2.0219\tTriple Loss(1): 0.3036\tClassification Loss: 1.4148\n","Train Epoch: 1 [54880/110534 (50%)]\tAll Loss: 2.5557\tTriple Loss(0): 0.0000\tClassification Loss: 2.5557\n","Train Epoch: 1 [55040/110534 (50%)]\tAll Loss: 2.1819\tTriple Loss(1): 0.3546\tClassification Loss: 1.4727\n","Train Epoch: 1 [55200/110534 (50%)]\tAll Loss: 2.4238\tTriple Loss(1): 0.4099\tClassification Loss: 1.6040\n","Train Epoch: 1 [55360/110534 (50%)]\tAll Loss: 1.9447\tTriple Loss(1): 0.3487\tClassification Loss: 1.2472\n","Train Epoch: 1 [55520/110534 (50%)]\tAll Loss: 2.8979\tTriple Loss(1): 0.4491\tClassification Loss: 1.9996\n","Train Epoch: 1 [55680/110534 (50%)]\tAll Loss: 2.4087\tTriple Loss(1): 0.5075\tClassification Loss: 1.3937\n","Train Epoch: 1 [55840/110534 (51%)]\tAll Loss: 1.5925\tTriple Loss(0): 0.0000\tClassification Loss: 1.5925\n","\n","Test set: Average loss: 1.6728, Accuracy: 264/480 (55%)\n","\n","Train Epoch: 1 [56000/110534 (51%)]\tAll Loss: 1.6978\tTriple Loss(1): 0.0271\tClassification Loss: 1.6437\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_3500.pth.tar\n","Train Epoch: 1 [56160/110534 (51%)]\tAll Loss: 2.0734\tTriple Loss(1): 0.2336\tClassification Loss: 1.6062\n","Train Epoch: 1 [56320/110534 (51%)]\tAll Loss: 8.8324\tTriple Loss(0): 3.7289\tClassification Loss: 1.3746\n","Train Epoch: 1 [56480/110534 (51%)]\tAll Loss: 2.2377\tTriple Loss(1): 0.5183\tClassification Loss: 1.2010\n","Train Epoch: 1 [56640/110534 (51%)]\tAll Loss: 2.2038\tTriple Loss(1): 0.3322\tClassification Loss: 1.5394\n","Train Epoch: 1 [56800/110534 (51%)]\tAll Loss: 1.9944\tTriple Loss(1): 0.2275\tClassification Loss: 1.5394\n","Train Epoch: 1 [56960/110534 (52%)]\tAll Loss: 2.7055\tTriple Loss(1): 0.4073\tClassification Loss: 1.8908\n","Train Epoch: 1 [57120/110534 (52%)]\tAll Loss: 1.3453\tTriple Loss(0): 0.0000\tClassification Loss: 1.3453\n","Train Epoch: 1 [57280/110534 (52%)]\tAll Loss: 2.0179\tTriple Loss(0): 0.0000\tClassification Loss: 2.0179\n","Train Epoch: 1 [57440/110534 (52%)]\tAll Loss: 2.1202\tTriple Loss(1): 0.3570\tClassification Loss: 1.4063\n","\n","Test set: Average loss: 1.7094, Accuracy: 249/480 (52%)\n","\n","Train Epoch: 1 [57600/110534 (52%)]\tAll Loss: 1.8623\tTriple Loss(1): 0.0383\tClassification Loss: 1.7856\n","Train Epoch: 1 [57760/110534 (52%)]\tAll Loss: 1.8490\tTriple Loss(1): 0.2395\tClassification Loss: 1.3701\n","Train Epoch: 1 [57920/110534 (52%)]\tAll Loss: 2.0692\tTriple Loss(1): 0.2580\tClassification Loss: 1.5533\n","Train Epoch: 1 [58080/110534 (53%)]\tAll Loss: 2.0210\tTriple Loss(1): 0.2395\tClassification Loss: 1.5420\n","Train Epoch: 1 [58240/110534 (53%)]\tAll Loss: 1.8118\tTriple Loss(1): 0.0000\tClassification Loss: 1.8118\n","Train Epoch: 1 [58400/110534 (53%)]\tAll Loss: 1.5438\tTriple Loss(1): 0.1001\tClassification Loss: 1.3435\n","Train Epoch: 1 [58560/110534 (53%)]\tAll Loss: 2.3922\tTriple Loss(1): 0.0000\tClassification Loss: 2.3922\n","Train Epoch: 1 [58720/110534 (53%)]\tAll Loss: 1.5593\tTriple Loss(0): 0.0339\tClassification Loss: 1.4915\n","Train Epoch: 1 [58880/110534 (53%)]\tAll Loss: 1.8890\tTriple Loss(1): 0.0689\tClassification Loss: 1.7513\n","Train Epoch: 1 [59040/110534 (53%)]\tAll Loss: 1.8132\tTriple Loss(1): 0.1625\tClassification Loss: 1.4882\n","\n","Test set: Average loss: 1.6466, Accuracy: 260/480 (54%)\n","\n","Train Epoch: 1 [59200/110534 (54%)]\tAll Loss: 1.7775\tTriple Loss(1): 0.0463\tClassification Loss: 1.6849\n","Train Epoch: 1 [59360/110534 (54%)]\tAll Loss: 1.4019\tTriple Loss(1): 0.0895\tClassification Loss: 1.2230\n","Train Epoch: 1 [59520/110534 (54%)]\tAll Loss: 2.2084\tTriple Loss(1): 0.1804\tClassification Loss: 1.8476\n","Train Epoch: 1 [59680/110534 (54%)]\tAll Loss: 2.3440\tTriple Loss(1): 0.2603\tClassification Loss: 1.8233\n","Train Epoch: 1 [59840/110534 (54%)]\tAll Loss: 2.2414\tTriple Loss(1): 0.1101\tClassification Loss: 2.0212\n","Train Epoch: 1 [60000/110534 (54%)]\tAll Loss: 2.5493\tTriple Loss(1): 0.2031\tClassification Loss: 2.1431\n","Train Epoch: 1 [60160/110534 (54%)]\tAll Loss: 3.3830\tTriple Loss(1): 0.5784\tClassification Loss: 2.2263\n","Train Epoch: 1 [60320/110534 (55%)]\tAll Loss: 2.1181\tTriple Loss(1): 0.0046\tClassification Loss: 2.1089\n","Train Epoch: 1 [60480/110534 (55%)]\tAll Loss: 1.8435\tTriple Loss(0): 0.2838\tClassification Loss: 1.2760\n","Train Epoch: 1 [60640/110534 (55%)]\tAll Loss: 1.8172\tTriple Loss(1): 0.2611\tClassification Loss: 1.2949\n","\n","Test set: Average loss: 1.6215, Accuracy: 257/480 (54%)\n","\n","Train Epoch: 1 [60800/110534 (55%)]\tAll Loss: 1.0416\tTriple Loss(1): 0.0567\tClassification Loss: 0.9282\n","Train Epoch: 1 [60960/110534 (55%)]\tAll Loss: 3.0365\tTriple Loss(1): 0.8589\tClassification Loss: 1.3187\n","Train Epoch: 1 [61120/110534 (55%)]\tAll Loss: 1.4907\tTriple Loss(0): 0.0000\tClassification Loss: 1.4907\n","Train Epoch: 1 [61280/110534 (55%)]\tAll Loss: 2.4558\tTriple Loss(1): 0.3532\tClassification Loss: 1.7494\n","Train Epoch: 1 [61440/110534 (56%)]\tAll Loss: 2.1144\tTriple Loss(1): 0.1948\tClassification Loss: 1.7247\n","Train Epoch: 1 [61600/110534 (56%)]\tAll Loss: 4.0638\tTriple Loss(1): 0.7430\tClassification Loss: 2.5778\n","Train Epoch: 1 [61760/110534 (56%)]\tAll Loss: 2.3771\tTriple Loss(1): 0.2536\tClassification Loss: 1.8698\n","Train Epoch: 1 [61920/110534 (56%)]\tAll Loss: 1.9793\tTriple Loss(1): 0.2430\tClassification Loss: 1.4933\n","Train Epoch: 1 [62080/110534 (56%)]\tAll Loss: 2.7705\tTriple Loss(1): 0.5037\tClassification Loss: 1.7631\n","Train Epoch: 1 [62240/110534 (56%)]\tAll Loss: 1.7750\tTriple Loss(1): 0.1895\tClassification Loss: 1.3960\n","\n","Test set: Average loss: 1.6773, Accuracy: 264/480 (55%)\n","\n","Train Epoch: 1 [62400/110534 (56%)]\tAll Loss: 2.4275\tTriple Loss(1): 0.3676\tClassification Loss: 1.6923\n","Train Epoch: 1 [62560/110534 (57%)]\tAll Loss: 2.4252\tTriple Loss(1): 0.2594\tClassification Loss: 1.9064\n","Train Epoch: 1 [62720/110534 (57%)]\tAll Loss: 2.5699\tTriple Loss(1): 0.6401\tClassification Loss: 1.2897\n","Train Epoch: 1 [62880/110534 (57%)]\tAll Loss: 1.4311\tTriple Loss(1): 0.0695\tClassification Loss: 1.2922\n","Train Epoch: 1 [63040/110534 (57%)]\tAll Loss: 2.8074\tTriple Loss(1): 0.5193\tClassification Loss: 1.7689\n","Train Epoch: 1 [63200/110534 (57%)]\tAll Loss: 1.9047\tTriple Loss(1): 0.2216\tClassification Loss: 1.4614\n","Train Epoch: 1 [63360/110534 (57%)]\tAll Loss: 2.0439\tTriple Loss(1): 0.0532\tClassification Loss: 1.9374\n","Train Epoch: 1 [63520/110534 (57%)]\tAll Loss: 2.3492\tTriple Loss(1): 0.4714\tClassification Loss: 1.4064\n","Train Epoch: 1 [63680/110534 (58%)]\tAll Loss: 1.7539\tTriple Loss(1): 0.1781\tClassification Loss: 1.3978\n","Train Epoch: 1 [63840/110534 (58%)]\tAll Loss: 1.5466\tTriple Loss(0): 0.0000\tClassification Loss: 1.5466\n","\n","Test set: Average loss: 1.6970, Accuracy: 254/480 (53%)\n","\n","Train Epoch: 1 [64000/110534 (58%)]\tAll Loss: 2.4669\tTriple Loss(1): 0.3871\tClassification Loss: 1.6927\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_4000.pth.tar\n","Train Epoch: 1 [64160/110534 (58%)]\tAll Loss: 4.4717\tTriple Loss(0): 1.3129\tClassification Loss: 1.8459\n","Train Epoch: 1 [64320/110534 (58%)]\tAll Loss: 1.3408\tTriple Loss(1): 0.2356\tClassification Loss: 0.8696\n","Train Epoch: 1 [64480/110534 (58%)]\tAll Loss: 1.8467\tTriple Loss(0): 0.0000\tClassification Loss: 1.8467\n","Train Epoch: 1 [64640/110534 (58%)]\tAll Loss: 12.3599\tTriple Loss(0): 5.2812\tClassification Loss: 1.7974\n","Train Epoch: 1 [64800/110534 (59%)]\tAll Loss: 1.8609\tTriple Loss(1): 0.3850\tClassification Loss: 1.0909\n","Train Epoch: 1 [64960/110534 (59%)]\tAll Loss: 1.6713\tTriple Loss(0): 0.0000\tClassification Loss: 1.6713\n","Train Epoch: 1 [65120/110534 (59%)]\tAll Loss: 2.4306\tTriple Loss(1): 0.3024\tClassification Loss: 1.8258\n","Train Epoch: 1 [65280/110534 (59%)]\tAll Loss: 2.8643\tTriple Loss(1): 0.4447\tClassification Loss: 1.9749\n","Train Epoch: 1 [65440/110534 (59%)]\tAll Loss: 1.3732\tTriple Loss(0): 0.0000\tClassification Loss: 1.3732\n","\n","Test set: Average loss: 1.6228, Accuracy: 268/480 (56%)\n","\n","Train Epoch: 1 [65600/110534 (59%)]\tAll Loss: 1.8259\tTriple Loss(1): 0.1240\tClassification Loss: 1.5778\n","Train Epoch: 1 [65760/110534 (59%)]\tAll Loss: 1.7455\tTriple Loss(0): 0.0000\tClassification Loss: 1.7455\n","Train Epoch: 1 [65920/110534 (60%)]\tAll Loss: 1.6739\tTriple Loss(0): 0.0000\tClassification Loss: 1.6739\n","Train Epoch: 1 [66080/110534 (60%)]\tAll Loss: 1.5385\tTriple Loss(1): 0.0667\tClassification Loss: 1.4050\n","Train Epoch: 1 [66240/110534 (60%)]\tAll Loss: 1.7049\tTriple Loss(1): 0.1375\tClassification Loss: 1.4299\n","Train Epoch: 1 [66400/110534 (60%)]\tAll Loss: 1.5913\tTriple Loss(1): 0.1281\tClassification Loss: 1.3352\n","Train Epoch: 1 [66560/110534 (60%)]\tAll Loss: 2.7134\tTriple Loss(1): 0.7555\tClassification Loss: 1.2025\n","Train Epoch: 1 [66720/110534 (60%)]\tAll Loss: 2.9466\tTriple Loss(1): 0.5740\tClassification Loss: 1.7987\n","Train Epoch: 1 [66880/110534 (61%)]\tAll Loss: 1.9978\tTriple Loss(0): 0.0000\tClassification Loss: 1.9978\n","Train Epoch: 1 [67040/110534 (61%)]\tAll Loss: 2.6706\tTriple Loss(1): 0.1907\tClassification Loss: 2.2892\n","\n","Test set: Average loss: 1.6213, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 1 [67200/110534 (61%)]\tAll Loss: 1.9734\tTriple Loss(0): 0.0000\tClassification Loss: 1.9734\n","Train Epoch: 1 [67360/110534 (61%)]\tAll Loss: 2.7345\tTriple Loss(1): 0.9053\tClassification Loss: 0.9239\n","Train Epoch: 1 [67520/110534 (61%)]\tAll Loss: 2.9239\tTriple Loss(1): 0.6169\tClassification Loss: 1.6901\n","Train Epoch: 1 [67680/110534 (61%)]\tAll Loss: 1.8416\tTriple Loss(1): 0.2683\tClassification Loss: 1.3050\n","Train Epoch: 1 [67840/110534 (61%)]\tAll Loss: 2.6034\tTriple Loss(1): 0.3311\tClassification Loss: 1.9412\n","Train Epoch: 1 [68000/110534 (62%)]\tAll Loss: 1.8589\tTriple Loss(1): 0.0234\tClassification Loss: 1.8120\n","Train Epoch: 1 [68160/110534 (62%)]\tAll Loss: 2.3961\tTriple Loss(1): 0.2425\tClassification Loss: 1.9111\n","Train Epoch: 1 [68320/110534 (62%)]\tAll Loss: 2.1717\tTriple Loss(1): 0.4606\tClassification Loss: 1.2506\n","Train Epoch: 1 [68480/110534 (62%)]\tAll Loss: 2.1801\tTriple Loss(1): 0.4238\tClassification Loss: 1.3326\n","Train Epoch: 1 [68640/110534 (62%)]\tAll Loss: 2.6938\tTriple Loss(1): 0.3069\tClassification Loss: 2.0800\n","\n","Test set: Average loss: 1.6464, Accuracy: 265/480 (55%)\n","\n","Train Epoch: 1 [68800/110534 (62%)]\tAll Loss: 2.1290\tTriple Loss(0): 0.0000\tClassification Loss: 2.1290\n","Train Epoch: 1 [68960/110534 (62%)]\tAll Loss: 2.2399\tTriple Loss(1): 0.3377\tClassification Loss: 1.5645\n","Train Epoch: 1 [69120/110534 (63%)]\tAll Loss: 2.9872\tTriple Loss(0): 0.7894\tClassification Loss: 1.4084\n","Train Epoch: 1 [69280/110534 (63%)]\tAll Loss: 2.2098\tTriple Loss(1): 0.3832\tClassification Loss: 1.4434\n","Train Epoch: 1 [69440/110534 (63%)]\tAll Loss: 2.2856\tTriple Loss(0): 0.0000\tClassification Loss: 2.2856\n","Train Epoch: 1 [69600/110534 (63%)]\tAll Loss: 2.2666\tTriple Loss(1): 0.4616\tClassification Loss: 1.3434\n","Train Epoch: 1 [69760/110534 (63%)]\tAll Loss: 1.8810\tTriple Loss(1): 0.2479\tClassification Loss: 1.3851\n","Train Epoch: 1 [69920/110534 (63%)]\tAll Loss: 2.8738\tTriple Loss(1): 0.5448\tClassification Loss: 1.7843\n","Train Epoch: 1 [70080/110534 (63%)]\tAll Loss: 1.7484\tTriple Loss(0): 0.0000\tClassification Loss: 1.7484\n","Train Epoch: 1 [70240/110534 (64%)]\tAll Loss: 2.7004\tTriple Loss(1): 0.3681\tClassification Loss: 1.9642\n","\n","Test set: Average loss: 1.6362, Accuracy: 265/480 (55%)\n","\n","Train Epoch: 1 [70400/110534 (64%)]\tAll Loss: 3.2055\tTriple Loss(1): 0.6243\tClassification Loss: 1.9570\n","Train Epoch: 1 [70560/110534 (64%)]\tAll Loss: 2.3078\tTriple Loss(1): 0.2318\tClassification Loss: 1.8441\n","Train Epoch: 1 [70720/110534 (64%)]\tAll Loss: 1.4025\tTriple Loss(0): 0.0000\tClassification Loss: 1.4025\n","Train Epoch: 1 [70880/110534 (64%)]\tAll Loss: 5.7508\tTriple Loss(0): 1.9609\tClassification Loss: 1.8291\n","Train Epoch: 1 [71040/110534 (64%)]\tAll Loss: 1.9687\tTriple Loss(1): 0.3277\tClassification Loss: 1.3132\n","Train Epoch: 1 [71200/110534 (64%)]\tAll Loss: 2.2150\tTriple Loss(1): 0.2395\tClassification Loss: 1.7360\n","Train Epoch: 1 [71360/110534 (65%)]\tAll Loss: 1.6800\tTriple Loss(1): 0.2928\tClassification Loss: 1.0944\n","Train Epoch: 1 [71520/110534 (65%)]\tAll Loss: 2.5482\tTriple Loss(1): 0.3351\tClassification Loss: 1.8780\n","Train Epoch: 1 [71680/110534 (65%)]\tAll Loss: 1.3393\tTriple Loss(1): 0.0564\tClassification Loss: 1.2265\n","Train Epoch: 1 [71840/110534 (65%)]\tAll Loss: 2.8414\tTriple Loss(1): 0.4300\tClassification Loss: 1.9813\n","\n","Test set: Average loss: 1.6323, Accuracy: 267/480 (56%)\n","\n","Train Epoch: 1 [72000/110534 (65%)]\tAll Loss: 2.0832\tTriple Loss(0): 0.0451\tClassification Loss: 1.9930\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_4500.pth.tar\n","Train Epoch: 1 [72160/110534 (65%)]\tAll Loss: 2.1252\tTriple Loss(1): 0.2203\tClassification Loss: 1.6847\n","Train Epoch: 1 [72320/110534 (65%)]\tAll Loss: 1.9454\tTriple Loss(1): 0.1973\tClassification Loss: 1.5507\n","Train Epoch: 1 [72480/110534 (66%)]\tAll Loss: 2.7652\tTriple Loss(1): 0.3053\tClassification Loss: 2.1547\n","Train Epoch: 1 [72640/110534 (66%)]\tAll Loss: 2.3933\tTriple Loss(1): 0.3527\tClassification Loss: 1.6879\n","Train Epoch: 1 [72800/110534 (66%)]\tAll Loss: 2.2348\tTriple Loss(1): 0.2156\tClassification Loss: 1.8035\n","Train Epoch: 1 [72960/110534 (66%)]\tAll Loss: 1.5331\tTriple Loss(1): 0.2482\tClassification Loss: 1.0367\n","Train Epoch: 1 [73120/110534 (66%)]\tAll Loss: 2.3286\tTriple Loss(1): 0.2611\tClassification Loss: 1.8065\n","Train Epoch: 1 [73280/110534 (66%)]\tAll Loss: 2.0925\tTriple Loss(1): 0.2968\tClassification Loss: 1.4990\n","Train Epoch: 1 [73440/110534 (66%)]\tAll Loss: 1.6177\tTriple Loss(1): 0.1575\tClassification Loss: 1.3026\n","\n","Test set: Average loss: 1.6414, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 1 [73600/110534 (67%)]\tAll Loss: 1.8930\tTriple Loss(1): 0.1976\tClassification Loss: 1.4979\n","Train Epoch: 1 [73760/110534 (67%)]\tAll Loss: 2.5732\tTriple Loss(1): 0.4274\tClassification Loss: 1.7184\n","Train Epoch: 1 [73920/110534 (67%)]\tAll Loss: 1.6676\tTriple Loss(1): 0.0970\tClassification Loss: 1.4736\n","Train Epoch: 1 [74080/110534 (67%)]\tAll Loss: 2.2858\tTriple Loss(1): 0.3353\tClassification Loss: 1.6151\n","Train Epoch: 1 [74240/110534 (67%)]\tAll Loss: 2.0023\tTriple Loss(0): 0.0000\tClassification Loss: 2.0023\n","Train Epoch: 1 [74400/110534 (67%)]\tAll Loss: 1.8681\tTriple Loss(1): 0.1121\tClassification Loss: 1.6438\n","Train Epoch: 1 [74560/110534 (67%)]\tAll Loss: 2.0431\tTriple Loss(1): 0.2406\tClassification Loss: 1.5619\n","Train Epoch: 1 [74720/110534 (68%)]\tAll Loss: 2.1939\tTriple Loss(1): 0.2588\tClassification Loss: 1.6763\n","Train Epoch: 1 [74880/110534 (68%)]\tAll Loss: 2.4023\tTriple Loss(1): 0.4809\tClassification Loss: 1.4405\n","Train Epoch: 1 [75040/110534 (68%)]\tAll Loss: 1.0253\tTriple Loss(0): 0.0000\tClassification Loss: 1.0253\n","\n","Test set: Average loss: 1.6173, Accuracy: 262/480 (55%)\n","\n","Train Epoch: 1 [75200/110534 (68%)]\tAll Loss: 1.1513\tTriple Loss(0): 0.0000\tClassification Loss: 1.1513\n","Train Epoch: 1 [75360/110534 (68%)]\tAll Loss: 2.0524\tTriple Loss(0): 0.0000\tClassification Loss: 2.0524\n","Train Epoch: 1 [75520/110534 (68%)]\tAll Loss: 2.1526\tTriple Loss(1): 0.2226\tClassification Loss: 1.7074\n","Train Epoch: 1 [75680/110534 (68%)]\tAll Loss: 2.4382\tTriple Loss(1): 0.0000\tClassification Loss: 2.4382\n","Train Epoch: 1 [75840/110534 (69%)]\tAll Loss: 2.0371\tTriple Loss(1): 0.2238\tClassification Loss: 1.5895\n","Train Epoch: 1 [76000/110534 (69%)]\tAll Loss: 3.4760\tTriple Loss(1): 0.7243\tClassification Loss: 2.0274\n","Train Epoch: 1 [76160/110534 (69%)]\tAll Loss: 2.6746\tTriple Loss(1): 0.4229\tClassification Loss: 1.8288\n","Train Epoch: 1 [76320/110534 (69%)]\tAll Loss: 10.3863\tTriple Loss(0): 4.3872\tClassification Loss: 1.6119\n","Train Epoch: 1 [76480/110534 (69%)]\tAll Loss: 2.2038\tTriple Loss(1): 0.2545\tClassification Loss: 1.6948\n","Train Epoch: 1 [76640/110534 (69%)]\tAll Loss: 1.7745\tTriple Loss(1): 0.3061\tClassification Loss: 1.1623\n","\n","Test set: Average loss: 1.6077, Accuracy: 264/480 (55%)\n","\n","Train Epoch: 1 [76800/110534 (69%)]\tAll Loss: 2.1368\tTriple Loss(1): 0.2414\tClassification Loss: 1.6541\n","Train Epoch: 1 [76960/110534 (70%)]\tAll Loss: 2.1666\tTriple Loss(1): 0.2466\tClassification Loss: 1.6734\n","Train Epoch: 1 [77120/110534 (70%)]\tAll Loss: 1.7655\tTriple Loss(1): 0.2981\tClassification Loss: 1.1693\n","Train Epoch: 1 [77280/110534 (70%)]\tAll Loss: 2.1322\tTriple Loss(1): 0.3421\tClassification Loss: 1.4480\n","Train Epoch: 1 [77440/110534 (70%)]\tAll Loss: 1.6206\tTriple Loss(0): 0.0000\tClassification Loss: 1.6206\n","Train Epoch: 1 [77600/110534 (70%)]\tAll Loss: 2.8827\tTriple Loss(1): 0.3847\tClassification Loss: 2.1133\n","Train Epoch: 1 [77760/110534 (70%)]\tAll Loss: 2.2949\tTriple Loss(1): 0.1570\tClassification Loss: 1.9810\n","Train Epoch: 1 [77920/110534 (70%)]\tAll Loss: 3.4215\tTriple Loss(1): 0.7171\tClassification Loss: 1.9874\n","Train Epoch: 1 [78080/110534 (71%)]\tAll Loss: 1.6556\tTriple Loss(0): 0.0000\tClassification Loss: 1.6556\n","Train Epoch: 1 [78240/110534 (71%)]\tAll Loss: 1.3832\tTriple Loss(0): 0.0000\tClassification Loss: 1.3832\n","\n","Test set: Average loss: 1.6392, Accuracy: 265/480 (55%)\n","\n","Train Epoch: 1 [78400/110534 (71%)]\tAll Loss: 2.2169\tTriple Loss(1): 0.4192\tClassification Loss: 1.3785\n","Train Epoch: 1 [78560/110534 (71%)]\tAll Loss: 2.9188\tTriple Loss(1): 0.4032\tClassification Loss: 2.1124\n","Train Epoch: 1 [78720/110534 (71%)]\tAll Loss: 1.2980\tTriple Loss(0): 0.0000\tClassification Loss: 1.2980\n","Train Epoch: 1 [78880/110534 (71%)]\tAll Loss: 2.5688\tTriple Loss(1): 0.4302\tClassification Loss: 1.7084\n","Train Epoch: 1 [79040/110534 (72%)]\tAll Loss: 1.8942\tTriple Loss(1): 0.2051\tClassification Loss: 1.4840\n","Train Epoch: 1 [79200/110534 (72%)]\tAll Loss: 2.0081\tTriple Loss(1): 0.1343\tClassification Loss: 1.7394\n","Train Epoch: 1 [79360/110534 (72%)]\tAll Loss: 1.8965\tTriple Loss(1): 0.0707\tClassification Loss: 1.7552\n","Train Epoch: 1 [79520/110534 (72%)]\tAll Loss: 1.6637\tTriple Loss(1): 0.0753\tClassification Loss: 1.5132\n","Train Epoch: 1 [79680/110534 (72%)]\tAll Loss: 2.4513\tTriple Loss(1): 0.3455\tClassification Loss: 1.7602\n","Train Epoch: 1 [79840/110534 (72%)]\tAll Loss: 1.4490\tTriple Loss(0): 0.0000\tClassification Loss: 1.4490\n","\n","Test set: Average loss: 1.6122, Accuracy: 271/480 (56%)\n","\n","Train Epoch: 1 [80000/110534 (72%)]\tAll Loss: 2.7491\tTriple Loss(1): 0.3813\tClassification Loss: 1.9865\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_5000.pth.tar\n","Train Epoch: 1 [80160/110534 (73%)]\tAll Loss: 2.8568\tTriple Loss(1): 0.3745\tClassification Loss: 2.1077\n","Train Epoch: 1 [80320/110534 (73%)]\tAll Loss: 2.5483\tTriple Loss(1): 0.0185\tClassification Loss: 2.5113\n","Train Epoch: 1 [80480/110534 (73%)]\tAll Loss: 1.9447\tTriple Loss(1): 0.1711\tClassification Loss: 1.6025\n","Train Epoch: 1 [80640/110534 (73%)]\tAll Loss: 2.2837\tTriple Loss(1): 0.2608\tClassification Loss: 1.7621\n","Train Epoch: 1 [80800/110534 (73%)]\tAll Loss: 2.0891\tTriple Loss(1): 0.0990\tClassification Loss: 1.8911\n","Train Epoch: 1 [80960/110534 (73%)]\tAll Loss: 1.4905\tTriple Loss(0): 0.0000\tClassification Loss: 1.4905\n","Train Epoch: 1 [81120/110534 (73%)]\tAll Loss: 1.8431\tTriple Loss(1): 0.2229\tClassification Loss: 1.3973\n","Train Epoch: 1 [81280/110534 (74%)]\tAll Loss: 1.5885\tTriple Loss(0): 0.0000\tClassification Loss: 1.5885\n","Train Epoch: 1 [81440/110534 (74%)]\tAll Loss: 1.1991\tTriple Loss(0): 0.0000\tClassification Loss: 1.1991\n","\n","Test set: Average loss: 1.6285, Accuracy: 262/480 (55%)\n","\n","Train Epoch: 1 [81600/110534 (74%)]\tAll Loss: 1.6059\tTriple Loss(0): 0.0000\tClassification Loss: 1.6059\n","Train Epoch: 1 [81760/110534 (74%)]\tAll Loss: 2.0426\tTriple Loss(1): 0.0929\tClassification Loss: 1.8568\n","Train Epoch: 1 [81920/110534 (74%)]\tAll Loss: 12.0505\tTriple Loss(0): 5.1340\tClassification Loss: 1.7824\n","Train Epoch: 1 [82080/110534 (74%)]\tAll Loss: 1.4411\tTriple Loss(0): 0.0000\tClassification Loss: 1.4411\n","Train Epoch: 1 [82240/110534 (74%)]\tAll Loss: 1.9713\tTriple Loss(1): 0.4048\tClassification Loss: 1.1618\n","Train Epoch: 1 [82400/110534 (75%)]\tAll Loss: 2.4091\tTriple Loss(1): 0.1687\tClassification Loss: 2.0718\n","Train Epoch: 1 [82560/110534 (75%)]\tAll Loss: 1.9484\tTriple Loss(1): 0.0119\tClassification Loss: 1.9247\n","Train Epoch: 1 [82720/110534 (75%)]\tAll Loss: 2.4120\tTriple Loss(1): 0.5285\tClassification Loss: 1.3550\n","Train Epoch: 1 [82880/110534 (75%)]\tAll Loss: 1.8389\tTriple Loss(1): 0.1911\tClassification Loss: 1.4567\n","Train Epoch: 1 [83040/110534 (75%)]\tAll Loss: 1.8285\tTriple Loss(1): 0.1263\tClassification Loss: 1.5759\n","\n","Test set: Average loss: 1.6313, Accuracy: 264/480 (55%)\n","\n","Train Epoch: 1 [83200/110534 (75%)]\tAll Loss: 1.9692\tTriple Loss(1): 0.2700\tClassification Loss: 1.4292\n","Train Epoch: 1 [83360/110534 (75%)]\tAll Loss: 2.9621\tTriple Loss(1): 0.5578\tClassification Loss: 1.8466\n","Train Epoch: 1 [83520/110534 (76%)]\tAll Loss: 2.3871\tTriple Loss(1): 0.3187\tClassification Loss: 1.7497\n","Train Epoch: 1 [83680/110534 (76%)]\tAll Loss: 2.0642\tTriple Loss(1): 0.1047\tClassification Loss: 1.8548\n","Train Epoch: 1 [83840/110534 (76%)]\tAll Loss: 2.2457\tTriple Loss(1): 0.2511\tClassification Loss: 1.7436\n","Train Epoch: 1 [84000/110534 (76%)]\tAll Loss: 2.0063\tTriple Loss(1): 0.3207\tClassification Loss: 1.3648\n","Train Epoch: 1 [84160/110534 (76%)]\tAll Loss: 2.4584\tTriple Loss(1): 0.2447\tClassification Loss: 1.9690\n","Train Epoch: 1 [84320/110534 (76%)]\tAll Loss: 0.9938\tTriple Loss(1): 0.0197\tClassification Loss: 0.9544\n","Train Epoch: 1 [84480/110534 (76%)]\tAll Loss: 2.0292\tTriple Loss(0): 0.0000\tClassification Loss: 2.0292\n","Train Epoch: 1 [84640/110534 (77%)]\tAll Loss: 1.7020\tTriple Loss(0): 0.0000\tClassification Loss: 1.7020\n","\n","Test set: Average loss: 1.6131, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 1 [84800/110534 (77%)]\tAll Loss: 2.2223\tTriple Loss(1): 0.1208\tClassification Loss: 1.9807\n","Train Epoch: 1 [84960/110534 (77%)]\tAll Loss: 2.8315\tTriple Loss(1): 0.5115\tClassification Loss: 1.8085\n","Train Epoch: 1 [85120/110534 (77%)]\tAll Loss: 1.7969\tTriple Loss(0): 0.0000\tClassification Loss: 1.7969\n","Train Epoch: 1 [85280/110534 (77%)]\tAll Loss: 1.7397\tTriple Loss(1): 0.2265\tClassification Loss: 1.2867\n","Train Epoch: 1 [85440/110534 (77%)]\tAll Loss: 2.2950\tTriple Loss(1): 0.0538\tClassification Loss: 2.1875\n","Train Epoch: 1 [85600/110534 (77%)]\tAll Loss: 1.9060\tTriple Loss(1): 0.1439\tClassification Loss: 1.6182\n","Train Epoch: 1 [85760/110534 (78%)]\tAll Loss: 1.2989\tTriple Loss(0): 0.0000\tClassification Loss: 1.2989\n","Train Epoch: 1 [85920/110534 (78%)]\tAll Loss: 1.9546\tTriple Loss(1): 0.3907\tClassification Loss: 1.1733\n","Train Epoch: 1 [86080/110534 (78%)]\tAll Loss: 1.8174\tTriple Loss(0): 0.0000\tClassification Loss: 1.8174\n","Train Epoch: 1 [86240/110534 (78%)]\tAll Loss: 2.0271\tTriple Loss(1): 0.2373\tClassification Loss: 1.5524\n","\n","Test set: Average loss: 1.6278, Accuracy: 261/480 (54%)\n","\n","Train Epoch: 1 [86400/110534 (78%)]\tAll Loss: 5.6184\tTriple Loss(0): 2.2035\tClassification Loss: 1.2115\n","Train Epoch: 1 [86560/110534 (78%)]\tAll Loss: 3.5278\tTriple Loss(1): 0.4921\tClassification Loss: 2.5435\n","Train Epoch: 1 [86720/110534 (78%)]\tAll Loss: 2.0507\tTriple Loss(1): 0.1562\tClassification Loss: 1.7383\n","Train Epoch: 1 [86880/110534 (79%)]\tAll Loss: 2.2720\tTriple Loss(1): 0.4076\tClassification Loss: 1.4569\n","Train Epoch: 1 [87040/110534 (79%)]\tAll Loss: 1.7838\tTriple Loss(1): 0.2876\tClassification Loss: 1.2085\n","Train Epoch: 1 [87200/110534 (79%)]\tAll Loss: 2.3008\tTriple Loss(1): 0.0000\tClassification Loss: 2.3008\n","Train Epoch: 1 [87360/110534 (79%)]\tAll Loss: 3.2363\tTriple Loss(1): 0.2639\tClassification Loss: 2.7085\n","Train Epoch: 1 [87520/110534 (79%)]\tAll Loss: 1.3571\tTriple Loss(1): 0.0338\tClassification Loss: 1.2895\n","Train Epoch: 1 [87680/110534 (79%)]\tAll Loss: 2.1168\tTriple Loss(0): 0.0000\tClassification Loss: 2.1168\n","Train Epoch: 1 [87840/110534 (79%)]\tAll Loss: 1.5228\tTriple Loss(1): 0.1882\tClassification Loss: 1.1463\n","\n","Test set: Average loss: 1.6210, Accuracy: 253/480 (53%)\n","\n","Train Epoch: 1 [88000/110534 (80%)]\tAll Loss: 1.4547\tTriple Loss(1): 0.0412\tClassification Loss: 1.3723\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_5500.pth.tar\n","Train Epoch: 1 [88160/110534 (80%)]\tAll Loss: 1.9666\tTriple Loss(0): 0.0000\tClassification Loss: 1.9666\n","Train Epoch: 1 [88320/110534 (80%)]\tAll Loss: 3.0943\tTriple Loss(1): 0.6219\tClassification Loss: 1.8505\n","Train Epoch: 1 [88480/110534 (80%)]\tAll Loss: 1.6250\tTriple Loss(1): 0.1265\tClassification Loss: 1.3721\n","Train Epoch: 1 [88640/110534 (80%)]\tAll Loss: 1.5524\tTriple Loss(0): 0.0000\tClassification Loss: 1.5524\n","Train Epoch: 1 [88800/110534 (80%)]\tAll Loss: 1.3054\tTriple Loss(1): 0.0916\tClassification Loss: 1.1223\n","Train Epoch: 1 [88960/110534 (80%)]\tAll Loss: 2.0717\tTriple Loss(1): 0.1082\tClassification Loss: 1.8554\n","Train Epoch: 1 [89120/110534 (81%)]\tAll Loss: 1.4399\tTriple Loss(0): 0.0000\tClassification Loss: 1.4399\n","Train Epoch: 1 [89280/110534 (81%)]\tAll Loss: 1.9232\tTriple Loss(1): 0.1362\tClassification Loss: 1.6509\n","Train Epoch: 1 [89440/110534 (81%)]\tAll Loss: 2.3254\tTriple Loss(1): 0.2393\tClassification Loss: 1.8469\n","\n","Test set: Average loss: 1.6355, Accuracy: 264/480 (55%)\n","\n","Train Epoch: 1 [89600/110534 (81%)]\tAll Loss: 1.4083\tTriple Loss(0): 0.0000\tClassification Loss: 1.4083\n","Train Epoch: 1 [89760/110534 (81%)]\tAll Loss: 2.3964\tTriple Loss(1): 0.1092\tClassification Loss: 2.1779\n","Train Epoch: 1 [89920/110534 (81%)]\tAll Loss: 2.0658\tTriple Loss(1): 0.2022\tClassification Loss: 1.6613\n","Train Epoch: 1 [90080/110534 (81%)]\tAll Loss: 2.0423\tTriple Loss(1): 0.0305\tClassification Loss: 1.9813\n","Train Epoch: 1 [90240/110534 (82%)]\tAll Loss: 2.0921\tTriple Loss(1): 0.2577\tClassification Loss: 1.5767\n","Train Epoch: 1 [90400/110534 (82%)]\tAll Loss: 2.1360\tTriple Loss(1): 0.1612\tClassification Loss: 1.8135\n","Train Epoch: 1 [90560/110534 (82%)]\tAll Loss: 1.9587\tTriple Loss(1): 0.2731\tClassification Loss: 1.4125\n","Train Epoch: 1 [90720/110534 (82%)]\tAll Loss: 2.1984\tTriple Loss(1): 0.2174\tClassification Loss: 1.7637\n","Train Epoch: 1 [90880/110534 (82%)]\tAll Loss: 1.8629\tTriple Loss(1): 0.1947\tClassification Loss: 1.4736\n","Train Epoch: 1 [91040/110534 (82%)]\tAll Loss: 1.6127\tTriple Loss(0): 0.0000\tClassification Loss: 1.6127\n","\n","Test set: Average loss: 1.6424, Accuracy: 264/480 (55%)\n","\n","Train Epoch: 1 [91200/110534 (83%)]\tAll Loss: 2.5497\tTriple Loss(1): 0.5052\tClassification Loss: 1.5394\n","Train Epoch: 1 [91360/110534 (83%)]\tAll Loss: 4.5371\tTriple Loss(0): 1.5734\tClassification Loss: 1.3903\n","Train Epoch: 1 [91520/110534 (83%)]\tAll Loss: 1.5642\tTriple Loss(0): 0.0000\tClassification Loss: 1.5642\n","Train Epoch: 1 [91680/110534 (83%)]\tAll Loss: 1.8908\tTriple Loss(0): 0.0000\tClassification Loss: 1.8908\n","Train Epoch: 1 [91840/110534 (83%)]\tAll Loss: 1.9284\tTriple Loss(1): 0.2488\tClassification Loss: 1.4309\n","Train Epoch: 1 [92000/110534 (83%)]\tAll Loss: 1.8916\tTriple Loss(1): 0.1690\tClassification Loss: 1.5535\n","Train Epoch: 1 [92160/110534 (83%)]\tAll Loss: 1.9216\tTriple Loss(1): 0.0869\tClassification Loss: 1.7477\n","Train Epoch: 1 [92320/110534 (84%)]\tAll Loss: 2.8723\tTriple Loss(1): 0.3339\tClassification Loss: 2.2045\n","Train Epoch: 1 [92480/110534 (84%)]\tAll Loss: 2.9791\tTriple Loss(1): 0.7679\tClassification Loss: 1.4434\n","Train Epoch: 1 [92640/110534 (84%)]\tAll Loss: 1.8334\tTriple Loss(1): 0.0000\tClassification Loss: 1.8334\n","\n","Test set: Average loss: 1.6033, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 1 [92800/110534 (84%)]\tAll Loss: 2.2757\tTriple Loss(1): 0.2994\tClassification Loss: 1.6769\n","Train Epoch: 1 [92960/110534 (84%)]\tAll Loss: 2.6117\tTriple Loss(1): 0.3755\tClassification Loss: 1.8607\n","Train Epoch: 1 [93120/110534 (84%)]\tAll Loss: 2.3034\tTriple Loss(1): 0.2192\tClassification Loss: 1.8650\n","Train Epoch: 1 [93280/110534 (84%)]\tAll Loss: 2.0858\tTriple Loss(1): 0.2118\tClassification Loss: 1.6622\n","Train Epoch: 1 [93440/110534 (85%)]\tAll Loss: 1.5995\tTriple Loss(0): 0.0000\tClassification Loss: 1.5995\n","Train Epoch: 1 [93600/110534 (85%)]\tAll Loss: 1.7617\tTriple Loss(1): 0.1755\tClassification Loss: 1.4106\n","Train Epoch: 1 [93760/110534 (85%)]\tAll Loss: 1.7167\tTriple Loss(1): 0.0931\tClassification Loss: 1.5305\n","Train Epoch: 1 [93920/110534 (85%)]\tAll Loss: 2.4095\tTriple Loss(1): 0.2029\tClassification Loss: 2.0038\n","Train Epoch: 1 [94080/110534 (85%)]\tAll Loss: 1.3689\tTriple Loss(0): 0.0341\tClassification Loss: 1.3006\n","Train Epoch: 1 [94240/110534 (85%)]\tAll Loss: 1.6628\tTriple Loss(1): 0.0969\tClassification Loss: 1.4691\n","\n","Test set: Average loss: 1.6082, Accuracy: 268/480 (56%)\n","\n","Train Epoch: 1 [94400/110534 (85%)]\tAll Loss: 1.8478\tTriple Loss(1): 0.2604\tClassification Loss: 1.3270\n","Train Epoch: 1 [94560/110534 (86%)]\tAll Loss: 1.7593\tTriple Loss(1): 0.3071\tClassification Loss: 1.1451\n","Train Epoch: 1 [94720/110534 (86%)]\tAll Loss: 2.0654\tTriple Loss(1): 0.2199\tClassification Loss: 1.6257\n","Train Epoch: 1 [94880/110534 (86%)]\tAll Loss: 2.2226\tTriple Loss(1): 0.2910\tClassification Loss: 1.6406\n","Train Epoch: 1 [95040/110534 (86%)]\tAll Loss: 2.5904\tTriple Loss(1): 0.4328\tClassification Loss: 1.7248\n","Train Epoch: 1 [95200/110534 (86%)]\tAll Loss: 2.4635\tTriple Loss(1): 0.2799\tClassification Loss: 1.9036\n","Train Epoch: 1 [95360/110534 (86%)]\tAll Loss: 2.2395\tTriple Loss(1): 0.0457\tClassification Loss: 2.1481\n","Train Epoch: 1 [95520/110534 (86%)]\tAll Loss: 1.6714\tTriple Loss(1): 0.0410\tClassification Loss: 1.5895\n","Train Epoch: 1 [95680/110534 (87%)]\tAll Loss: 2.0420\tTriple Loss(1): 0.0000\tClassification Loss: 2.0420\n","Train Epoch: 1 [95840/110534 (87%)]\tAll Loss: 1.7844\tTriple Loss(0): 0.0000\tClassification Loss: 1.7844\n","\n","Test set: Average loss: 1.5934, Accuracy: 264/480 (55%)\n","\n","Train Epoch: 1 [96000/110534 (87%)]\tAll Loss: 1.4245\tTriple Loss(1): 0.0645\tClassification Loss: 1.2954\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_6000.pth.tar\n","Train Epoch: 1 [96160/110534 (87%)]\tAll Loss: 2.2365\tTriple Loss(1): 0.3471\tClassification Loss: 1.5423\n","Train Epoch: 1 [96320/110534 (87%)]\tAll Loss: 4.3945\tTriple Loss(0): 1.1854\tClassification Loss: 2.0238\n","Train Epoch: 1 [96480/110534 (87%)]\tAll Loss: 2.9505\tTriple Loss(1): 0.3420\tClassification Loss: 2.2665\n","Train Epoch: 1 [96640/110534 (87%)]\tAll Loss: 1.8154\tTriple Loss(1): 0.1799\tClassification Loss: 1.4556\n","Train Epoch: 1 [96800/110534 (88%)]\tAll Loss: 2.6817\tTriple Loss(1): 0.2699\tClassification Loss: 2.1419\n","Train Epoch: 1 [96960/110534 (88%)]\tAll Loss: 1.5593\tTriple Loss(0): 0.0000\tClassification Loss: 1.5593\n","Train Epoch: 1 [97120/110534 (88%)]\tAll Loss: 2.0663\tTriple Loss(1): 0.0724\tClassification Loss: 1.9215\n","Train Epoch: 1 [97280/110534 (88%)]\tAll Loss: 2.3018\tTriple Loss(1): 0.4528\tClassification Loss: 1.3961\n","Train Epoch: 1 [97440/110534 (88%)]\tAll Loss: 1.1405\tTriple Loss(0): 0.0000\tClassification Loss: 1.1405\n","\n","Test set: Average loss: 1.6128, Accuracy: 257/480 (54%)\n","\n","Train Epoch: 1 [97600/110534 (88%)]\tAll Loss: 1.3649\tTriple Loss(0): 0.0000\tClassification Loss: 1.3649\n","Train Epoch: 1 [97760/110534 (88%)]\tAll Loss: 2.5929\tTriple Loss(1): 0.5203\tClassification Loss: 1.5524\n","Train Epoch: 1 [97920/110534 (89%)]\tAll Loss: 2.2973\tTriple Loss(1): 0.4356\tClassification Loss: 1.4261\n","Train Epoch: 1 [98080/110534 (89%)]\tAll Loss: 2.1050\tTriple Loss(1): 0.2627\tClassification Loss: 1.5797\n","Train Epoch: 1 [98240/110534 (89%)]\tAll Loss: 1.4664\tTriple Loss(1): 0.1837\tClassification Loss: 1.0990\n","Train Epoch: 1 [98400/110534 (89%)]\tAll Loss: 1.4343\tTriple Loss(0): 0.0000\tClassification Loss: 1.4343\n","Train Epoch: 1 [98560/110534 (89%)]\tAll Loss: 0.9048\tTriple Loss(0): 0.0000\tClassification Loss: 0.9048\n","Train Epoch: 1 [98720/110534 (89%)]\tAll Loss: 1.9872\tTriple Loss(1): 0.4019\tClassification Loss: 1.1834\n","Train Epoch: 1 [98880/110534 (89%)]\tAll Loss: 1.2789\tTriple Loss(1): 0.0912\tClassification Loss: 1.0966\n","Train Epoch: 1 [99040/110534 (90%)]\tAll Loss: 2.8115\tTriple Loss(0): 0.7184\tClassification Loss: 1.3746\n","\n","Test set: Average loss: 1.6044, Accuracy: 266/480 (55%)\n","\n","Train Epoch: 1 [99200/110534 (90%)]\tAll Loss: 2.4871\tTriple Loss(1): 0.5733\tClassification Loss: 1.3405\n","Train Epoch: 1 [99360/110534 (90%)]\tAll Loss: 2.0345\tTriple Loss(1): 0.2623\tClassification Loss: 1.5100\n","Train Epoch: 1 [99520/110534 (90%)]\tAll Loss: 1.9514\tTriple Loss(1): 0.1109\tClassification Loss: 1.7296\n","Train Epoch: 1 [99680/110534 (90%)]\tAll Loss: 1.7969\tTriple Loss(1): 0.0415\tClassification Loss: 1.7140\n","Train Epoch: 1 [99840/110534 (90%)]\tAll Loss: 2.4856\tTriple Loss(1): 0.1531\tClassification Loss: 2.1793\n","Train Epoch: 1 [100000/110534 (90%)]\tAll Loss: 1.9550\tTriple Loss(1): 0.1473\tClassification Loss: 1.6605\n","Train Epoch: 1 [100160/110534 (91%)]\tAll Loss: 2.5739\tTriple Loss(1): 0.3463\tClassification Loss: 1.8814\n","Train Epoch: 1 [100320/110534 (91%)]\tAll Loss: 2.2523\tTriple Loss(1): 0.2464\tClassification Loss: 1.7595\n","Train Epoch: 1 [100480/110534 (91%)]\tAll Loss: 2.2476\tTriple Loss(1): 0.1409\tClassification Loss: 1.9659\n","Train Epoch: 1 [100640/110534 (91%)]\tAll Loss: 2.5433\tTriple Loss(1): 0.4321\tClassification Loss: 1.6792\n","\n","Test set: Average loss: 1.5923, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 1 [100800/110534 (91%)]\tAll Loss: 2.5827\tTriple Loss(1): 0.1701\tClassification Loss: 2.2426\n","Train Epoch: 1 [100960/110534 (91%)]\tAll Loss: 1.7663\tTriple Loss(1): 0.0796\tClassification Loss: 1.6072\n","Train Epoch: 1 [101120/110534 (91%)]\tAll Loss: 1.6871\tTriple Loss(1): 0.0498\tClassification Loss: 1.5875\n","Train Epoch: 1 [101280/110534 (92%)]\tAll Loss: 1.7416\tTriple Loss(1): 0.1249\tClassification Loss: 1.4918\n","Train Epoch: 1 [101440/110534 (92%)]\tAll Loss: 2.2660\tTriple Loss(1): 0.4290\tClassification Loss: 1.4079\n","Train Epoch: 1 [101600/110534 (92%)]\tAll Loss: 1.3595\tTriple Loss(0): 0.0000\tClassification Loss: 1.3595\n","Train Epoch: 1 [101760/110534 (92%)]\tAll Loss: 2.0243\tTriple Loss(1): 0.2550\tClassification Loss: 1.5143\n","Train Epoch: 1 [101920/110534 (92%)]\tAll Loss: 2.4287\tTriple Loss(1): 0.4727\tClassification Loss: 1.4833\n","Train Epoch: 1 [102080/110534 (92%)]\tAll Loss: 1.8362\tTriple Loss(1): 0.0965\tClassification Loss: 1.6432\n","Train Epoch: 1 [102240/110534 (92%)]\tAll Loss: 1.9576\tTriple Loss(1): 0.1912\tClassification Loss: 1.5752\n","\n","Test set: Average loss: 1.6372, Accuracy: 257/480 (54%)\n","\n","Train Epoch: 1 [102400/110534 (93%)]\tAll Loss: 1.7130\tTriple Loss(1): 0.1789\tClassification Loss: 1.3553\n","Train Epoch: 1 [102560/110534 (93%)]\tAll Loss: 2.4498\tTriple Loss(1): 0.4023\tClassification Loss: 1.6452\n","Train Epoch: 1 [102720/110534 (93%)]\tAll Loss: 0.9897\tTriple Loss(0): 0.0000\tClassification Loss: 0.9897\n","Train Epoch: 1 [102880/110534 (93%)]\tAll Loss: 1.3747\tTriple Loss(0): 0.0000\tClassification Loss: 1.3747\n","Train Epoch: 1 [103040/110534 (93%)]\tAll Loss: 1.8016\tTriple Loss(0): 0.0000\tClassification Loss: 1.8016\n","Train Epoch: 1 [103200/110534 (93%)]\tAll Loss: 3.2160\tTriple Loss(1): 0.3827\tClassification Loss: 2.4507\n","Train Epoch: 1 [103360/110534 (94%)]\tAll Loss: 1.4491\tTriple Loss(0): 0.0000\tClassification Loss: 1.4491\n","Train Epoch: 1 [103520/110534 (94%)]\tAll Loss: 2.1159\tTriple Loss(1): 0.0660\tClassification Loss: 1.9840\n","Train Epoch: 1 [103680/110534 (94%)]\tAll Loss: 1.7949\tTriple Loss(1): 0.1492\tClassification Loss: 1.4964\n","Train Epoch: 1 [103840/110534 (94%)]\tAll Loss: 2.0146\tTriple Loss(1): 0.3883\tClassification Loss: 1.2380\n","\n","Test set: Average loss: 1.5880, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 1 [104000/110534 (94%)]\tAll Loss: 1.9373\tTriple Loss(1): 0.1885\tClassification Loss: 1.5602\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_6500.pth.tar\n","Train Epoch: 1 [104160/110534 (94%)]\tAll Loss: 1.5387\tTriple Loss(0): 0.0000\tClassification Loss: 1.5387\n","Train Epoch: 1 [104320/110534 (94%)]\tAll Loss: 1.8088\tTriple Loss(1): 0.0561\tClassification Loss: 1.6967\n","Train Epoch: 1 [104480/110534 (95%)]\tAll Loss: 2.8519\tTriple Loss(1): 0.5918\tClassification Loss: 1.6683\n","Train Epoch: 1 [104640/110534 (95%)]\tAll Loss: 1.7273\tTriple Loss(0): 0.0000\tClassification Loss: 1.7273\n","Train Epoch: 1 [104800/110534 (95%)]\tAll Loss: 1.6782\tTriple Loss(0): 0.0000\tClassification Loss: 1.6782\n","Train Epoch: 1 [104960/110534 (95%)]\tAll Loss: 2.8218\tTriple Loss(1): 0.4863\tClassification Loss: 1.8492\n","Train Epoch: 1 [105120/110534 (95%)]\tAll Loss: 2.0762\tTriple Loss(1): 0.2005\tClassification Loss: 1.6753\n","Train Epoch: 1 [105280/110534 (95%)]\tAll Loss: 2.9477\tTriple Loss(1): 0.7779\tClassification Loss: 1.3920\n","Train Epoch: 1 [105440/110534 (95%)]\tAll Loss: 2.2124\tTriple Loss(1): 0.4168\tClassification Loss: 1.3787\n","\n","Test set: Average loss: 1.5886, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 1 [105600/110534 (96%)]\tAll Loss: 2.1995\tTriple Loss(1): 0.1948\tClassification Loss: 1.8099\n","Train Epoch: 1 [105760/110534 (96%)]\tAll Loss: 2.2962\tTriple Loss(1): 0.5372\tClassification Loss: 1.2217\n","Train Epoch: 1 [105920/110534 (96%)]\tAll Loss: 1.9806\tTriple Loss(1): 0.0619\tClassification Loss: 1.8567\n","Train Epoch: 1 [106080/110534 (96%)]\tAll Loss: 1.6097\tTriple Loss(1): 0.2325\tClassification Loss: 1.1448\n","Train Epoch: 1 [106240/110534 (96%)]\tAll Loss: 2.7950\tTriple Loss(1): 0.4766\tClassification Loss: 1.8417\n","Train Epoch: 1 [106400/110534 (96%)]\tAll Loss: 1.7793\tTriple Loss(0): 0.0000\tClassification Loss: 1.7793\n","Train Epoch: 1 [106560/110534 (96%)]\tAll Loss: 1.4143\tTriple Loss(1): 0.0803\tClassification Loss: 1.2537\n","Train Epoch: 1 [106720/110534 (97%)]\tAll Loss: 2.1279\tTriple Loss(1): 0.2766\tClassification Loss: 1.5748\n","Train Epoch: 1 [106880/110534 (97%)]\tAll Loss: 2.1051\tTriple Loss(1): 0.2804\tClassification Loss: 1.5443\n","Train Epoch: 1 [107040/110534 (97%)]\tAll Loss: 1.9129\tTriple Loss(1): 0.0071\tClassification Loss: 1.8986\n","\n","Test set: Average loss: 1.6159, Accuracy: 262/480 (55%)\n","\n","Train Epoch: 1 [107200/110534 (97%)]\tAll Loss: 2.1070\tTriple Loss(1): 0.2813\tClassification Loss: 1.5444\n","Train Epoch: 1 [107360/110534 (97%)]\tAll Loss: 1.3893\tTriple Loss(0): 0.0000\tClassification Loss: 1.3893\n","Train Epoch: 1 [107520/110534 (97%)]\tAll Loss: 1.2737\tTriple Loss(0): 0.0000\tClassification Loss: 1.2737\n","Train Epoch: 1 [107680/110534 (97%)]\tAll Loss: 2.6656\tTriple Loss(1): 0.2031\tClassification Loss: 2.2595\n","Train Epoch: 1 [107840/110534 (98%)]\tAll Loss: 1.5672\tTriple Loss(0): 0.0000\tClassification Loss: 1.5672\n","Train Epoch: 1 [108000/110534 (98%)]\tAll Loss: 1.7925\tTriple Loss(1): 0.2255\tClassification Loss: 1.3415\n","Train Epoch: 1 [108160/110534 (98%)]\tAll Loss: 2.6362\tTriple Loss(1): 0.4451\tClassification Loss: 1.7460\n","Train Epoch: 1 [108320/110534 (98%)]\tAll Loss: 1.9369\tTriple Loss(1): 0.0082\tClassification Loss: 1.9206\n","Train Epoch: 1 [108480/110534 (98%)]\tAll Loss: 1.1388\tTriple Loss(1): 0.0444\tClassification Loss: 1.0501\n","Train Epoch: 1 [108640/110534 (98%)]\tAll Loss: 1.8608\tTriple Loss(1): 0.3401\tClassification Loss: 1.1806\n","\n","Test set: Average loss: 1.6182, Accuracy: 267/480 (56%)\n","\n","Train Epoch: 1 [108800/110534 (98%)]\tAll Loss: 2.1634\tTriple Loss(1): 0.1385\tClassification Loss: 1.8865\n","Train Epoch: 1 [108960/110534 (99%)]\tAll Loss: 2.1224\tTriple Loss(1): 0.3186\tClassification Loss: 1.4852\n","Train Epoch: 1 [109120/110534 (99%)]\tAll Loss: 1.8259\tTriple Loss(1): 0.3503\tClassification Loss: 1.1253\n","Train Epoch: 1 [109280/110534 (99%)]\tAll Loss: 2.7598\tTriple Loss(1): 0.2992\tClassification Loss: 2.1615\n","Train Epoch: 1 [109440/110534 (99%)]\tAll Loss: 2.2880\tTriple Loss(1): 0.2145\tClassification Loss: 1.8590\n","Train Epoch: 1 [109600/110534 (99%)]\tAll Loss: 2.4262\tTriple Loss(1): 0.3957\tClassification Loss: 1.6349\n","Train Epoch: 1 [109760/110534 (99%)]\tAll Loss: 2.0283\tTriple Loss(1): 0.3547\tClassification Loss: 1.3189\n","Train Epoch: 1 [109920/110534 (99%)]\tAll Loss: 1.5791\tTriple Loss(1): 0.1057\tClassification Loss: 1.3677\n","Train Epoch: 1 [110080/110534 (100%)]\tAll Loss: 1.9497\tTriple Loss(1): 0.0379\tClassification Loss: 1.8739\n","Train Epoch: 1 [110240/110534 (100%)]\tAll Loss: 1.7699\tTriple Loss(1): 0.0448\tClassification Loss: 1.6802\n","\n","Test set: Average loss: 1.6080, Accuracy: 266/480 (55%)\n","\n","Train Epoch: 1 [110400/110534 (100%)]\tAll Loss: 2.9124\tTriple Loss(1): 0.3272\tClassification Loss: 2.2581\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_final.pth.tar\n","\n","Test set: Average loss: 1.6307, Accuracy: 264/480 (55%)\n","\n","Train Epoch: 2 [0/110534 (0%)]\tAll Loss: 1.8613\tTriple Loss(0): 0.0000\tClassification Loss: 1.8613\n","Train Epoch: 2 [160/110534 (0%)]\tAll Loss: 2.0272\tTriple Loss(1): 0.1774\tClassification Loss: 1.6725\n","Train Epoch: 2 [320/110534 (0%)]\tAll Loss: 8.1190\tTriple Loss(0): 3.2554\tClassification Loss: 1.6082\n","Train Epoch: 2 [480/110534 (0%)]\tAll Loss: 2.4909\tTriple Loss(1): 0.4087\tClassification Loss: 1.6735\n","Train Epoch: 2 [640/110534 (1%)]\tAll Loss: 1.9952\tTriple Loss(0): 0.1760\tClassification Loss: 1.6431\n","Train Epoch: 2 [800/110534 (1%)]\tAll Loss: 1.8366\tTriple Loss(1): 0.1631\tClassification Loss: 1.5104\n","Train Epoch: 2 [960/110534 (1%)]\tAll Loss: 1.4717\tTriple Loss(1): 0.0741\tClassification Loss: 1.3234\n","Train Epoch: 2 [1120/110534 (1%)]\tAll Loss: 2.1275\tTriple Loss(0): 0.0000\tClassification Loss: 2.1275\n","Train Epoch: 2 [1280/110534 (1%)]\tAll Loss: 2.5542\tTriple Loss(1): 0.2746\tClassification Loss: 2.0051\n","Train Epoch: 2 [1440/110534 (1%)]\tAll Loss: 2.2058\tTriple Loss(1): 0.2755\tClassification Loss: 1.6547\n","\n","Test set: Average loss: 1.5904, Accuracy: 266/480 (55%)\n","\n","Train Epoch: 2 [1600/110534 (1%)]\tAll Loss: 8.4666\tTriple Loss(0): 3.5746\tClassification Loss: 1.3174\n","Train Epoch: 2 [1760/110534 (2%)]\tAll Loss: 1.5643\tTriple Loss(0): 0.0000\tClassification Loss: 1.5643\n","Train Epoch: 2 [1920/110534 (2%)]\tAll Loss: 1.7785\tTriple Loss(1): 0.0899\tClassification Loss: 1.5988\n","Train Epoch: 2 [2080/110534 (2%)]\tAll Loss: 2.2515\tTriple Loss(0): 0.0000\tClassification Loss: 2.2515\n","Train Epoch: 2 [2240/110534 (2%)]\tAll Loss: 2.3900\tTriple Loss(1): 0.1828\tClassification Loss: 2.0244\n","Train Epoch: 2 [2400/110534 (2%)]\tAll Loss: 1.6833\tTriple Loss(0): 0.0000\tClassification Loss: 1.6833\n","Train Epoch: 2 [2560/110534 (2%)]\tAll Loss: 1.1006\tTriple Loss(1): 0.0380\tClassification Loss: 1.0246\n","Train Epoch: 2 [2720/110534 (2%)]\tAll Loss: 1.2927\tTriple Loss(1): 0.0272\tClassification Loss: 1.2383\n","Train Epoch: 2 [2880/110534 (3%)]\tAll Loss: 2.8929\tTriple Loss(1): 0.2725\tClassification Loss: 2.3480\n","Train Epoch: 2 [3040/110534 (3%)]\tAll Loss: 1.6230\tTriple Loss(1): 0.1950\tClassification Loss: 1.2330\n","\n","Test set: Average loss: 1.6093, Accuracy: 254/480 (53%)\n","\n","Train Epoch: 2 [3200/110534 (3%)]\tAll Loss: 1.0366\tTriple Loss(0): 0.0000\tClassification Loss: 1.0366\n","Train Epoch: 2 [3360/110534 (3%)]\tAll Loss: 2.8029\tTriple Loss(1): 0.6323\tClassification Loss: 1.5383\n","Train Epoch: 2 [3520/110534 (3%)]\tAll Loss: 2.5856\tTriple Loss(1): 0.2886\tClassification Loss: 2.0084\n","Train Epoch: 2 [3680/110534 (3%)]\tAll Loss: 1.9019\tTriple Loss(1): 0.1836\tClassification Loss: 1.5347\n","Train Epoch: 2 [3840/110534 (3%)]\tAll Loss: 2.0821\tTriple Loss(1): 0.1916\tClassification Loss: 1.6990\n","Train Epoch: 2 [4000/110534 (4%)]\tAll Loss: 2.3853\tTriple Loss(1): 0.3299\tClassification Loss: 1.7255\n","Train Epoch: 2 [4160/110534 (4%)]\tAll Loss: 1.4673\tTriple Loss(0): 0.0000\tClassification Loss: 1.4673\n","Train Epoch: 2 [4320/110534 (4%)]\tAll Loss: 2.1905\tTriple Loss(1): 0.2196\tClassification Loss: 1.7513\n","Train Epoch: 2 [4480/110534 (4%)]\tAll Loss: 3.1499\tTriple Loss(1): 0.4180\tClassification Loss: 2.3140\n","Train Epoch: 2 [4640/110534 (4%)]\tAll Loss: 1.6359\tTriple Loss(0): 0.0000\tClassification Loss: 1.6359\n","\n","Test set: Average loss: 1.6240, Accuracy: 256/480 (53%)\n","\n","Train Epoch: 2 [4800/110534 (4%)]\tAll Loss: 2.3815\tTriple Loss(1): 0.2716\tClassification Loss: 1.8383\n","Train Epoch: 2 [4960/110534 (4%)]\tAll Loss: 2.2083\tTriple Loss(1): 0.3954\tClassification Loss: 1.4174\n","Train Epoch: 2 [5120/110534 (5%)]\tAll Loss: 2.3390\tTriple Loss(1): 0.2883\tClassification Loss: 1.7624\n","Train Epoch: 2 [5280/110534 (5%)]\tAll Loss: 1.4291\tTriple Loss(1): 0.1314\tClassification Loss: 1.1663\n","Train Epoch: 2 [5440/110534 (5%)]\tAll Loss: 1.6385\tTriple Loss(1): 0.0586\tClassification Loss: 1.5213\n","Train Epoch: 2 [5600/110534 (5%)]\tAll Loss: 1.8429\tTriple Loss(1): 0.2354\tClassification Loss: 1.3721\n","Train Epoch: 2 [5760/110534 (5%)]\tAll Loss: 1.2374\tTriple Loss(1): 0.0696\tClassification Loss: 1.0981\n","Train Epoch: 2 [5920/110534 (5%)]\tAll Loss: 2.1585\tTriple Loss(1): 0.2396\tClassification Loss: 1.6793\n","Train Epoch: 2 [6080/110534 (6%)]\tAll Loss: 1.8026\tTriple Loss(1): 0.2112\tClassification Loss: 1.3802\n","Train Epoch: 2 [6240/110534 (6%)]\tAll Loss: 1.3471\tTriple Loss(1): 0.0000\tClassification Loss: 1.3471\n","\n","Test set: Average loss: 1.6099, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 2 [6400/110534 (6%)]\tAll Loss: 1.4944\tTriple Loss(1): 0.0638\tClassification Loss: 1.3669\n","Train Epoch: 2 [6560/110534 (6%)]\tAll Loss: 1.7194\tTriple Loss(0): 0.0000\tClassification Loss: 1.7194\n","Train Epoch: 2 [6720/110534 (6%)]\tAll Loss: 8.3955\tTriple Loss(0): 3.2828\tClassification Loss: 1.8298\n","Train Epoch: 2 [6880/110534 (6%)]\tAll Loss: 1.1902\tTriple Loss(1): 0.0052\tClassification Loss: 1.1798\n","Train Epoch: 2 [7040/110534 (6%)]\tAll Loss: 1.5339\tTriple Loss(1): 0.1056\tClassification Loss: 1.3228\n","Train Epoch: 2 [7200/110534 (7%)]\tAll Loss: 3.0335\tTriple Loss(1): 0.5011\tClassification Loss: 2.0314\n","Train Epoch: 2 [7360/110534 (7%)]\tAll Loss: 2.1137\tTriple Loss(1): 0.2952\tClassification Loss: 1.5232\n","Train Epoch: 2 [7520/110534 (7%)]\tAll Loss: 1.7849\tTriple Loss(0): 0.0000\tClassification Loss: 1.7849\n","Train Epoch: 2 [7680/110534 (7%)]\tAll Loss: 1.9133\tTriple Loss(1): 0.0911\tClassification Loss: 1.7310\n","Train Epoch: 2 [7840/110534 (7%)]\tAll Loss: 3.2880\tTriple Loss(1): 0.8039\tClassification Loss: 1.6802\n","\n","Test set: Average loss: 1.6019, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 2 [8000/110534 (7%)]\tAll Loss: 2.3337\tTriple Loss(1): 0.3273\tClassification Loss: 1.6791\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_500.pth.tar\n","Train Epoch: 2 [8160/110534 (7%)]\tAll Loss: 2.7284\tTriple Loss(1): 0.4983\tClassification Loss: 1.7319\n","Train Epoch: 2 [8320/110534 (8%)]\tAll Loss: 2.3611\tTriple Loss(1): 0.2014\tClassification Loss: 1.9583\n","Train Epoch: 2 [8480/110534 (8%)]\tAll Loss: 1.7363\tTriple Loss(1): 0.0118\tClassification Loss: 1.7127\n","Train Epoch: 2 [8640/110534 (8%)]\tAll Loss: 1.7069\tTriple Loss(1): 0.2187\tClassification Loss: 1.2695\n","Train Epoch: 2 [8800/110534 (8%)]\tAll Loss: 2.3636\tTriple Loss(1): 0.2427\tClassification Loss: 1.8782\n","Train Epoch: 2 [8960/110534 (8%)]\tAll Loss: 2.0030\tTriple Loss(1): 0.2463\tClassification Loss: 1.5104\n","Train Epoch: 2 [9120/110534 (8%)]\tAll Loss: 1.7774\tTriple Loss(1): 0.0000\tClassification Loss: 1.7774\n","Train Epoch: 2 [9280/110534 (8%)]\tAll Loss: 1.7249\tTriple Loss(1): 0.0741\tClassification Loss: 1.5767\n","Train Epoch: 2 [9440/110534 (9%)]\tAll Loss: 2.4719\tTriple Loss(1): 0.2838\tClassification Loss: 1.9042\n","\n","Test set: Average loss: 1.5906, Accuracy: 256/480 (53%)\n","\n","Train Epoch: 2 [9600/110534 (9%)]\tAll Loss: 2.1409\tTriple Loss(1): 0.2135\tClassification Loss: 1.7140\n","Train Epoch: 2 [9760/110534 (9%)]\tAll Loss: 2.0010\tTriple Loss(0): 0.0000\tClassification Loss: 2.0010\n","Train Epoch: 2 [9920/110534 (9%)]\tAll Loss: 1.9101\tTriple Loss(1): 0.0889\tClassification Loss: 1.7322\n","Train Epoch: 2 [10080/110534 (9%)]\tAll Loss: 1.6830\tTriple Loss(1): 0.2386\tClassification Loss: 1.2059\n","Train Epoch: 2 [10240/110534 (9%)]\tAll Loss: 1.7360\tTriple Loss(1): 0.0820\tClassification Loss: 1.5720\n","Train Epoch: 2 [10400/110534 (9%)]\tAll Loss: 2.1607\tTriple Loss(1): 0.1824\tClassification Loss: 1.7959\n","Train Epoch: 2 [10560/110534 (10%)]\tAll Loss: 2.0575\tTriple Loss(1): 0.1271\tClassification Loss: 1.8034\n","Train Epoch: 2 [10720/110534 (10%)]\tAll Loss: 2.1496\tTriple Loss(1): 0.2094\tClassification Loss: 1.7307\n","Train Epoch: 2 [10880/110534 (10%)]\tAll Loss: 1.5803\tTriple Loss(1): 0.1855\tClassification Loss: 1.2093\n","Train Epoch: 2 [11040/110534 (10%)]\tAll Loss: 2.6786\tTriple Loss(1): 0.2428\tClassification Loss: 2.1930\n","\n","Test set: Average loss: 1.5667, Accuracy: 272/480 (57%)\n","\n","Train Epoch: 2 [11200/110534 (10%)]\tAll Loss: 2.3815\tTriple Loss(1): 0.1530\tClassification Loss: 2.0755\n","Train Epoch: 2 [11360/110534 (10%)]\tAll Loss: 2.3051\tTriple Loss(1): 0.0282\tClassification Loss: 2.2487\n","Train Epoch: 2 [11520/110534 (10%)]\tAll Loss: 2.7528\tTriple Loss(1): 0.2768\tClassification Loss: 2.1991\n","Train Epoch: 2 [11680/110534 (11%)]\tAll Loss: 1.4798\tTriple Loss(1): 0.0000\tClassification Loss: 1.4798\n","Train Epoch: 2 [11840/110534 (11%)]\tAll Loss: 2.5201\tTriple Loss(1): 0.1614\tClassification Loss: 2.1973\n","Train Epoch: 2 [12000/110534 (11%)]\tAll Loss: 2.6015\tTriple Loss(1): 0.2808\tClassification Loss: 2.0400\n","Train Epoch: 2 [12160/110534 (11%)]\tAll Loss: 1.2322\tTriple Loss(1): 0.0727\tClassification Loss: 1.0869\n","Train Epoch: 2 [12320/110534 (11%)]\tAll Loss: 2.0850\tTriple Loss(1): 0.2583\tClassification Loss: 1.5684\n","Train Epoch: 2 [12480/110534 (11%)]\tAll Loss: 1.4069\tTriple Loss(1): 0.0000\tClassification Loss: 1.4069\n","Train Epoch: 2 [12640/110534 (11%)]\tAll Loss: 1.3678\tTriple Loss(0): 0.0000\tClassification Loss: 1.3678\n","\n","Test set: Average loss: 1.5772, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 2 [12800/110534 (12%)]\tAll Loss: 2.1820\tTriple Loss(1): 0.3823\tClassification Loss: 1.4174\n","Train Epoch: 2 [12960/110534 (12%)]\tAll Loss: 2.0062\tTriple Loss(1): 0.1953\tClassification Loss: 1.6156\n","Train Epoch: 2 [13120/110534 (12%)]\tAll Loss: 1.8712\tTriple Loss(1): 0.1215\tClassification Loss: 1.6282\n","Train Epoch: 2 [13280/110534 (12%)]\tAll Loss: 3.4272\tTriple Loss(1): 0.6599\tClassification Loss: 2.1074\n","Train Epoch: 2 [13440/110534 (12%)]\tAll Loss: 2.6376\tTriple Loss(1): 0.3939\tClassification Loss: 1.8497\n","Train Epoch: 2 [13600/110534 (12%)]\tAll Loss: 2.9708\tTriple Loss(1): 0.6701\tClassification Loss: 1.6306\n","Train Epoch: 2 [13760/110534 (12%)]\tAll Loss: 2.1299\tTriple Loss(1): 0.2980\tClassification Loss: 1.5339\n","Train Epoch: 2 [13920/110534 (13%)]\tAll Loss: 1.8164\tTriple Loss(1): 0.1746\tClassification Loss: 1.4671\n","Train Epoch: 2 [14080/110534 (13%)]\tAll Loss: 1.4805\tTriple Loss(1): 0.1467\tClassification Loss: 1.1871\n","Train Epoch: 2 [14240/110534 (13%)]\tAll Loss: 1.2347\tTriple Loss(0): 0.0000\tClassification Loss: 1.2347\n","\n","Test set: Average loss: 1.5787, Accuracy: 265/480 (55%)\n","\n","Train Epoch: 2 [14400/110534 (13%)]\tAll Loss: 2.2276\tTriple Loss(1): 0.1723\tClassification Loss: 1.8830\n","Train Epoch: 2 [14560/110534 (13%)]\tAll Loss: 0.9554\tTriple Loss(1): 0.1168\tClassification Loss: 0.7217\n","Train Epoch: 2 [14720/110534 (13%)]\tAll Loss: 1.7226\tTriple Loss(1): 0.0888\tClassification Loss: 1.5449\n","Train Epoch: 2 [14880/110534 (13%)]\tAll Loss: 2.1694\tTriple Loss(1): 0.2290\tClassification Loss: 1.7114\n","Train Epoch: 2 [15040/110534 (14%)]\tAll Loss: 1.8583\tTriple Loss(0): 0.0000\tClassification Loss: 1.8583\n","Train Epoch: 2 [15200/110534 (14%)]\tAll Loss: 1.9969\tTriple Loss(0): 0.0000\tClassification Loss: 1.9969\n","Train Epoch: 2 [15360/110534 (14%)]\tAll Loss: 1.7002\tTriple Loss(1): 0.3094\tClassification Loss: 1.0814\n","Train Epoch: 2 [15520/110534 (14%)]\tAll Loss: 3.0864\tTriple Loss(1): 0.5420\tClassification Loss: 2.0024\n","Train Epoch: 2 [15680/110534 (14%)]\tAll Loss: 2.3673\tTriple Loss(1): 0.3025\tClassification Loss: 1.7622\n","Train Epoch: 2 [15840/110534 (14%)]\tAll Loss: 2.0030\tTriple Loss(1): 0.1998\tClassification Loss: 1.6035\n","\n","Test set: Average loss: 1.5868, Accuracy: 271/480 (56%)\n","\n","Train Epoch: 2 [16000/110534 (14%)]\tAll Loss: 2.2377\tTriple Loss(1): 0.0157\tClassification Loss: 2.2062\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_1000.pth.tar\n","Train Epoch: 2 [16160/110534 (15%)]\tAll Loss: 1.5659\tTriple Loss(1): 0.2902\tClassification Loss: 0.9854\n","Train Epoch: 2 [16320/110534 (15%)]\tAll Loss: 1.9948\tTriple Loss(1): 0.4935\tClassification Loss: 1.0078\n","Train Epoch: 2 [16480/110534 (15%)]\tAll Loss: 1.4886\tTriple Loss(0): 0.0000\tClassification Loss: 1.4886\n","Train Epoch: 2 [16640/110534 (15%)]\tAll Loss: 2.7846\tTriple Loss(1): 0.4867\tClassification Loss: 1.8112\n","Train Epoch: 2 [16800/110534 (15%)]\tAll Loss: 1.7708\tTriple Loss(1): 0.0894\tClassification Loss: 1.5920\n","Train Epoch: 2 [16960/110534 (15%)]\tAll Loss: 9.8810\tTriple Loss(0): 3.9814\tClassification Loss: 1.9181\n","Train Epoch: 2 [17120/110534 (15%)]\tAll Loss: 2.6019\tTriple Loss(1): 0.4546\tClassification Loss: 1.6927\n","Train Epoch: 2 [17280/110534 (16%)]\tAll Loss: 2.2420\tTriple Loss(1): 0.0961\tClassification Loss: 2.0498\n","Train Epoch: 2 [17440/110534 (16%)]\tAll Loss: 1.6613\tTriple Loss(1): 0.2591\tClassification Loss: 1.1431\n","\n","Test set: Average loss: 1.5720, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 2 [17600/110534 (16%)]\tAll Loss: 2.2180\tTriple Loss(1): 0.5009\tClassification Loss: 1.2162\n","Train Epoch: 2 [17760/110534 (16%)]\tAll Loss: 2.1314\tTriple Loss(1): 0.1722\tClassification Loss: 1.7870\n","Train Epoch: 2 [17920/110534 (16%)]\tAll Loss: 2.1484\tTriple Loss(1): 0.2370\tClassification Loss: 1.6743\n","Train Epoch: 2 [18080/110534 (16%)]\tAll Loss: 1.8882\tTriple Loss(1): 0.2695\tClassification Loss: 1.3493\n","Train Epoch: 2 [18240/110534 (17%)]\tAll Loss: 1.5850\tTriple Loss(0): 0.0000\tClassification Loss: 1.5850\n","Train Epoch: 2 [18400/110534 (17%)]\tAll Loss: 1.3745\tTriple Loss(0): 0.0000\tClassification Loss: 1.3745\n","Train Epoch: 2 [18560/110534 (17%)]\tAll Loss: 2.1680\tTriple Loss(0): 0.0000\tClassification Loss: 2.1680\n","Train Epoch: 2 [18720/110534 (17%)]\tAll Loss: 1.9838\tTriple Loss(1): 0.0757\tClassification Loss: 1.8324\n","Train Epoch: 2 [18880/110534 (17%)]\tAll Loss: 2.3401\tTriple Loss(1): 0.4408\tClassification Loss: 1.4585\n","Train Epoch: 2 [19040/110534 (17%)]\tAll Loss: 1.2115\tTriple Loss(1): 0.1047\tClassification Loss: 1.0021\n","\n","Test set: Average loss: 1.5654, Accuracy: 266/480 (55%)\n","\n","Train Epoch: 2 [19200/110534 (17%)]\tAll Loss: 3.8770\tTriple Loss(1): 0.7435\tClassification Loss: 2.3900\n","Train Epoch: 2 [19360/110534 (18%)]\tAll Loss: 1.7758\tTriple Loss(1): 0.1301\tClassification Loss: 1.5157\n","Train Epoch: 2 [19520/110534 (18%)]\tAll Loss: 2.0893\tTriple Loss(1): 0.2902\tClassification Loss: 1.5089\n","Train Epoch: 2 [19680/110534 (18%)]\tAll Loss: 1.2419\tTriple Loss(1): 0.0315\tClassification Loss: 1.1789\n","Train Epoch: 2 [19840/110534 (18%)]\tAll Loss: 2.3904\tTriple Loss(1): 0.2500\tClassification Loss: 1.8904\n","Train Epoch: 2 [20000/110534 (18%)]\tAll Loss: 2.3499\tTriple Loss(1): 0.2260\tClassification Loss: 1.8978\n","Train Epoch: 2 [20160/110534 (18%)]\tAll Loss: 1.6665\tTriple Loss(0): 0.0000\tClassification Loss: 1.6665\n","Train Epoch: 2 [20320/110534 (18%)]\tAll Loss: 2.2994\tTriple Loss(1): 0.0977\tClassification Loss: 2.1040\n","Train Epoch: 2 [20480/110534 (19%)]\tAll Loss: 2.8666\tTriple Loss(1): 0.2798\tClassification Loss: 2.3070\n","Train Epoch: 2 [20640/110534 (19%)]\tAll Loss: 2.1705\tTriple Loss(1): 0.3149\tClassification Loss: 1.5408\n","\n","Test set: Average loss: 1.5692, Accuracy: 264/480 (55%)\n","\n","Train Epoch: 2 [20800/110534 (19%)]\tAll Loss: 1.7413\tTriple Loss(1): 0.0776\tClassification Loss: 1.5861\n","Train Epoch: 2 [20960/110534 (19%)]\tAll Loss: 3.4041\tTriple Loss(1): 0.9667\tClassification Loss: 1.4707\n","Train Epoch: 2 [21120/110534 (19%)]\tAll Loss: 1.2882\tTriple Loss(1): 0.1203\tClassification Loss: 1.0477\n","Train Epoch: 2 [21280/110534 (19%)]\tAll Loss: 1.9553\tTriple Loss(0): 0.0000\tClassification Loss: 1.9553\n","Train Epoch: 2 [21440/110534 (19%)]\tAll Loss: 2.5342\tTriple Loss(1): 0.5137\tClassification Loss: 1.5068\n","Train Epoch: 2 [21600/110534 (20%)]\tAll Loss: 2.4021\tTriple Loss(1): 0.3438\tClassification Loss: 1.7144\n","Train Epoch: 2 [21760/110534 (20%)]\tAll Loss: 1.3114\tTriple Loss(1): 0.0080\tClassification Loss: 1.2953\n","Train Epoch: 2 [21920/110534 (20%)]\tAll Loss: 1.2970\tTriple Loss(1): 0.0583\tClassification Loss: 1.1805\n","Train Epoch: 2 [22080/110534 (20%)]\tAll Loss: 1.4420\tTriple Loss(0): 0.0000\tClassification Loss: 1.4420\n","Train Epoch: 2 [22240/110534 (20%)]\tAll Loss: 2.5303\tTriple Loss(1): 0.4336\tClassification Loss: 1.6631\n","\n","Test set: Average loss: 1.5779, Accuracy: 262/480 (55%)\n","\n","Train Epoch: 2 [22400/110534 (20%)]\tAll Loss: 1.8096\tTriple Loss(1): 0.2133\tClassification Loss: 1.3830\n","Train Epoch: 2 [22560/110534 (20%)]\tAll Loss: 1.6350\tTriple Loss(0): 0.0000\tClassification Loss: 1.6350\n","Train Epoch: 2 [22720/110534 (21%)]\tAll Loss: 2.7390\tTriple Loss(1): 0.4567\tClassification Loss: 1.8255\n","Train Epoch: 2 [22880/110534 (21%)]\tAll Loss: 1.7918\tTriple Loss(1): 0.2795\tClassification Loss: 1.2328\n","Train Epoch: 2 [23040/110534 (21%)]\tAll Loss: 1.5468\tTriple Loss(1): 0.1701\tClassification Loss: 1.2066\n","Train Epoch: 2 [23200/110534 (21%)]\tAll Loss: 1.9338\tTriple Loss(1): 0.1396\tClassification Loss: 1.6546\n","Train Epoch: 2 [23360/110534 (21%)]\tAll Loss: 1.4225\tTriple Loss(1): 0.2271\tClassification Loss: 0.9682\n","Train Epoch: 2 [23520/110534 (21%)]\tAll Loss: 2.2196\tTriple Loss(1): 0.1417\tClassification Loss: 1.9362\n","Train Epoch: 2 [23680/110534 (21%)]\tAll Loss: 3.5369\tTriple Loss(1): 0.6588\tClassification Loss: 2.2193\n","Train Epoch: 2 [23840/110534 (22%)]\tAll Loss: 2.1071\tTriple Loss(1): 0.2586\tClassification Loss: 1.5900\n","\n","Test set: Average loss: 1.5899, Accuracy: 265/480 (55%)\n","\n","Train Epoch: 2 [24000/110534 (22%)]\tAll Loss: 2.2149\tTriple Loss(1): 0.2635\tClassification Loss: 1.6880\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_1500.pth.tar\n","Train Epoch: 2 [24160/110534 (22%)]\tAll Loss: 1.3762\tTriple Loss(0): 0.0000\tClassification Loss: 1.3762\n","Train Epoch: 2 [24320/110534 (22%)]\tAll Loss: 1.3889\tTriple Loss(1): 0.0419\tClassification Loss: 1.3052\n","Train Epoch: 2 [24480/110534 (22%)]\tAll Loss: 1.6826\tTriple Loss(1): 0.0395\tClassification Loss: 1.6037\n","Train Epoch: 2 [24640/110534 (22%)]\tAll Loss: 2.6923\tTriple Loss(1): 0.2768\tClassification Loss: 2.1387\n","Train Epoch: 2 [24800/110534 (22%)]\tAll Loss: 2.0432\tTriple Loss(1): 0.2759\tClassification Loss: 1.4914\n","Train Epoch: 2 [24960/110534 (23%)]\tAll Loss: 1.8587\tTriple Loss(1): 0.0199\tClassification Loss: 1.8190\n","Train Epoch: 2 [25120/110534 (23%)]\tAll Loss: 5.0729\tTriple Loss(0): 1.9277\tClassification Loss: 1.2174\n","Train Epoch: 2 [25280/110534 (23%)]\tAll Loss: 1.7057\tTriple Loss(1): 0.0442\tClassification Loss: 1.6174\n","Train Epoch: 2 [25440/110534 (23%)]\tAll Loss: 1.4306\tTriple Loss(0): 0.0000\tClassification Loss: 1.4306\n","\n","Test set: Average loss: 1.6013, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 2 [25600/110534 (23%)]\tAll Loss: 1.4395\tTriple Loss(0): 0.0000\tClassification Loss: 1.4395\n","Train Epoch: 2 [25760/110534 (23%)]\tAll Loss: 1.1971\tTriple Loss(0): 0.0000\tClassification Loss: 1.1971\n","Train Epoch: 2 [25920/110534 (23%)]\tAll Loss: 1.1190\tTriple Loss(1): 0.0655\tClassification Loss: 0.9880\n","Train Epoch: 2 [26080/110534 (24%)]\tAll Loss: 1.4804\tTriple Loss(1): 0.2333\tClassification Loss: 1.0138\n","Train Epoch: 2 [26240/110534 (24%)]\tAll Loss: 1.7914\tTriple Loss(1): 0.0993\tClassification Loss: 1.5928\n","Train Epoch: 2 [26400/110534 (24%)]\tAll Loss: 1.5165\tTriple Loss(0): 0.0000\tClassification Loss: 1.5165\n","Train Epoch: 2 [26560/110534 (24%)]\tAll Loss: 2.3186\tTriple Loss(1): 0.1765\tClassification Loss: 1.9656\n","Train Epoch: 2 [26720/110534 (24%)]\tAll Loss: 2.1297\tTriple Loss(1): 0.0583\tClassification Loss: 2.0132\n","Train Epoch: 2 [26880/110534 (24%)]\tAll Loss: 2.1333\tTriple Loss(1): 0.3135\tClassification Loss: 1.5063\n","Train Epoch: 2 [27040/110534 (24%)]\tAll Loss: 1.3362\tTriple Loss(1): 0.0425\tClassification Loss: 1.2511\n","\n","Test set: Average loss: 1.5902, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 2 [27200/110534 (25%)]\tAll Loss: 0.9294\tTriple Loss(0): 0.0000\tClassification Loss: 0.9294\n","Train Epoch: 2 [27360/110534 (25%)]\tAll Loss: 2.0663\tTriple Loss(1): 0.3623\tClassification Loss: 1.3417\n","Train Epoch: 2 [27520/110534 (25%)]\tAll Loss: 1.5790\tTriple Loss(1): 0.0876\tClassification Loss: 1.4039\n","Train Epoch: 2 [27680/110534 (25%)]\tAll Loss: 1.7236\tTriple Loss(1): 0.1173\tClassification Loss: 1.4889\n","Train Epoch: 2 [27840/110534 (25%)]\tAll Loss: 1.5079\tTriple Loss(1): 0.1973\tClassification Loss: 1.1133\n","Train Epoch: 2 [28000/110534 (25%)]\tAll Loss: 1.4148\tTriple Loss(0): 0.0000\tClassification Loss: 1.4148\n","Train Epoch: 2 [28160/110534 (25%)]\tAll Loss: 2.0164\tTriple Loss(1): 0.2585\tClassification Loss: 1.4995\n","Train Epoch: 2 [28320/110534 (26%)]\tAll Loss: 5.2671\tTriple Loss(0): 1.8366\tClassification Loss: 1.5940\n","Train Epoch: 2 [28480/110534 (26%)]\tAll Loss: 2.2616\tTriple Loss(1): 0.4750\tClassification Loss: 1.3117\n","Train Epoch: 2 [28640/110534 (26%)]\tAll Loss: 1.4356\tTriple Loss(0): 0.0000\tClassification Loss: 1.4356\n","\n","Test set: Average loss: 1.5790, Accuracy: 278/480 (58%)\n","\n","Train Epoch: 2 [28800/110534 (26%)]\tAll Loss: 1.8998\tTriple Loss(1): 0.3308\tClassification Loss: 1.2383\n","Train Epoch: 2 [28960/110534 (26%)]\tAll Loss: 1.9073\tTriple Loss(1): 0.2852\tClassification Loss: 1.3369\n","Train Epoch: 2 [29120/110534 (26%)]\tAll Loss: 1.9399\tTriple Loss(0): 0.0000\tClassification Loss: 1.9399\n","Train Epoch: 2 [29280/110534 (26%)]\tAll Loss: 2.3224\tTriple Loss(1): 0.2979\tClassification Loss: 1.7265\n","Train Epoch: 2 [29440/110534 (27%)]\tAll Loss: 1.9694\tTriple Loss(1): 0.0756\tClassification Loss: 1.8182\n","Train Epoch: 2 [29600/110534 (27%)]\tAll Loss: 2.1243\tTriple Loss(1): 0.3088\tClassification Loss: 1.5067\n","Train Epoch: 2 [29760/110534 (27%)]\tAll Loss: 5.3807\tTriple Loss(0): 1.7686\tClassification Loss: 1.8435\n","Train Epoch: 2 [29920/110534 (27%)]\tAll Loss: 3.3381\tTriple Loss(1): 0.5361\tClassification Loss: 2.2659\n","Train Epoch: 2 [30080/110534 (27%)]\tAll Loss: 2.0016\tTriple Loss(1): 0.3686\tClassification Loss: 1.2645\n","Train Epoch: 2 [30240/110534 (27%)]\tAll Loss: 2.2485\tTriple Loss(1): 0.3288\tClassification Loss: 1.5909\n","\n","Test set: Average loss: 1.5922, Accuracy: 262/480 (55%)\n","\n","Train Epoch: 2 [30400/110534 (28%)]\tAll Loss: 2.9633\tTriple Loss(1): 0.5500\tClassification Loss: 1.8632\n","Train Epoch: 2 [30560/110534 (28%)]\tAll Loss: 1.6330\tTriple Loss(1): 0.0531\tClassification Loss: 1.5268\n","Train Epoch: 2 [30720/110534 (28%)]\tAll Loss: 2.1808\tTriple Loss(1): 0.3040\tClassification Loss: 1.5729\n","Train Epoch: 2 [30880/110534 (28%)]\tAll Loss: 1.8429\tTriple Loss(1): 0.2716\tClassification Loss: 1.2997\n","Train Epoch: 2 [31040/110534 (28%)]\tAll Loss: 1.9877\tTriple Loss(1): 0.2009\tClassification Loss: 1.5858\n","Train Epoch: 2 [31200/110534 (28%)]\tAll Loss: 3.6635\tTriple Loss(0): 1.2379\tClassification Loss: 1.1877\n","Train Epoch: 2 [31360/110534 (28%)]\tAll Loss: 2.3596\tTriple Loss(1): 0.2866\tClassification Loss: 1.7863\n","Train Epoch: 2 [31520/110534 (29%)]\tAll Loss: 1.3032\tTriple Loss(0): 0.0000\tClassification Loss: 1.3032\n","Train Epoch: 2 [31680/110534 (29%)]\tAll Loss: 2.3045\tTriple Loss(1): 0.2582\tClassification Loss: 1.7881\n","Train Epoch: 2 [31840/110534 (29%)]\tAll Loss: 1.9753\tTriple Loss(1): 0.2017\tClassification Loss: 1.5719\n","\n","Test set: Average loss: 1.5644, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 2 [32000/110534 (29%)]\tAll Loss: 1.4191\tTriple Loss(0): 0.0000\tClassification Loss: 1.4191\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_2000.pth.tar\n","Train Epoch: 2 [32160/110534 (29%)]\tAll Loss: 2.3841\tTriple Loss(1): 0.3860\tClassification Loss: 1.6121\n","Train Epoch: 2 [32320/110534 (29%)]\tAll Loss: 2.4205\tTriple Loss(1): 0.1381\tClassification Loss: 2.1442\n","Train Epoch: 2 [32480/110534 (29%)]\tAll Loss: 1.4242\tTriple Loss(0): 0.0000\tClassification Loss: 1.4242\n","Train Epoch: 2 [32640/110534 (30%)]\tAll Loss: 1.2808\tTriple Loss(0): 0.0000\tClassification Loss: 1.2808\n","Train Epoch: 2 [32800/110534 (30%)]\tAll Loss: 2.0007\tTriple Loss(1): 0.2158\tClassification Loss: 1.5690\n","Train Epoch: 2 [32960/110534 (30%)]\tAll Loss: 1.4246\tTriple Loss(1): 0.0941\tClassification Loss: 1.2363\n","Train Epoch: 2 [33120/110534 (30%)]\tAll Loss: 1.4309\tTriple Loss(1): 0.2045\tClassification Loss: 1.0219\n","Train Epoch: 2 [33280/110534 (30%)]\tAll Loss: 1.6963\tTriple Loss(1): 0.2307\tClassification Loss: 1.2349\n","Train Epoch: 2 [33440/110534 (30%)]\tAll Loss: 1.9573\tTriple Loss(1): 0.1657\tClassification Loss: 1.6259\n","\n","Test set: Average loss: 1.5592, Accuracy: 268/480 (56%)\n","\n","Train Epoch: 2 [33600/110534 (30%)]\tAll Loss: 1.8865\tTriple Loss(1): 0.0952\tClassification Loss: 1.6962\n","Train Epoch: 2 [33760/110534 (31%)]\tAll Loss: 2.7607\tTriple Loss(1): 0.5249\tClassification Loss: 1.7109\n","Train Epoch: 2 [33920/110534 (31%)]\tAll Loss: 1.6400\tTriple Loss(0): 0.0000\tClassification Loss: 1.6400\n","Train Epoch: 2 [34080/110534 (31%)]\tAll Loss: 2.1922\tTriple Loss(1): 0.2156\tClassification Loss: 1.7611\n","Train Epoch: 2 [34240/110534 (31%)]\tAll Loss: 1.3417\tTriple Loss(1): 0.0987\tClassification Loss: 1.1443\n","Train Epoch: 2 [34400/110534 (31%)]\tAll Loss: 1.4089\tTriple Loss(1): 0.0934\tClassification Loss: 1.2221\n","Train Epoch: 2 [34560/110534 (31%)]\tAll Loss: 1.7086\tTriple Loss(1): 0.1734\tClassification Loss: 1.3617\n","Train Epoch: 2 [34720/110534 (31%)]\tAll Loss: 2.7405\tTriple Loss(1): 0.4666\tClassification Loss: 1.8074\n","Train Epoch: 2 [34880/110534 (32%)]\tAll Loss: 1.4857\tTriple Loss(1): 0.1211\tClassification Loss: 1.2436\n","Train Epoch: 2 [35040/110534 (32%)]\tAll Loss: 2.7462\tTriple Loss(1): 0.3666\tClassification Loss: 2.0130\n","\n","Test set: Average loss: 1.5809, Accuracy: 263/480 (55%)\n","\n","Train Epoch: 2 [35200/110534 (32%)]\tAll Loss: 2.3388\tTriple Loss(1): 0.1473\tClassification Loss: 2.0442\n","Train Epoch: 2 [35360/110534 (32%)]\tAll Loss: 1.8280\tTriple Loss(1): 0.2385\tClassification Loss: 1.3510\n","Train Epoch: 2 [35520/110534 (32%)]\tAll Loss: 2.8308\tTriple Loss(1): 0.4066\tClassification Loss: 2.0175\n","Train Epoch: 2 [35680/110534 (32%)]\tAll Loss: 2.1971\tTriple Loss(1): 0.2562\tClassification Loss: 1.6848\n","Train Epoch: 2 [35840/110534 (32%)]\tAll Loss: 2.5191\tTriple Loss(1): 0.5766\tClassification Loss: 1.3658\n","Train Epoch: 2 [36000/110534 (33%)]\tAll Loss: 2.1994\tTriple Loss(1): 0.0471\tClassification Loss: 2.1051\n","Train Epoch: 2 [36160/110534 (33%)]\tAll Loss: 1.3555\tTriple Loss(1): 0.0187\tClassification Loss: 1.3182\n","Train Epoch: 2 [36320/110534 (33%)]\tAll Loss: 1.7680\tTriple Loss(0): 0.0000\tClassification Loss: 1.7680\n","Train Epoch: 2 [36480/110534 (33%)]\tAll Loss: 1.4731\tTriple Loss(0): 0.0000\tClassification Loss: 1.4731\n","Train Epoch: 2 [36640/110534 (33%)]\tAll Loss: 1.8763\tTriple Loss(1): 0.1153\tClassification Loss: 1.6457\n","\n","Test set: Average loss: 1.5799, Accuracy: 267/480 (56%)\n","\n","Train Epoch: 2 [36800/110534 (33%)]\tAll Loss: 1.2533\tTriple Loss(1): 0.0000\tClassification Loss: 1.2533\n","Train Epoch: 2 [36960/110534 (33%)]\tAll Loss: 1.5610\tTriple Loss(1): 0.0880\tClassification Loss: 1.3849\n","Train Epoch: 2 [37120/110534 (34%)]\tAll Loss: 2.3875\tTriple Loss(1): 0.3348\tClassification Loss: 1.7180\n","Train Epoch: 2 [37280/110534 (34%)]\tAll Loss: 3.4680\tTriple Loss(1): 0.6210\tClassification Loss: 2.2260\n","Train Epoch: 2 [37440/110534 (34%)]\tAll Loss: 2.3571\tTriple Loss(1): 0.1164\tClassification Loss: 2.1243\n","Train Epoch: 2 [37600/110534 (34%)]\tAll Loss: 1.7722\tTriple Loss(1): 0.0917\tClassification Loss: 1.5888\n","Train Epoch: 2 [37760/110534 (34%)]\tAll Loss: 2.1013\tTriple Loss(1): 0.1020\tClassification Loss: 1.8973\n","Train Epoch: 2 [37920/110534 (34%)]\tAll Loss: 1.7735\tTriple Loss(1): 0.0000\tClassification Loss: 1.7735\n","Train Epoch: 2 [38080/110534 (34%)]\tAll Loss: 1.5426\tTriple Loss(1): 0.0047\tClassification Loss: 1.5332\n","Train Epoch: 2 [38240/110534 (35%)]\tAll Loss: 1.3482\tTriple Loss(1): 0.0958\tClassification Loss: 1.1566\n","\n","Test set: Average loss: 1.5675, Accuracy: 271/480 (56%)\n","\n","Train Epoch: 2 [38400/110534 (35%)]\tAll Loss: 1.7984\tTriple Loss(1): 0.3994\tClassification Loss: 0.9996\n","Train Epoch: 2 [38560/110534 (35%)]\tAll Loss: 1.6958\tTriple Loss(0): 0.0000\tClassification Loss: 1.6958\n","Train Epoch: 2 [38720/110534 (35%)]\tAll Loss: 2.0350\tTriple Loss(1): 0.0000\tClassification Loss: 2.0350\n","Train Epoch: 2 [38880/110534 (35%)]\tAll Loss: 1.5932\tTriple Loss(1): 0.0888\tClassification Loss: 1.4157\n","Train Epoch: 2 [39040/110534 (35%)]\tAll Loss: 1.8159\tTriple Loss(1): 0.1777\tClassification Loss: 1.4605\n","Train Epoch: 2 [39200/110534 (35%)]\tAll Loss: 1.8255\tTriple Loss(1): 0.0000\tClassification Loss: 1.8255\n","Train Epoch: 2 [39360/110534 (36%)]\tAll Loss: 2.8234\tTriple Loss(1): 0.7061\tClassification Loss: 1.4113\n","Train Epoch: 2 [39520/110534 (36%)]\tAll Loss: 2.4521\tTriple Loss(1): 0.0952\tClassification Loss: 2.2616\n","Train Epoch: 2 [39680/110534 (36%)]\tAll Loss: 1.7884\tTriple Loss(1): 0.3055\tClassification Loss: 1.1773\n","Train Epoch: 2 [39840/110534 (36%)]\tAll Loss: 1.4693\tTriple Loss(1): 0.3097\tClassification Loss: 0.8499\n","\n","Test set: Average loss: 1.5534, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 2 [40000/110534 (36%)]\tAll Loss: 1.8924\tTriple Loss(1): 0.2383\tClassification Loss: 1.4158\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_2500.pth.tar\n","Train Epoch: 2 [40160/110534 (36%)]\tAll Loss: 1.2245\tTriple Loss(0): 0.0000\tClassification Loss: 1.2245\n","Train Epoch: 2 [40320/110534 (36%)]\tAll Loss: 1.4002\tTriple Loss(1): 0.1837\tClassification Loss: 1.0327\n","Train Epoch: 2 [40480/110534 (37%)]\tAll Loss: 2.6483\tTriple Loss(1): 0.4449\tClassification Loss: 1.7584\n","Train Epoch: 2 [40640/110534 (37%)]\tAll Loss: 1.0758\tTriple Loss(0): 0.0000\tClassification Loss: 1.0758\n","Train Epoch: 2 [40800/110534 (37%)]\tAll Loss: 1.7640\tTriple Loss(1): 0.0414\tClassification Loss: 1.6811\n","Train Epoch: 2 [40960/110534 (37%)]\tAll Loss: 1.6423\tTriple Loss(0): 0.0000\tClassification Loss: 1.6423\n","Train Epoch: 2 [41120/110534 (37%)]\tAll Loss: 1.9167\tTriple Loss(1): 0.0000\tClassification Loss: 1.9167\n","Train Epoch: 2 [41280/110534 (37%)]\tAll Loss: 1.6982\tTriple Loss(0): 0.0000\tClassification Loss: 1.6982\n","Train Epoch: 2 [41440/110534 (37%)]\tAll Loss: 1.4796\tTriple Loss(1): 0.1200\tClassification Loss: 1.2396\n","\n","Test set: Average loss: 1.6016, Accuracy: 260/480 (54%)\n","\n","Train Epoch: 2 [41600/110534 (38%)]\tAll Loss: 1.6863\tTriple Loss(0): 0.0000\tClassification Loss: 1.6863\n","Train Epoch: 2 [41760/110534 (38%)]\tAll Loss: 1.6993\tTriple Loss(1): 0.4410\tClassification Loss: 0.8172\n","Train Epoch: 2 [41920/110534 (38%)]\tAll Loss: 1.7626\tTriple Loss(1): 0.1107\tClassification Loss: 1.5412\n","Train Epoch: 2 [42080/110534 (38%)]\tAll Loss: 1.3296\tTriple Loss(0): 0.0000\tClassification Loss: 1.3296\n","Train Epoch: 2 [42240/110534 (38%)]\tAll Loss: 1.9496\tTriple Loss(1): 0.4348\tClassification Loss: 1.0800\n","Train Epoch: 2 [42400/110534 (38%)]\tAll Loss: 2.2309\tTriple Loss(1): 0.1596\tClassification Loss: 1.9116\n","Train Epoch: 2 [42560/110534 (39%)]\tAll Loss: 2.3104\tTriple Loss(1): 0.1992\tClassification Loss: 1.9121\n","Train Epoch: 2 [42720/110534 (39%)]\tAll Loss: 1.6127\tTriple Loss(1): 0.0606\tClassification Loss: 1.4915\n","Train Epoch: 2 [42880/110534 (39%)]\tAll Loss: 1.9879\tTriple Loss(1): 0.3917\tClassification Loss: 1.2044\n","Train Epoch: 2 [43040/110534 (39%)]\tAll Loss: 2.1169\tTriple Loss(1): 0.2016\tClassification Loss: 1.7137\n","\n","Test set: Average loss: 1.5972, Accuracy: 263/480 (55%)\n","\n","Train Epoch: 2 [43200/110534 (39%)]\tAll Loss: 2.7001\tTriple Loss(1): 0.2322\tClassification Loss: 2.2357\n","Train Epoch: 2 [43360/110534 (39%)]\tAll Loss: 1.6117\tTriple Loss(1): 0.2564\tClassification Loss: 1.0990\n","Train Epoch: 2 [43520/110534 (39%)]\tAll Loss: 2.3026\tTriple Loss(1): 0.2999\tClassification Loss: 1.7027\n","Train Epoch: 2 [43680/110534 (40%)]\tAll Loss: 2.1808\tTriple Loss(1): 0.1157\tClassification Loss: 1.9494\n","Train Epoch: 2 [43840/110534 (40%)]\tAll Loss: 2.1285\tTriple Loss(1): 0.2512\tClassification Loss: 1.6261\n","Train Epoch: 2 [44000/110534 (40%)]\tAll Loss: 1.2689\tTriple Loss(1): 0.0515\tClassification Loss: 1.1659\n","Train Epoch: 2 [44160/110534 (40%)]\tAll Loss: 1.8157\tTriple Loss(1): 0.0529\tClassification Loss: 1.7099\n","Train Epoch: 2 [44320/110534 (40%)]\tAll Loss: 1.2148\tTriple Loss(0): 0.0000\tClassification Loss: 1.2148\n","Train Epoch: 2 [44480/110534 (40%)]\tAll Loss: 1.4180\tTriple Loss(0): 0.0000\tClassification Loss: 1.4180\n","Train Epoch: 2 [44640/110534 (40%)]\tAll Loss: 2.1316\tTriple Loss(1): 0.2379\tClassification Loss: 1.6559\n","\n","Test set: Average loss: 1.6264, Accuracy: 265/480 (55%)\n","\n","Train Epoch: 2 [44800/110534 (41%)]\tAll Loss: 2.4419\tTriple Loss(1): 0.2577\tClassification Loss: 1.9265\n","Train Epoch: 2 [44960/110534 (41%)]\tAll Loss: 2.5757\tTriple Loss(1): 0.2595\tClassification Loss: 2.0568\n","Train Epoch: 2 [45120/110534 (41%)]\tAll Loss: 1.3285\tTriple Loss(0): 0.0000\tClassification Loss: 1.3285\n","Train Epoch: 2 [45280/110534 (41%)]\tAll Loss: 1.7895\tTriple Loss(1): 0.1052\tClassification Loss: 1.5791\n","Train Epoch: 2 [45440/110534 (41%)]\tAll Loss: 2.0638\tTriple Loss(1): 0.2545\tClassification Loss: 1.5548\n","Train Epoch: 2 [45600/110534 (41%)]\tAll Loss: 2.7464\tTriple Loss(1): 0.4979\tClassification Loss: 1.7506\n","Train Epoch: 2 [45760/110534 (41%)]\tAll Loss: 1.8698\tTriple Loss(0): 0.0000\tClassification Loss: 1.8698\n","Train Epoch: 2 [45920/110534 (42%)]\tAll Loss: 1.9523\tTriple Loss(1): 0.3969\tClassification Loss: 1.1584\n","Train Epoch: 2 [46080/110534 (42%)]\tAll Loss: 1.6983\tTriple Loss(1): 0.1274\tClassification Loss: 1.4435\n","Train Epoch: 2 [46240/110534 (42%)]\tAll Loss: 2.4835\tTriple Loss(1): 0.4157\tClassification Loss: 1.6521\n","\n","Test set: Average loss: 1.5819, Accuracy: 268/480 (56%)\n","\n","Train Epoch: 2 [46400/110534 (42%)]\tAll Loss: 1.5314\tTriple Loss(1): 0.0706\tClassification Loss: 1.3902\n","Train Epoch: 2 [46560/110534 (42%)]\tAll Loss: 2.7431\tTriple Loss(1): 0.5473\tClassification Loss: 1.6485\n","Train Epoch: 2 [46720/110534 (42%)]\tAll Loss: 1.6024\tTriple Loss(1): 0.0584\tClassification Loss: 1.4856\n","Train Epoch: 2 [46880/110534 (42%)]\tAll Loss: 2.1569\tTriple Loss(1): 0.3237\tClassification Loss: 1.5094\n","Train Epoch: 2 [47040/110534 (43%)]\tAll Loss: 2.6322\tTriple Loss(1): 0.5230\tClassification Loss: 1.5861\n","Train Epoch: 2 [47200/110534 (43%)]\tAll Loss: 2.2382\tTriple Loss(1): 0.1351\tClassification Loss: 1.9681\n","Train Epoch: 2 [47360/110534 (43%)]\tAll Loss: 2.9184\tTriple Loss(1): 0.3086\tClassification Loss: 2.3012\n","Train Epoch: 2 [47520/110534 (43%)]\tAll Loss: 2.6223\tTriple Loss(1): 0.1880\tClassification Loss: 2.2463\n","Train Epoch: 2 [47680/110534 (43%)]\tAll Loss: 1.7167\tTriple Loss(1): 0.2881\tClassification Loss: 1.1405\n","Train Epoch: 2 [47840/110534 (43%)]\tAll Loss: 1.2883\tTriple Loss(0): 0.0000\tClassification Loss: 1.2883\n","\n","Test set: Average loss: 1.5764, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 2 [48000/110534 (43%)]\tAll Loss: 2.1462\tTriple Loss(1): 0.1604\tClassification Loss: 1.8254\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_3000.pth.tar\n","Train Epoch: 2 [48160/110534 (44%)]\tAll Loss: 1.6822\tTriple Loss(1): 0.1621\tClassification Loss: 1.3579\n","Train Epoch: 2 [48320/110534 (44%)]\tAll Loss: 1.5347\tTriple Loss(1): 0.1514\tClassification Loss: 1.2319\n","Train Epoch: 2 [48480/110534 (44%)]\tAll Loss: 2.1251\tTriple Loss(1): 0.2517\tClassification Loss: 1.6217\n","Train Epoch: 2 [48640/110534 (44%)]\tAll Loss: 1.5258\tTriple Loss(0): 0.0000\tClassification Loss: 1.5258\n","Train Epoch: 2 [48800/110534 (44%)]\tAll Loss: 1.9374\tTriple Loss(0): 0.0000\tClassification Loss: 1.9374\n","Train Epoch: 2 [48960/110534 (44%)]\tAll Loss: 1.8718\tTriple Loss(1): 0.2092\tClassification Loss: 1.4534\n","Train Epoch: 2 [49120/110534 (44%)]\tAll Loss: 2.6738\tTriple Loss(1): 0.4865\tClassification Loss: 1.7008\n","Train Epoch: 2 [49280/110534 (45%)]\tAll Loss: 1.9642\tTriple Loss(1): 0.0359\tClassification Loss: 1.8925\n","Train Epoch: 2 [49440/110534 (45%)]\tAll Loss: 2.1800\tTriple Loss(1): 0.2521\tClassification Loss: 1.6759\n","\n","Test set: Average loss: 1.5774, Accuracy: 268/480 (56%)\n","\n","Train Epoch: 2 [49600/110534 (45%)]\tAll Loss: 1.6061\tTriple Loss(1): 0.0112\tClassification Loss: 1.5837\n","Train Epoch: 2 [49760/110534 (45%)]\tAll Loss: 1.9875\tTriple Loss(0): 0.0000\tClassification Loss: 1.9875\n","Train Epoch: 2 [49920/110534 (45%)]\tAll Loss: 9.6593\tTriple Loss(0): 4.3916\tClassification Loss: 0.8761\n","Train Epoch: 2 [50080/110534 (45%)]\tAll Loss: 2.5575\tTriple Loss(1): 0.2530\tClassification Loss: 2.0514\n","Train Epoch: 2 [50240/110534 (45%)]\tAll Loss: 2.5406\tTriple Loss(1): 0.5091\tClassification Loss: 1.5223\n","Train Epoch: 2 [50400/110534 (46%)]\tAll Loss: 1.5967\tTriple Loss(0): 0.0000\tClassification Loss: 1.5967\n","Train Epoch: 2 [50560/110534 (46%)]\tAll Loss: 2.2519\tTriple Loss(1): 0.2553\tClassification Loss: 1.7413\n","Train Epoch: 2 [50720/110534 (46%)]\tAll Loss: 2.5910\tTriple Loss(1): 0.4059\tClassification Loss: 1.7792\n","Train Epoch: 2 [50880/110534 (46%)]\tAll Loss: 1.3003\tTriple Loss(1): 0.0014\tClassification Loss: 1.2975\n","Train Epoch: 2 [51040/110534 (46%)]\tAll Loss: 1.6456\tTriple Loss(1): 0.0387\tClassification Loss: 1.5682\n","\n","Test set: Average loss: 1.6701, Accuracy: 259/480 (54%)\n","\n","Train Epoch: 2 [51200/110534 (46%)]\tAll Loss: 1.6292\tTriple Loss(1): 0.2485\tClassification Loss: 1.1323\n","Train Epoch: 2 [51360/110534 (46%)]\tAll Loss: 2.2936\tTriple Loss(1): 0.2772\tClassification Loss: 1.7392\n","Train Epoch: 2 [51520/110534 (47%)]\tAll Loss: 2.5451\tTriple Loss(1): 0.5079\tClassification Loss: 1.5293\n","Train Epoch: 2 [51680/110534 (47%)]\tAll Loss: 2.1020\tTriple Loss(1): 0.0635\tClassification Loss: 1.9749\n","Train Epoch: 2 [51840/110534 (47%)]\tAll Loss: 2.7941\tTriple Loss(1): 0.4324\tClassification Loss: 1.9293\n","Train Epoch: 2 [52000/110534 (47%)]\tAll Loss: 3.5733\tTriple Loss(1): 0.7002\tClassification Loss: 2.1729\n","Train Epoch: 2 [52160/110534 (47%)]\tAll Loss: 2.9136\tTriple Loss(1): 0.3438\tClassification Loss: 2.2261\n","Train Epoch: 2 [52320/110534 (47%)]\tAll Loss: 1.9284\tTriple Loss(1): 0.3631\tClassification Loss: 1.2023\n","Train Epoch: 2 [52480/110534 (47%)]\tAll Loss: 1.8786\tTriple Loss(0): 0.0000\tClassification Loss: 1.8786\n","Train Epoch: 2 [52640/110534 (48%)]\tAll Loss: 1.6898\tTriple Loss(1): 0.1306\tClassification Loss: 1.4286\n","\n","Test set: Average loss: 1.6075, Accuracy: 267/480 (56%)\n","\n","Train Epoch: 2 [52800/110534 (48%)]\tAll Loss: 1.8422\tTriple Loss(1): 0.0443\tClassification Loss: 1.7537\n","Train Epoch: 2 [52960/110534 (48%)]\tAll Loss: 2.2144\tTriple Loss(1): 0.3673\tClassification Loss: 1.4798\n","Train Epoch: 2 [53120/110534 (48%)]\tAll Loss: 2.7038\tTriple Loss(1): 0.4373\tClassification Loss: 1.8292\n","Train Epoch: 2 [53280/110534 (48%)]\tAll Loss: 1.1163\tTriple Loss(1): 0.0719\tClassification Loss: 0.9725\n","Train Epoch: 2 [53440/110534 (48%)]\tAll Loss: 2.4730\tTriple Loss(1): 0.1596\tClassification Loss: 2.1538\n","Train Epoch: 2 [53600/110534 (48%)]\tAll Loss: 1.4741\tTriple Loss(0): 0.0000\tClassification Loss: 1.4741\n","Train Epoch: 2 [53760/110534 (49%)]\tAll Loss: 2.7468\tTriple Loss(1): 0.4227\tClassification Loss: 1.9013\n","Train Epoch: 2 [53920/110534 (49%)]\tAll Loss: 3.2806\tTriple Loss(1): 0.5068\tClassification Loss: 2.2671\n","Train Epoch: 2 [54080/110534 (49%)]\tAll Loss: 1.4771\tTriple Loss(0): 0.0000\tClassification Loss: 1.4771\n","Train Epoch: 2 [54240/110534 (49%)]\tAll Loss: 1.7608\tTriple Loss(1): 0.1746\tClassification Loss: 1.4115\n","\n","Test set: Average loss: 1.6384, Accuracy: 257/480 (54%)\n","\n","Train Epoch: 2 [54400/110534 (49%)]\tAll Loss: 2.0864\tTriple Loss(0): 0.0000\tClassification Loss: 2.0864\n","Train Epoch: 2 [54560/110534 (49%)]\tAll Loss: 1.7623\tTriple Loss(1): 0.0085\tClassification Loss: 1.7453\n","Train Epoch: 2 [54720/110534 (50%)]\tAll Loss: 1.7794\tTriple Loss(1): 0.2826\tClassification Loss: 1.2142\n","Train Epoch: 2 [54880/110534 (50%)]\tAll Loss: 2.6069\tTriple Loss(1): 0.0401\tClassification Loss: 2.5266\n","Train Epoch: 2 [55040/110534 (50%)]\tAll Loss: 1.9806\tTriple Loss(1): 0.2486\tClassification Loss: 1.4834\n","Train Epoch: 2 [55200/110534 (50%)]\tAll Loss: 2.0528\tTriple Loss(0): 0.2707\tClassification Loss: 1.5113\n","Train Epoch: 2 [55360/110534 (50%)]\tAll Loss: 7.0714\tTriple Loss(0): 3.0068\tClassification Loss: 1.0578\n","Train Epoch: 2 [55520/110534 (50%)]\tAll Loss: 2.7983\tTriple Loss(1): 0.3897\tClassification Loss: 2.0188\n","Train Epoch: 2 [55680/110534 (50%)]\tAll Loss: 1.8032\tTriple Loss(1): 0.1002\tClassification Loss: 1.6028\n","Train Epoch: 2 [55840/110534 (51%)]\tAll Loss: 2.9500\tTriple Loss(1): 0.7190\tClassification Loss: 1.5120\n","\n","Test set: Average loss: 1.5862, Accuracy: 275/480 (57%)\n","\n","Train Epoch: 2 [56000/110534 (51%)]\tAll Loss: 2.2268\tTriple Loss(1): 0.2822\tClassification Loss: 1.6624\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_3500.pth.tar\n","Train Epoch: 2 [56160/110534 (51%)]\tAll Loss: 1.4027\tTriple Loss(0): 0.0000\tClassification Loss: 1.4027\n","Train Epoch: 2 [56320/110534 (51%)]\tAll Loss: 1.4715\tTriple Loss(1): 0.0494\tClassification Loss: 1.3727\n","Train Epoch: 2 [56480/110534 (51%)]\tAll Loss: 1.5646\tTriple Loss(1): 0.0120\tClassification Loss: 1.5406\n","Train Epoch: 2 [56640/110534 (51%)]\tAll Loss: 2.0947\tTriple Loss(1): 0.2435\tClassification Loss: 1.6078\n","Train Epoch: 2 [56800/110534 (51%)]\tAll Loss: 2.5831\tTriple Loss(1): 0.2145\tClassification Loss: 2.1541\n","Train Epoch: 2 [56960/110534 (52%)]\tAll Loss: 1.9123\tTriple Loss(1): 0.0644\tClassification Loss: 1.7835\n","Train Epoch: 2 [57120/110534 (52%)]\tAll Loss: 1.9731\tTriple Loss(1): 0.1999\tClassification Loss: 1.5732\n","Train Epoch: 2 [57280/110534 (52%)]\tAll Loss: 2.6414\tTriple Loss(1): 0.4530\tClassification Loss: 1.7354\n","Train Epoch: 2 [57440/110534 (52%)]\tAll Loss: 1.8571\tTriple Loss(1): 0.2233\tClassification Loss: 1.4106\n","\n","Test set: Average loss: 1.6374, Accuracy: 264/480 (55%)\n","\n","Train Epoch: 2 [57600/110534 (52%)]\tAll Loss: 2.9671\tTriple Loss(1): 0.7593\tClassification Loss: 1.4486\n","Train Epoch: 2 [57760/110534 (52%)]\tAll Loss: 1.3143\tTriple Loss(1): 0.0636\tClassification Loss: 1.1871\n","Train Epoch: 2 [57920/110534 (52%)]\tAll Loss: 1.2562\tTriple Loss(0): 0.0000\tClassification Loss: 1.2562\n","Train Epoch: 2 [58080/110534 (53%)]\tAll Loss: 3.0085\tTriple Loss(1): 0.7060\tClassification Loss: 1.5965\n","Train Epoch: 2 [58240/110534 (53%)]\tAll Loss: 1.7161\tTriple Loss(0): 0.0000\tClassification Loss: 1.7161\n","Train Epoch: 2 [58400/110534 (53%)]\tAll Loss: 1.2721\tTriple Loss(0): 0.0000\tClassification Loss: 1.2721\n","Train Epoch: 2 [58560/110534 (53%)]\tAll Loss: 2.3503\tTriple Loss(0): 0.0000\tClassification Loss: 2.3503\n","Train Epoch: 2 [58720/110534 (53%)]\tAll Loss: 3.1742\tTriple Loss(1): 0.6622\tClassification Loss: 1.8498\n","Train Epoch: 2 [58880/110534 (53%)]\tAll Loss: 2.4203\tTriple Loss(1): 0.2128\tClassification Loss: 1.9947\n","Train Epoch: 2 [59040/110534 (53%)]\tAll Loss: 1.6680\tTriple Loss(1): 0.1743\tClassification Loss: 1.3195\n","\n","Test set: Average loss: 1.5761, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 2 [59200/110534 (54%)]\tAll Loss: 1.6278\tTriple Loss(1): 0.0802\tClassification Loss: 1.4675\n","Train Epoch: 2 [59360/110534 (54%)]\tAll Loss: 1.5978\tTriple Loss(1): 0.1136\tClassification Loss: 1.3706\n","Train Epoch: 2 [59520/110534 (54%)]\tAll Loss: 2.1598\tTriple Loss(1): 0.1367\tClassification Loss: 1.8865\n","Train Epoch: 2 [59680/110534 (54%)]\tAll Loss: 1.7875\tTriple Loss(1): 0.0212\tClassification Loss: 1.7451\n","Train Epoch: 2 [59840/110534 (54%)]\tAll Loss: 2.6354\tTriple Loss(1): 0.1370\tClassification Loss: 2.3613\n","Train Epoch: 2 [60000/110534 (54%)]\tAll Loss: 2.0099\tTriple Loss(1): 0.0498\tClassification Loss: 1.9103\n","Train Epoch: 2 [60160/110534 (54%)]\tAll Loss: 2.1497\tTriple Loss(1): 0.1171\tClassification Loss: 1.9155\n","Train Epoch: 2 [60320/110534 (55%)]\tAll Loss: 1.7602\tTriple Loss(1): 0.0474\tClassification Loss: 1.6654\n","Train Epoch: 2 [60480/110534 (55%)]\tAll Loss: 1.9694\tTriple Loss(1): 0.2385\tClassification Loss: 1.4924\n","Train Epoch: 2 [60640/110534 (55%)]\tAll Loss: 1.6572\tTriple Loss(1): 0.1554\tClassification Loss: 1.3464\n","\n","Test set: Average loss: 1.5588, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 2 [60800/110534 (55%)]\tAll Loss: 1.0677\tTriple Loss(1): 0.0060\tClassification Loss: 1.0557\n","Train Epoch: 2 [60960/110534 (55%)]\tAll Loss: 1.5722\tTriple Loss(1): 0.2475\tClassification Loss: 1.0772\n","Train Epoch: 2 [61120/110534 (55%)]\tAll Loss: 2.3956\tTriple Loss(1): 0.4031\tClassification Loss: 1.5894\n","Train Epoch: 2 [61280/110534 (55%)]\tAll Loss: 1.6666\tTriple Loss(1): 0.0755\tClassification Loss: 1.5157\n","Train Epoch: 2 [61440/110534 (56%)]\tAll Loss: 1.7301\tTriple Loss(1): 0.1630\tClassification Loss: 1.4040\n","Train Epoch: 2 [61600/110534 (56%)]\tAll Loss: 3.1028\tTriple Loss(1): 0.3787\tClassification Loss: 2.3455\n","Train Epoch: 2 [61760/110534 (56%)]\tAll Loss: 1.4888\tTriple Loss(0): 0.0000\tClassification Loss: 1.4888\n","Train Epoch: 2 [61920/110534 (56%)]\tAll Loss: 1.4723\tTriple Loss(0): 0.0000\tClassification Loss: 1.4723\n","Train Epoch: 2 [62080/110534 (56%)]\tAll Loss: 1.7864\tTriple Loss(0): 0.0000\tClassification Loss: 1.7864\n","Train Epoch: 2 [62240/110534 (56%)]\tAll Loss: 1.8657\tTriple Loss(1): 0.2861\tClassification Loss: 1.2934\n","\n","Test set: Average loss: 1.5548, Accuracy: 280/480 (58%)\n","\n","Train Epoch: 2 [62400/110534 (56%)]\tAll Loss: 2.2821\tTriple Loss(1): 0.3451\tClassification Loss: 1.5919\n","Train Epoch: 2 [62560/110534 (57%)]\tAll Loss: 2.5909\tTriple Loss(1): 0.3048\tClassification Loss: 1.9814\n","Train Epoch: 2 [62720/110534 (57%)]\tAll Loss: 1.9334\tTriple Loss(1): 0.2476\tClassification Loss: 1.4381\n","Train Epoch: 2 [62880/110534 (57%)]\tAll Loss: 1.4233\tTriple Loss(0): 0.0000\tClassification Loss: 1.4233\n","Train Epoch: 2 [63040/110534 (57%)]\tAll Loss: 2.1501\tTriple Loss(1): 0.0380\tClassification Loss: 2.0740\n","Train Epoch: 2 [63200/110534 (57%)]\tAll Loss: 1.4795\tTriple Loss(1): 0.0034\tClassification Loss: 1.4728\n","Train Epoch: 2 [63360/110534 (57%)]\tAll Loss: 2.5607\tTriple Loss(1): 0.1364\tClassification Loss: 2.2879\n","Train Epoch: 2 [63520/110534 (57%)]\tAll Loss: 1.6074\tTriple Loss(1): 0.0000\tClassification Loss: 1.6074\n","Train Epoch: 2 [63680/110534 (58%)]\tAll Loss: 1.1632\tTriple Loss(1): 0.0000\tClassification Loss: 1.1632\n","Train Epoch: 2 [63840/110534 (58%)]\tAll Loss: 1.8332\tTriple Loss(1): 0.1542\tClassification Loss: 1.5247\n","\n","Test set: Average loss: 1.6150, Accuracy: 261/480 (54%)\n","\n","Train Epoch: 2 [64000/110534 (58%)]\tAll Loss: 1.5744\tTriple Loss(0): 0.0000\tClassification Loss: 1.5744\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_4000.pth.tar\n","Train Epoch: 2 [64160/110534 (58%)]\tAll Loss: 2.3643\tTriple Loss(1): 0.3303\tClassification Loss: 1.7038\n","Train Epoch: 2 [64320/110534 (58%)]\tAll Loss: 1.0221\tTriple Loss(0): 0.0000\tClassification Loss: 1.0221\n","Train Epoch: 2 [64480/110534 (58%)]\tAll Loss: 1.8509\tTriple Loss(1): 0.1644\tClassification Loss: 1.5221\n","Train Epoch: 2 [64640/110534 (58%)]\tAll Loss: 1.5189\tTriple Loss(0): 0.0000\tClassification Loss: 1.5189\n","Train Epoch: 2 [64800/110534 (59%)]\tAll Loss: 1.2937\tTriple Loss(0): 0.0000\tClassification Loss: 1.2937\n","Train Epoch: 2 [64960/110534 (59%)]\tAll Loss: 1.8996\tTriple Loss(1): 0.2164\tClassification Loss: 1.4667\n","Train Epoch: 2 [65120/110534 (59%)]\tAll Loss: 2.2689\tTriple Loss(1): 0.2938\tClassification Loss: 1.6813\n","Train Epoch: 2 [65280/110534 (59%)]\tAll Loss: 2.0528\tTriple Loss(0): 0.0000\tClassification Loss: 2.0528\n","Train Epoch: 2 [65440/110534 (59%)]\tAll Loss: 1.6033\tTriple Loss(1): 0.1806\tClassification Loss: 1.2421\n","\n","Test set: Average loss: 1.5773, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 2 [65600/110534 (59%)]\tAll Loss: 2.1121\tTriple Loss(1): 0.2935\tClassification Loss: 1.5251\n","Train Epoch: 2 [65760/110534 (59%)]\tAll Loss: 1.5847\tTriple Loss(0): 0.0000\tClassification Loss: 1.5847\n","Train Epoch: 2 [65920/110534 (60%)]\tAll Loss: 1.8208\tTriple Loss(1): 0.2047\tClassification Loss: 1.4113\n","Train Epoch: 2 [66080/110534 (60%)]\tAll Loss: 1.3412\tTriple Loss(0): 0.0000\tClassification Loss: 1.3412\n","Train Epoch: 2 [66240/110534 (60%)]\tAll Loss: 1.4742\tTriple Loss(1): 0.1396\tClassification Loss: 1.1949\n","Train Epoch: 2 [66400/110534 (60%)]\tAll Loss: 2.6209\tTriple Loss(1): 0.4657\tClassification Loss: 1.6896\n","Train Epoch: 2 [66560/110534 (60%)]\tAll Loss: 1.6423\tTriple Loss(1): 0.2128\tClassification Loss: 1.2166\n","Train Epoch: 2 [66720/110534 (60%)]\tAll Loss: 2.1421\tTriple Loss(1): 0.2488\tClassification Loss: 1.6445\n","Train Epoch: 2 [66880/110534 (61%)]\tAll Loss: 2.3950\tTriple Loss(1): 0.1325\tClassification Loss: 2.1300\n","Train Epoch: 2 [67040/110534 (61%)]\tAll Loss: 3.3346\tTriple Loss(1): 0.6844\tClassification Loss: 1.9659\n","\n","Test set: Average loss: 1.5514, Accuracy: 285/480 (59%)\n","\n","Train Epoch: 2 [67200/110534 (61%)]\tAll Loss: 2.1444\tTriple Loss(1): 0.1937\tClassification Loss: 1.7570\n","Train Epoch: 2 [67360/110534 (61%)]\tAll Loss: 1.3187\tTriple Loss(1): 0.0569\tClassification Loss: 1.2050\n","Train Epoch: 2 [67520/110534 (61%)]\tAll Loss: 1.4815\tTriple Loss(1): 0.0824\tClassification Loss: 1.3167\n","Train Epoch: 2 [67680/110534 (61%)]\tAll Loss: 1.0106\tTriple Loss(0): 0.0000\tClassification Loss: 1.0106\n","Train Epoch: 2 [67840/110534 (61%)]\tAll Loss: 2.2036\tTriple Loss(1): 0.2750\tClassification Loss: 1.6536\n","Train Epoch: 2 [68000/110534 (62%)]\tAll Loss: 2.0734\tTriple Loss(1): 0.1574\tClassification Loss: 1.7586\n","Train Epoch: 2 [68160/110534 (62%)]\tAll Loss: 1.9275\tTriple Loss(1): 0.1163\tClassification Loss: 1.6950\n","Train Epoch: 2 [68320/110534 (62%)]\tAll Loss: 1.8544\tTriple Loss(1): 0.4129\tClassification Loss: 1.0285\n","Train Epoch: 2 [68480/110534 (62%)]\tAll Loss: 1.8800\tTriple Loss(1): 0.3353\tClassification Loss: 1.2094\n","Train Epoch: 2 [68640/110534 (62%)]\tAll Loss: 2.5910\tTriple Loss(1): 0.4384\tClassification Loss: 1.7142\n","\n","Test set: Average loss: 1.5933, Accuracy: 277/480 (58%)\n","\n","Train Epoch: 2 [68800/110534 (62%)]\tAll Loss: 2.1955\tTriple Loss(1): 0.0688\tClassification Loss: 2.0578\n","Train Epoch: 2 [68960/110534 (62%)]\tAll Loss: 2.6174\tTriple Loss(1): 0.3754\tClassification Loss: 1.8665\n","Train Epoch: 2 [69120/110534 (63%)]\tAll Loss: 1.8092\tTriple Loss(1): 0.1993\tClassification Loss: 1.4106\n","Train Epoch: 2 [69280/110534 (63%)]\tAll Loss: 1.6934\tTriple Loss(1): 0.0555\tClassification Loss: 1.5824\n","Train Epoch: 2 [69440/110534 (63%)]\tAll Loss: 2.1906\tTriple Loss(1): 0.1035\tClassification Loss: 1.9836\n","Train Epoch: 2 [69600/110534 (63%)]\tAll Loss: 1.6706\tTriple Loss(1): 0.2975\tClassification Loss: 1.0757\n","Train Epoch: 2 [69760/110534 (63%)]\tAll Loss: 2.1280\tTriple Loss(1): 0.3844\tClassification Loss: 1.3593\n","Train Epoch: 2 [69920/110534 (63%)]\tAll Loss: 2.0889\tTriple Loss(0): 0.0000\tClassification Loss: 2.0889\n","Train Epoch: 2 [70080/110534 (63%)]\tAll Loss: 1.9072\tTriple Loss(1): 0.2096\tClassification Loss: 1.4880\n","Train Epoch: 2 [70240/110534 (64%)]\tAll Loss: 2.1114\tTriple Loss(0): 0.0000\tClassification Loss: 2.1114\n","\n","Test set: Average loss: 1.5676, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 2 [70400/110534 (64%)]\tAll Loss: 2.0856\tTriple Loss(1): 0.0000\tClassification Loss: 2.0856\n","Train Epoch: 2 [70560/110534 (64%)]\tAll Loss: 1.8415\tTriple Loss(0): 0.0000\tClassification Loss: 1.8415\n","Train Epoch: 2 [70720/110534 (64%)]\tAll Loss: 2.1864\tTriple Loss(1): 0.2710\tClassification Loss: 1.6444\n","Train Epoch: 2 [70880/110534 (64%)]\tAll Loss: 2.0169\tTriple Loss(1): 0.2191\tClassification Loss: 1.5787\n","Train Epoch: 2 [71040/110534 (64%)]\tAll Loss: 1.7021\tTriple Loss(1): 0.2734\tClassification Loss: 1.1554\n","Train Epoch: 2 [71200/110534 (64%)]\tAll Loss: 1.2449\tTriple Loss(0): 0.0000\tClassification Loss: 1.2449\n","Train Epoch: 2 [71360/110534 (65%)]\tAll Loss: 2.9060\tTriple Loss(1): 0.8547\tClassification Loss: 1.1966\n","Train Epoch: 2 [71520/110534 (65%)]\tAll Loss: 1.5659\tTriple Loss(1): 0.0000\tClassification Loss: 1.5659\n","Train Epoch: 2 [71680/110534 (65%)]\tAll Loss: 1.5189\tTriple Loss(1): 0.2879\tClassification Loss: 0.9431\n","Train Epoch: 2 [71840/110534 (65%)]\tAll Loss: 2.3230\tTriple Loss(0): 0.0000\tClassification Loss: 2.3230\n","\n","Test set: Average loss: 1.5789, Accuracy: 282/480 (59%)\n","\n","Train Epoch: 2 [72000/110534 (65%)]\tAll Loss: 1.4465\tTriple Loss(1): 0.0912\tClassification Loss: 1.2640\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_4500.pth.tar\n","Train Epoch: 2 [72160/110534 (65%)]\tAll Loss: 1.2785\tTriple Loss(0): 0.0000\tClassification Loss: 1.2785\n","Train Epoch: 2 [72320/110534 (65%)]\tAll Loss: 2.1920\tTriple Loss(1): 0.2741\tClassification Loss: 1.6438\n","Train Epoch: 2 [72480/110534 (66%)]\tAll Loss: 2.4605\tTriple Loss(1): 0.0162\tClassification Loss: 2.4281\n","Train Epoch: 2 [72640/110534 (66%)]\tAll Loss: 3.0603\tTriple Loss(0): 0.6220\tClassification Loss: 1.8162\n","Train Epoch: 2 [72800/110534 (66%)]\tAll Loss: 1.9933\tTriple Loss(1): 0.2005\tClassification Loss: 1.5923\n","Train Epoch: 2 [72960/110534 (66%)]\tAll Loss: 1.0929\tTriple Loss(0): 0.0000\tClassification Loss: 1.0929\n","Train Epoch: 2 [73120/110534 (66%)]\tAll Loss: 2.3678\tTriple Loss(1): 0.4802\tClassification Loss: 1.4074\n","Train Epoch: 2 [73280/110534 (66%)]\tAll Loss: 1.4754\tTriple Loss(0): 0.0000\tClassification Loss: 1.4754\n","Train Epoch: 2 [73440/110534 (66%)]\tAll Loss: 1.5508\tTriple Loss(0): 0.0000\tClassification Loss: 1.5508\n","\n","Test set: Average loss: 1.5901, Accuracy: 282/480 (59%)\n","\n","Train Epoch: 2 [73600/110534 (67%)]\tAll Loss: 2.1337\tTriple Loss(1): 0.4141\tClassification Loss: 1.3054\n","Train Epoch: 2 [73760/110534 (67%)]\tAll Loss: 1.7225\tTriple Loss(1): 0.0000\tClassification Loss: 1.7225\n","Train Epoch: 2 [73920/110534 (67%)]\tAll Loss: 2.4074\tTriple Loss(1): 0.4027\tClassification Loss: 1.6020\n","Train Epoch: 2 [74080/110534 (67%)]\tAll Loss: 1.4128\tTriple Loss(1): 0.1002\tClassification Loss: 1.2124\n","Train Epoch: 2 [74240/110534 (67%)]\tAll Loss: 1.9654\tTriple Loss(0): 0.0000\tClassification Loss: 1.9654\n","Train Epoch: 2 [74400/110534 (67%)]\tAll Loss: 2.6589\tTriple Loss(1): 0.4565\tClassification Loss: 1.7459\n","Train Epoch: 2 [74560/110534 (67%)]\tAll Loss: 2.0653\tTriple Loss(1): 0.2084\tClassification Loss: 1.6485\n","Train Epoch: 2 [74720/110534 (68%)]\tAll Loss: 2.0581\tTriple Loss(1): 0.1278\tClassification Loss: 1.8026\n","Train Epoch: 2 [74880/110534 (68%)]\tAll Loss: 2.4351\tTriple Loss(1): 0.3739\tClassification Loss: 1.6873\n","Train Epoch: 2 [75040/110534 (68%)]\tAll Loss: 1.0004\tTriple Loss(1): 0.0586\tClassification Loss: 0.8832\n","\n","Test set: Average loss: 1.5789, Accuracy: 268/480 (56%)\n","\n","Train Epoch: 2 [75200/110534 (68%)]\tAll Loss: 1.4384\tTriple Loss(1): 0.1620\tClassification Loss: 1.1144\n","Train Epoch: 2 [75360/110534 (68%)]\tAll Loss: 2.2871\tTriple Loss(1): 0.1237\tClassification Loss: 2.0398\n","Train Epoch: 2 [75520/110534 (68%)]\tAll Loss: 1.7712\tTriple Loss(1): 0.1301\tClassification Loss: 1.5110\n","Train Epoch: 2 [75680/110534 (68%)]\tAll Loss: 1.9560\tTriple Loss(1): 0.0037\tClassification Loss: 1.9486\n","Train Epoch: 2 [75840/110534 (69%)]\tAll Loss: 2.2953\tTriple Loss(1): 0.1564\tClassification Loss: 1.9826\n","Train Epoch: 2 [76000/110534 (69%)]\tAll Loss: 1.7699\tTriple Loss(0): 0.0000\tClassification Loss: 1.7699\n","Train Epoch: 2 [76160/110534 (69%)]\tAll Loss: 2.0858\tTriple Loss(1): 0.1958\tClassification Loss: 1.6942\n","Train Epoch: 2 [76320/110534 (69%)]\tAll Loss: 2.1352\tTriple Loss(1): 0.1539\tClassification Loss: 1.8275\n","Train Epoch: 2 [76480/110534 (69%)]\tAll Loss: 1.8036\tTriple Loss(1): 0.1541\tClassification Loss: 1.4953\n","Train Epoch: 2 [76640/110534 (69%)]\tAll Loss: 2.0334\tTriple Loss(1): 0.2558\tClassification Loss: 1.5219\n","\n","Test set: Average loss: 1.5536, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 2 [76800/110534 (69%)]\tAll Loss: 1.3241\tTriple Loss(0): 0.0000\tClassification Loss: 1.3241\n","Train Epoch: 2 [76960/110534 (70%)]\tAll Loss: 1.5399\tTriple Loss(0): 0.0000\tClassification Loss: 1.5399\n","Train Epoch: 2 [77120/110534 (70%)]\tAll Loss: 2.6057\tTriple Loss(1): 0.5400\tClassification Loss: 1.5257\n","Train Epoch: 2 [77280/110534 (70%)]\tAll Loss: 1.8812\tTriple Loss(1): 0.1344\tClassification Loss: 1.6124\n","Train Epoch: 2 [77440/110534 (70%)]\tAll Loss: 1.8673\tTriple Loss(1): 0.2158\tClassification Loss: 1.4357\n","Train Epoch: 2 [77600/110534 (70%)]\tAll Loss: 3.1425\tTriple Loss(1): 0.4749\tClassification Loss: 2.1926\n","Train Epoch: 2 [77760/110534 (70%)]\tAll Loss: 1.8595\tTriple Loss(1): 0.0900\tClassification Loss: 1.6795\n","Train Epoch: 2 [77920/110534 (70%)]\tAll Loss: 4.7703\tTriple Loss(0): 1.4308\tClassification Loss: 1.9087\n","Train Epoch: 2 [78080/110534 (71%)]\tAll Loss: 1.5190\tTriple Loss(0): 0.0000\tClassification Loss: 1.5190\n","Train Epoch: 2 [78240/110534 (71%)]\tAll Loss: 2.9151\tTriple Loss(1): 0.6196\tClassification Loss: 1.6758\n","\n","Test set: Average loss: 1.5796, Accuracy: 266/480 (55%)\n","\n","Train Epoch: 2 [78400/110534 (71%)]\tAll Loss: 2.1215\tTriple Loss(1): 0.3754\tClassification Loss: 1.3706\n","Train Epoch: 2 [78560/110534 (71%)]\tAll Loss: 2.0936\tTriple Loss(0): 0.0000\tClassification Loss: 2.0936\n","Train Epoch: 2 [78720/110534 (71%)]\tAll Loss: 1.9574\tTriple Loss(1): 0.3752\tClassification Loss: 1.2070\n","Train Epoch: 2 [78880/110534 (71%)]\tAll Loss: 1.9112\tTriple Loss(1): 0.1079\tClassification Loss: 1.6954\n","Train Epoch: 2 [79040/110534 (72%)]\tAll Loss: 2.2456\tTriple Loss(1): 0.4503\tClassification Loss: 1.3450\n","Train Epoch: 2 [79200/110534 (72%)]\tAll Loss: 2.3814\tTriple Loss(1): 0.3731\tClassification Loss: 1.6351\n","Train Epoch: 2 [79360/110534 (72%)]\tAll Loss: 1.7547\tTriple Loss(1): 0.0867\tClassification Loss: 1.5813\n","Train Epoch: 2 [79520/110534 (72%)]\tAll Loss: 2.0622\tTriple Loss(1): 0.3963\tClassification Loss: 1.2697\n","Train Epoch: 2 [79680/110534 (72%)]\tAll Loss: 1.5713\tTriple Loss(0): 0.0000\tClassification Loss: 1.5713\n","Train Epoch: 2 [79840/110534 (72%)]\tAll Loss: 2.1590\tTriple Loss(1): 0.3580\tClassification Loss: 1.4430\n","\n","Test set: Average loss: 1.5775, Accuracy: 279/480 (58%)\n","\n","Train Epoch: 2 [80000/110534 (72%)]\tAll Loss: 2.4911\tTriple Loss(1): 0.3062\tClassification Loss: 1.8788\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_5000.pth.tar\n","Train Epoch: 2 [80160/110534 (73%)]\tAll Loss: 2.0072\tTriple Loss(1): 0.0479\tClassification Loss: 1.9113\n","Train Epoch: 2 [80320/110534 (73%)]\tAll Loss: 2.6892\tTriple Loss(1): 0.1961\tClassification Loss: 2.2970\n","Train Epoch: 2 [80480/110534 (73%)]\tAll Loss: 1.9736\tTriple Loss(1): 0.3289\tClassification Loss: 1.3158\n","Train Epoch: 2 [80640/110534 (73%)]\tAll Loss: 1.9850\tTriple Loss(1): 0.2433\tClassification Loss: 1.4984\n","Train Epoch: 2 [80800/110534 (73%)]\tAll Loss: 2.6311\tTriple Loss(1): 0.4216\tClassification Loss: 1.7880\n","Train Epoch: 2 [80960/110534 (73%)]\tAll Loss: 2.4405\tTriple Loss(1): 0.5841\tClassification Loss: 1.2722\n","Train Epoch: 2 [81120/110534 (73%)]\tAll Loss: 1.5528\tTriple Loss(1): 0.0011\tClassification Loss: 1.5506\n","Train Epoch: 2 [81280/110534 (74%)]\tAll Loss: 2.9518\tTriple Loss(1): 0.7095\tClassification Loss: 1.5328\n","Train Epoch: 2 [81440/110534 (74%)]\tAll Loss: 1.2680\tTriple Loss(1): 0.0258\tClassification Loss: 1.2165\n","\n","Test set: Average loss: 1.5756, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 2 [81600/110534 (74%)]\tAll Loss: 1.8426\tTriple Loss(1): 0.0125\tClassification Loss: 1.8176\n","Train Epoch: 2 [81760/110534 (74%)]\tAll Loss: 1.4652\tTriple Loss(0): 0.0000\tClassification Loss: 1.4652\n","Train Epoch: 2 [81920/110534 (74%)]\tAll Loss: 1.8102\tTriple Loss(0): 0.0000\tClassification Loss: 1.8102\n","Train Epoch: 2 [82080/110534 (74%)]\tAll Loss: 1.2265\tTriple Loss(0): 0.0000\tClassification Loss: 1.2265\n","Train Epoch: 2 [82240/110534 (74%)]\tAll Loss: 1.3728\tTriple Loss(1): 0.0625\tClassification Loss: 1.2477\n","Train Epoch: 2 [82400/110534 (75%)]\tAll Loss: 2.8363\tTriple Loss(1): 0.4316\tClassification Loss: 1.9732\n","Train Epoch: 2 [82560/110534 (75%)]\tAll Loss: 1.8524\tTriple Loss(0): 0.0000\tClassification Loss: 1.8524\n","Train Epoch: 2 [82720/110534 (75%)]\tAll Loss: 1.5255\tTriple Loss(1): 0.2150\tClassification Loss: 1.0955\n","Train Epoch: 2 [82880/110534 (75%)]\tAll Loss: 2.4296\tTriple Loss(1): 0.5071\tClassification Loss: 1.4153\n","Train Epoch: 2 [83040/110534 (75%)]\tAll Loss: 1.5148\tTriple Loss(0): 0.0000\tClassification Loss: 1.5148\n","\n","Test set: Average loss: 1.5701, Accuracy: 271/480 (56%)\n","\n","Train Epoch: 2 [83200/110534 (75%)]\tAll Loss: 1.4499\tTriple Loss(1): 0.0820\tClassification Loss: 1.2859\n","Train Epoch: 2 [83360/110534 (75%)]\tAll Loss: 2.0886\tTriple Loss(1): 0.0617\tClassification Loss: 1.9651\n","Train Epoch: 2 [83520/110534 (76%)]\tAll Loss: 1.3398\tTriple Loss(0): 0.0000\tClassification Loss: 1.3398\n","Train Epoch: 2 [83680/110534 (76%)]\tAll Loss: 2.5032\tTriple Loss(1): 0.2677\tClassification Loss: 1.9678\n","Train Epoch: 2 [83840/110534 (76%)]\tAll Loss: 1.4053\tTriple Loss(1): 0.0000\tClassification Loss: 1.4053\n","Train Epoch: 2 [84000/110534 (76%)]\tAll Loss: 2.0371\tTriple Loss(1): 0.2539\tClassification Loss: 1.5292\n","Train Epoch: 2 [84160/110534 (76%)]\tAll Loss: 1.9073\tTriple Loss(1): 0.2170\tClassification Loss: 1.4732\n","Train Epoch: 2 [84320/110534 (76%)]\tAll Loss: 1.3226\tTriple Loss(1): 0.1364\tClassification Loss: 1.0497\n","Train Epoch: 2 [84480/110534 (76%)]\tAll Loss: 2.0361\tTriple Loss(1): 0.0576\tClassification Loss: 1.9210\n","Train Epoch: 2 [84640/110534 (77%)]\tAll Loss: 1.7428\tTriple Loss(0): 0.0000\tClassification Loss: 1.7428\n","\n","Test set: Average loss: 1.5878, Accuracy: 277/480 (58%)\n","\n","Train Epoch: 2 [84800/110534 (77%)]\tAll Loss: 1.9550\tTriple Loss(0): 0.0000\tClassification Loss: 1.9550\n","Train Epoch: 2 [84960/110534 (77%)]\tAll Loss: 3.0864\tTriple Loss(1): 0.5784\tClassification Loss: 1.9297\n","Train Epoch: 2 [85120/110534 (77%)]\tAll Loss: 2.2400\tTriple Loss(1): 0.2931\tClassification Loss: 1.6539\n","Train Epoch: 2 [85280/110534 (77%)]\tAll Loss: 1.5983\tTriple Loss(0): 0.0000\tClassification Loss: 1.5983\n","Train Epoch: 2 [85440/110534 (77%)]\tAll Loss: 2.4916\tTriple Loss(1): 0.2897\tClassification Loss: 1.9123\n","Train Epoch: 2 [85600/110534 (77%)]\tAll Loss: 1.8954\tTriple Loss(1): 0.1849\tClassification Loss: 1.5257\n","Train Epoch: 2 [85760/110534 (78%)]\tAll Loss: 2.0222\tTriple Loss(1): 0.2873\tClassification Loss: 1.4476\n","Train Epoch: 2 [85920/110534 (78%)]\tAll Loss: 1.3984\tTriple Loss(0): 0.0000\tClassification Loss: 1.3984\n","Train Epoch: 2 [86080/110534 (78%)]\tAll Loss: 2.0961\tTriple Loss(1): 0.1948\tClassification Loss: 1.7064\n","Train Epoch: 2 [86240/110534 (78%)]\tAll Loss: 1.7558\tTriple Loss(1): 0.0027\tClassification Loss: 1.7505\n","\n","Test set: Average loss: 1.5892, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 2 [86400/110534 (78%)]\tAll Loss: 2.2750\tTriple Loss(1): 0.2340\tClassification Loss: 1.8071\n","Train Epoch: 2 [86560/110534 (78%)]\tAll Loss: 2.3897\tTriple Loss(1): 0.1729\tClassification Loss: 2.0440\n","Train Epoch: 2 [86720/110534 (78%)]\tAll Loss: 2.7133\tTriple Loss(1): 0.4219\tClassification Loss: 1.8695\n","Train Epoch: 2 [86880/110534 (79%)]\tAll Loss: 1.6596\tTriple Loss(1): 0.0000\tClassification Loss: 1.6596\n","Train Epoch: 2 [87040/110534 (79%)]\tAll Loss: 2.4944\tTriple Loss(1): 0.4564\tClassification Loss: 1.5816\n","Train Epoch: 2 [87200/110534 (79%)]\tAll Loss: 2.0607\tTriple Loss(1): 0.0960\tClassification Loss: 1.8688\n","Train Epoch: 2 [87360/110534 (79%)]\tAll Loss: 2.3001\tTriple Loss(1): 0.0329\tClassification Loss: 2.2342\n","Train Epoch: 2 [87520/110534 (79%)]\tAll Loss: 1.3431\tTriple Loss(1): 0.0184\tClassification Loss: 1.3063\n","Train Epoch: 2 [87680/110534 (79%)]\tAll Loss: 2.2724\tTriple Loss(1): 0.2260\tClassification Loss: 1.8205\n","Train Epoch: 2 [87840/110534 (79%)]\tAll Loss: 1.3329\tTriple Loss(1): 0.0615\tClassification Loss: 1.2100\n","\n","Test set: Average loss: 1.5858, Accuracy: 266/480 (55%)\n","\n","Train Epoch: 2 [88000/110534 (80%)]\tAll Loss: 1.6222\tTriple Loss(1): 0.1424\tClassification Loss: 1.3374\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_5500.pth.tar\n","Train Epoch: 2 [88160/110534 (80%)]\tAll Loss: 2.9099\tTriple Loss(1): 0.1920\tClassification Loss: 2.5259\n","Train Epoch: 2 [88320/110534 (80%)]\tAll Loss: 1.9083\tTriple Loss(1): 0.1136\tClassification Loss: 1.6810\n","Train Epoch: 2 [88480/110534 (80%)]\tAll Loss: 1.2822\tTriple Loss(0): 0.0000\tClassification Loss: 1.2822\n","Train Epoch: 2 [88640/110534 (80%)]\tAll Loss: 1.7044\tTriple Loss(1): 0.1031\tClassification Loss: 1.4983\n","Train Epoch: 2 [88800/110534 (80%)]\tAll Loss: 1.1968\tTriple Loss(1): 0.1374\tClassification Loss: 0.9221\n","Train Epoch: 2 [88960/110534 (80%)]\tAll Loss: 2.1426\tTriple Loss(1): 0.1483\tClassification Loss: 1.8461\n","Train Epoch: 2 [89120/110534 (81%)]\tAll Loss: 2.1197\tTriple Loss(1): 0.4096\tClassification Loss: 1.3004\n","Train Epoch: 2 [89280/110534 (81%)]\tAll Loss: 1.6321\tTriple Loss(1): 0.1588\tClassification Loss: 1.3144\n","Train Epoch: 2 [89440/110534 (81%)]\tAll Loss: 2.1703\tTriple Loss(1): 0.1906\tClassification Loss: 1.7891\n","\n","Test set: Average loss: 1.6058, Accuracy: 262/480 (55%)\n","\n","Train Epoch: 2 [89600/110534 (81%)]\tAll Loss: 1.6600\tTriple Loss(0): 0.0000\tClassification Loss: 1.6600\n","Train Epoch: 2 [89760/110534 (81%)]\tAll Loss: 2.7254\tTriple Loss(1): 0.3764\tClassification Loss: 1.9726\n","Train Epoch: 2 [89920/110534 (81%)]\tAll Loss: 1.7069\tTriple Loss(1): 0.0961\tClassification Loss: 1.5146\n","Train Epoch: 2 [90080/110534 (81%)]\tAll Loss: 2.3537\tTriple Loss(1): 0.1437\tClassification Loss: 2.0663\n","Train Epoch: 2 [90240/110534 (82%)]\tAll Loss: 1.4790\tTriple Loss(1): 0.0164\tClassification Loss: 1.4462\n","Train Epoch: 2 [90400/110534 (82%)]\tAll Loss: 1.8298\tTriple Loss(0): 0.0000\tClassification Loss: 1.8298\n","Train Epoch: 2 [90560/110534 (82%)]\tAll Loss: 1.6492\tTriple Loss(1): 0.1723\tClassification Loss: 1.3046\n","Train Epoch: 2 [90720/110534 (82%)]\tAll Loss: 2.0642\tTriple Loss(1): 0.0272\tClassification Loss: 2.0099\n","Train Epoch: 2 [90880/110534 (82%)]\tAll Loss: 1.5744\tTriple Loss(1): 0.0809\tClassification Loss: 1.4125\n","Train Epoch: 2 [91040/110534 (82%)]\tAll Loss: 1.7920\tTriple Loss(1): 0.1452\tClassification Loss: 1.5015\n","\n","Test set: Average loss: 1.6119, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 2 [91200/110534 (83%)]\tAll Loss: 1.5284\tTriple Loss(0): 0.0000\tClassification Loss: 1.5284\n","Train Epoch: 2 [91360/110534 (83%)]\tAll Loss: 2.1017\tTriple Loss(1): 0.3692\tClassification Loss: 1.3633\n","Train Epoch: 2 [91520/110534 (83%)]\tAll Loss: 2.0549\tTriple Loss(1): 0.2472\tClassification Loss: 1.5606\n","Train Epoch: 2 [91680/110534 (83%)]\tAll Loss: 2.3537\tTriple Loss(1): 0.1477\tClassification Loss: 2.0583\n","Train Epoch: 2 [91840/110534 (83%)]\tAll Loss: 1.6807\tTriple Loss(1): 0.0000\tClassification Loss: 1.6807\n","Train Epoch: 2 [92000/110534 (83%)]\tAll Loss: 2.2026\tTriple Loss(1): 0.3671\tClassification Loss: 1.4684\n","Train Epoch: 2 [92160/110534 (83%)]\tAll Loss: 1.9990\tTriple Loss(1): 0.1681\tClassification Loss: 1.6628\n","Train Epoch: 2 [92320/110534 (84%)]\tAll Loss: 1.5941\tTriple Loss(0): 0.0000\tClassification Loss: 1.5941\n","Train Epoch: 2 [92480/110534 (84%)]\tAll Loss: 1.4881\tTriple Loss(0): 0.0000\tClassification Loss: 1.4881\n","Train Epoch: 2 [92640/110534 (84%)]\tAll Loss: 1.9585\tTriple Loss(0): 0.0000\tClassification Loss: 1.9585\n","\n","Test set: Average loss: 1.5471, Accuracy: 281/480 (59%)\n","\n","Train Epoch: 2 [92800/110534 (84%)]\tAll Loss: 1.9197\tTriple Loss(1): 0.1426\tClassification Loss: 1.6345\n","Train Epoch: 2 [92960/110534 (84%)]\tAll Loss: 2.4699\tTriple Loss(1): 0.2841\tClassification Loss: 1.9017\n","Train Epoch: 2 [93120/110534 (84%)]\tAll Loss: 1.9600\tTriple Loss(0): 0.0000\tClassification Loss: 1.9600\n","Train Epoch: 2 [93280/110534 (84%)]\tAll Loss: 1.7978\tTriple Loss(1): 0.0913\tClassification Loss: 1.6151\n","Train Epoch: 2 [93440/110534 (85%)]\tAll Loss: 2.0090\tTriple Loss(1): 0.2691\tClassification Loss: 1.4707\n","Train Epoch: 2 [93600/110534 (85%)]\tAll Loss: 2.4274\tTriple Loss(1): 0.2925\tClassification Loss: 1.8424\n","Train Epoch: 2 [93760/110534 (85%)]\tAll Loss: 1.8327\tTriple Loss(1): 0.0000\tClassification Loss: 1.8327\n","Train Epoch: 2 [93920/110534 (85%)]\tAll Loss: 2.4502\tTriple Loss(1): 0.2766\tClassification Loss: 1.8970\n","Train Epoch: 2 [94080/110534 (85%)]\tAll Loss: 1.8352\tTriple Loss(1): 0.2269\tClassification Loss: 1.3814\n","Train Epoch: 2 [94240/110534 (85%)]\tAll Loss: 1.5276\tTriple Loss(1): 0.0733\tClassification Loss: 1.3810\n","\n","Test set: Average loss: 1.5557, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 2 [94400/110534 (85%)]\tAll Loss: 1.6484\tTriple Loss(1): 0.1788\tClassification Loss: 1.2908\n","Train Epoch: 2 [94560/110534 (86%)]\tAll Loss: 1.7596\tTriple Loss(1): 0.2393\tClassification Loss: 1.2809\n","Train Epoch: 2 [94720/110534 (86%)]\tAll Loss: 1.5980\tTriple Loss(1): 0.0000\tClassification Loss: 1.5980\n","Train Epoch: 2 [94880/110534 (86%)]\tAll Loss: 1.9595\tTriple Loss(1): 0.3824\tClassification Loss: 1.1946\n","Train Epoch: 2 [95040/110534 (86%)]\tAll Loss: 1.8054\tTriple Loss(1): 0.0730\tClassification Loss: 1.6595\n","Train Epoch: 2 [95200/110534 (86%)]\tAll Loss: 2.2391\tTriple Loss(1): 0.2627\tClassification Loss: 1.7138\n","Train Epoch: 2 [95360/110534 (86%)]\tAll Loss: 2.4999\tTriple Loss(1): 0.1413\tClassification Loss: 2.2174\n","Train Epoch: 2 [95520/110534 (86%)]\tAll Loss: 6.2187\tTriple Loss(0): 2.4171\tClassification Loss: 1.3846\n","Train Epoch: 2 [95680/110534 (87%)]\tAll Loss: 2.2892\tTriple Loss(1): 0.3086\tClassification Loss: 1.6721\n","Train Epoch: 2 [95840/110534 (87%)]\tAll Loss: 2.2098\tTriple Loss(1): 0.0529\tClassification Loss: 2.1039\n","\n","Test set: Average loss: 1.5477, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 2 [96000/110534 (87%)]\tAll Loss: 2.5691\tTriple Loss(1): 0.4962\tClassification Loss: 1.5766\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_6000.pth.tar\n","Train Epoch: 2 [96160/110534 (87%)]\tAll Loss: 2.1362\tTriple Loss(1): 0.2251\tClassification Loss: 1.6859\n","Train Epoch: 2 [96320/110534 (87%)]\tAll Loss: 2.1520\tTriple Loss(1): 0.0944\tClassification Loss: 1.9631\n","Train Epoch: 2 [96480/110534 (87%)]\tAll Loss: 2.6207\tTriple Loss(1): 0.3571\tClassification Loss: 1.9064\n","Train Epoch: 2 [96640/110534 (87%)]\tAll Loss: 2.4115\tTriple Loss(1): 0.4979\tClassification Loss: 1.4158\n","Train Epoch: 2 [96800/110534 (88%)]\tAll Loss: 2.7400\tTriple Loss(1): 0.2256\tClassification Loss: 2.2887\n","Train Epoch: 2 [96960/110534 (88%)]\tAll Loss: 2.0084\tTriple Loss(0): 0.0000\tClassification Loss: 2.0084\n","Train Epoch: 2 [97120/110534 (88%)]\tAll Loss: 2.5653\tTriple Loss(1): 0.4734\tClassification Loss: 1.6184\n","Train Epoch: 2 [97280/110534 (88%)]\tAll Loss: 1.8013\tTriple Loss(1): 0.1620\tClassification Loss: 1.4774\n","Train Epoch: 2 [97440/110534 (88%)]\tAll Loss: 2.0605\tTriple Loss(1): 0.4099\tClassification Loss: 1.2408\n","\n","Test set: Average loss: 1.5487, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 2 [97600/110534 (88%)]\tAll Loss: 2.3343\tTriple Loss(1): 0.3609\tClassification Loss: 1.6126\n","Train Epoch: 2 [97760/110534 (88%)]\tAll Loss: 2.2812\tTriple Loss(1): 0.2690\tClassification Loss: 1.7433\n","Train Epoch: 2 [97920/110534 (89%)]\tAll Loss: 4.4349\tTriple Loss(0): 1.3981\tClassification Loss: 1.6387\n","Train Epoch: 2 [98080/110534 (89%)]\tAll Loss: 1.1785\tTriple Loss(0): 0.0000\tClassification Loss: 1.1785\n","Train Epoch: 2 [98240/110534 (89%)]\tAll Loss: 1.1373\tTriple Loss(0): 0.0000\tClassification Loss: 1.1373\n","Train Epoch: 2 [98400/110534 (89%)]\tAll Loss: 1.1769\tTriple Loss(0): 0.0000\tClassification Loss: 1.1769\n","Train Epoch: 2 [98560/110534 (89%)]\tAll Loss: 2.0585\tTriple Loss(1): 0.5534\tClassification Loss: 0.9516\n","Train Epoch: 2 [98720/110534 (89%)]\tAll Loss: 2.2200\tTriple Loss(1): 0.5122\tClassification Loss: 1.1955\n","Train Epoch: 2 [98880/110534 (89%)]\tAll Loss: 1.7139\tTriple Loss(1): 0.3422\tClassification Loss: 1.0295\n","Train Epoch: 2 [99040/110534 (90%)]\tAll Loss: 1.8638\tTriple Loss(1): 0.2284\tClassification Loss: 1.4070\n","\n","Test set: Average loss: 1.5584, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 2 [99200/110534 (90%)]\tAll Loss: 2.5532\tTriple Loss(1): 0.6017\tClassification Loss: 1.3499\n","Train Epoch: 2 [99360/110534 (90%)]\tAll Loss: 1.5155\tTriple Loss(0): 0.0000\tClassification Loss: 1.5155\n","Train Epoch: 2 [99520/110534 (90%)]\tAll Loss: 1.8792\tTriple Loss(1): 0.1682\tClassification Loss: 1.5429\n","Train Epoch: 2 [99680/110534 (90%)]\tAll Loss: 1.9263\tTriple Loss(1): 0.0922\tClassification Loss: 1.7418\n","Train Epoch: 2 [99840/110534 (90%)]\tAll Loss: 2.2239\tTriple Loss(1): 0.0144\tClassification Loss: 2.1951\n","Train Epoch: 2 [100000/110534 (90%)]\tAll Loss: 2.1028\tTriple Loss(1): 0.2099\tClassification Loss: 1.6830\n","Train Epoch: 2 [100160/110534 (91%)]\tAll Loss: 2.9217\tTriple Loss(1): 0.3354\tClassification Loss: 2.2509\n","Train Epoch: 2 [100320/110534 (91%)]\tAll Loss: 1.2900\tTriple Loss(0): 0.0000\tClassification Loss: 1.2900\n","Train Epoch: 2 [100480/110534 (91%)]\tAll Loss: 3.2922\tTriple Loss(1): 0.6868\tClassification Loss: 1.9186\n","Train Epoch: 2 [100640/110534 (91%)]\tAll Loss: 8.8895\tTriple Loss(0): 3.4770\tClassification Loss: 1.9354\n","\n","Test set: Average loss: 1.5398, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 2 [100800/110534 (91%)]\tAll Loss: 2.6917\tTriple Loss(1): 0.2823\tClassification Loss: 2.1272\n","Train Epoch: 2 [100960/110534 (91%)]\tAll Loss: 1.8858\tTriple Loss(1): 0.0951\tClassification Loss: 1.6957\n","Train Epoch: 2 [101120/110534 (91%)]\tAll Loss: 1.8598\tTriple Loss(1): 0.0530\tClassification Loss: 1.7537\n","Train Epoch: 2 [101280/110534 (92%)]\tAll Loss: 1.4334\tTriple Loss(1): 0.1485\tClassification Loss: 1.1364\n","Train Epoch: 2 [101440/110534 (92%)]\tAll Loss: 1.2044\tTriple Loss(1): 0.0046\tClassification Loss: 1.1953\n","Train Epoch: 2 [101600/110534 (92%)]\tAll Loss: 17.6106\tTriple Loss(0): 8.1364\tClassification Loss: 1.3378\n","Train Epoch: 2 [101760/110534 (92%)]\tAll Loss: 1.8870\tTriple Loss(1): 0.0831\tClassification Loss: 1.7208\n","Train Epoch: 2 [101920/110534 (92%)]\tAll Loss: 2.1720\tTriple Loss(1): 0.4053\tClassification Loss: 1.3613\n","Train Epoch: 2 [102080/110534 (92%)]\tAll Loss: 1.4089\tTriple Loss(1): 0.1258\tClassification Loss: 1.1573\n","Train Epoch: 2 [102240/110534 (92%)]\tAll Loss: 1.4517\tTriple Loss(1): 0.1380\tClassification Loss: 1.1756\n","\n","Test set: Average loss: 1.5757, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 2 [102400/110534 (93%)]\tAll Loss: 1.5512\tTriple Loss(1): 0.1225\tClassification Loss: 1.3062\n","Train Epoch: 2 [102560/110534 (93%)]\tAll Loss: 2.5962\tTriple Loss(1): 0.6559\tClassification Loss: 1.2844\n","Train Epoch: 2 [102720/110534 (93%)]\tAll Loss: 0.8125\tTriple Loss(0): 0.0000\tClassification Loss: 0.8125\n","Train Epoch: 2 [102880/110534 (93%)]\tAll Loss: 1.4695\tTriple Loss(1): 0.0717\tClassification Loss: 1.3262\n","Train Epoch: 2 [103040/110534 (93%)]\tAll Loss: 1.4070\tTriple Loss(1): 0.0212\tClassification Loss: 1.3647\n","Train Epoch: 2 [103200/110534 (93%)]\tAll Loss: 2.2092\tTriple Loss(1): 0.0054\tClassification Loss: 2.1984\n","Train Epoch: 2 [103360/110534 (94%)]\tAll Loss: 1.6246\tTriple Loss(1): 0.2563\tClassification Loss: 1.1121\n","Train Epoch: 2 [103520/110534 (94%)]\tAll Loss: 2.0634\tTriple Loss(0): 0.0000\tClassification Loss: 2.0634\n","Train Epoch: 2 [103680/110534 (94%)]\tAll Loss: 1.9176\tTriple Loss(1): 0.1620\tClassification Loss: 1.5936\n","Train Epoch: 2 [103840/110534 (94%)]\tAll Loss: 1.6900\tTriple Loss(1): 0.1527\tClassification Loss: 1.3846\n","\n","Test set: Average loss: 1.5406, Accuracy: 272/480 (57%)\n","\n","Train Epoch: 2 [104000/110534 (94%)]\tAll Loss: 1.0893\tTriple Loss(0): 0.0000\tClassification Loss: 1.0893\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_6500.pth.tar\n","Train Epoch: 2 [104160/110534 (94%)]\tAll Loss: 1.3896\tTriple Loss(0): 0.0000\tClassification Loss: 1.3896\n","Train Epoch: 2 [104320/110534 (94%)]\tAll Loss: 2.4351\tTriple Loss(1): 0.2942\tClassification Loss: 1.8467\n","Train Epoch: 2 [104480/110534 (95%)]\tAll Loss: 2.9009\tTriple Loss(1): 0.6107\tClassification Loss: 1.6796\n","Train Epoch: 2 [104640/110534 (95%)]\tAll Loss: 2.0416\tTriple Loss(1): 0.1184\tClassification Loss: 1.8048\n","Train Epoch: 2 [104800/110534 (95%)]\tAll Loss: 1.5749\tTriple Loss(0): 0.0000\tClassification Loss: 1.5749\n","Train Epoch: 2 [104960/110534 (95%)]\tAll Loss: 2.3836\tTriple Loss(1): 0.2237\tClassification Loss: 1.9363\n","Train Epoch: 2 [105120/110534 (95%)]\tAll Loss: 1.6028\tTriple Loss(0): 0.0000\tClassification Loss: 1.6028\n","Train Epoch: 2 [105280/110534 (95%)]\tAll Loss: 1.9542\tTriple Loss(1): 0.2268\tClassification Loss: 1.5007\n","Train Epoch: 2 [105440/110534 (95%)]\tAll Loss: 1.5608\tTriple Loss(0): 0.0000\tClassification Loss: 1.5608\n","\n","Test set: Average loss: 1.5236, Accuracy: 277/480 (58%)\n","\n","Train Epoch: 2 [105600/110534 (96%)]\tAll Loss: 2.0773\tTriple Loss(1): 0.0210\tClassification Loss: 2.0353\n","Train Epoch: 2 [105760/110534 (96%)]\tAll Loss: 1.7969\tTriple Loss(1): 0.2228\tClassification Loss: 1.3512\n","Train Epoch: 2 [105920/110534 (96%)]\tAll Loss: 2.1420\tTriple Loss(1): 0.1925\tClassification Loss: 1.7570\n","Train Epoch: 2 [106080/110534 (96%)]\tAll Loss: 1.0293\tTriple Loss(0): 0.0000\tClassification Loss: 1.0293\n","Train Epoch: 2 [106240/110534 (96%)]\tAll Loss: 2.1771\tTriple Loss(1): 0.0348\tClassification Loss: 2.1075\n","Train Epoch: 2 [106400/110534 (96%)]\tAll Loss: 1.7903\tTriple Loss(0): 0.0000\tClassification Loss: 1.7903\n","Train Epoch: 2 [106560/110534 (96%)]\tAll Loss: 1.3711\tTriple Loss(1): 0.0576\tClassification Loss: 1.2558\n","Train Epoch: 2 [106720/110534 (97%)]\tAll Loss: 2.0482\tTriple Loss(1): 0.2392\tClassification Loss: 1.5697\n","Train Epoch: 2 [106880/110534 (97%)]\tAll Loss: 1.9823\tTriple Loss(1): 0.2319\tClassification Loss: 1.5185\n","Train Epoch: 2 [107040/110534 (97%)]\tAll Loss: 2.3147\tTriple Loss(1): 0.0666\tClassification Loss: 2.1816\n","\n","Test set: Average loss: 1.5752, Accuracy: 272/480 (57%)\n","\n","Train Epoch: 2 [107200/110534 (97%)]\tAll Loss: 1.6003\tTriple Loss(1): 0.1226\tClassification Loss: 1.3551\n","Train Epoch: 2 [107360/110534 (97%)]\tAll Loss: 2.5045\tTriple Loss(1): 0.3217\tClassification Loss: 1.8611\n","Train Epoch: 2 [107520/110534 (97%)]\tAll Loss: 2.0382\tTriple Loss(1): 0.3749\tClassification Loss: 1.2883\n","Train Epoch: 2 [107680/110534 (97%)]\tAll Loss: 1.7952\tTriple Loss(1): 0.0000\tClassification Loss: 1.7952\n","Train Epoch: 2 [107840/110534 (98%)]\tAll Loss: 1.5885\tTriple Loss(1): 0.1578\tClassification Loss: 1.2730\n","Train Epoch: 2 [108000/110534 (98%)]\tAll Loss: 1.7826\tTriple Loss(1): 0.2364\tClassification Loss: 1.3099\n","Train Epoch: 2 [108160/110534 (98%)]\tAll Loss: 1.8306\tTriple Loss(0): 0.0000\tClassification Loss: 1.8306\n","Train Epoch: 2 [108320/110534 (98%)]\tAll Loss: 2.6178\tTriple Loss(1): 0.3143\tClassification Loss: 1.9891\n","Train Epoch: 2 [108480/110534 (98%)]\tAll Loss: 1.2232\tTriple Loss(1): 0.0899\tClassification Loss: 1.0435\n","Train Epoch: 2 [108640/110534 (98%)]\tAll Loss: 0.9227\tTriple Loss(0): 0.0000\tClassification Loss: 0.9227\n","\n","Test set: Average loss: 1.5571, Accuracy: 281/480 (59%)\n","\n","Train Epoch: 2 [108800/110534 (98%)]\tAll Loss: 2.3309\tTriple Loss(1): 0.1001\tClassification Loss: 2.1308\n","Train Epoch: 2 [108960/110534 (99%)]\tAll Loss: 1.8940\tTriple Loss(1): 0.2037\tClassification Loss: 1.4867\n","Train Epoch: 2 [109120/110534 (99%)]\tAll Loss: 1.1311\tTriple Loss(0): 0.0000\tClassification Loss: 1.1311\n","Train Epoch: 2 [109280/110534 (99%)]\tAll Loss: 1.4444\tTriple Loss(0): 0.0000\tClassification Loss: 1.4444\n","Train Epoch: 2 [109440/110534 (99%)]\tAll Loss: 2.0622\tTriple Loss(1): 0.2795\tClassification Loss: 1.5032\n","Train Epoch: 2 [109600/110534 (99%)]\tAll Loss: 1.8350\tTriple Loss(1): 0.0215\tClassification Loss: 1.7920\n","Train Epoch: 2 [109760/110534 (99%)]\tAll Loss: 2.3059\tTriple Loss(1): 0.3776\tClassification Loss: 1.5507\n","Train Epoch: 2 [109920/110534 (99%)]\tAll Loss: 1.9968\tTriple Loss(1): 0.3445\tClassification Loss: 1.3079\n","Train Epoch: 2 [110080/110534 (100%)]\tAll Loss: 2.7193\tTriple Loss(1): 0.3544\tClassification Loss: 2.0105\n","Train Epoch: 2 [110240/110534 (100%)]\tAll Loss: 1.8296\tTriple Loss(1): 0.2369\tClassification Loss: 1.3558\n","\n","Test set: Average loss: 1.5807, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 2 [110400/110534 (100%)]\tAll Loss: 2.4018\tTriple Loss(1): 0.1091\tClassification Loss: 2.1837\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_final.pth.tar\n","\n","Test set: Average loss: 1.5817, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 3 [0/110534 (0%)]\tAll Loss: 1.6751\tTriple Loss(1): 0.0786\tClassification Loss: 1.5179\n","Train Epoch: 3 [160/110534 (0%)]\tAll Loss: 1.8031\tTriple Loss(1): 0.0970\tClassification Loss: 1.6091\n","Train Epoch: 3 [320/110534 (0%)]\tAll Loss: 1.5273\tTriple Loss(1): 0.0218\tClassification Loss: 1.4837\n","Train Epoch: 3 [480/110534 (0%)]\tAll Loss: 1.8743\tTriple Loss(1): 0.2018\tClassification Loss: 1.4708\n","Train Epoch: 3 [640/110534 (1%)]\tAll Loss: 2.2672\tTriple Loss(1): 0.3907\tClassification Loss: 1.4859\n","Train Epoch: 3 [800/110534 (1%)]\tAll Loss: 1.9218\tTriple Loss(1): 0.3283\tClassification Loss: 1.2651\n","Train Epoch: 3 [960/110534 (1%)]\tAll Loss: 1.3206\tTriple Loss(1): 0.0000\tClassification Loss: 1.3206\n","Train Epoch: 3 [1120/110534 (1%)]\tAll Loss: 2.0316\tTriple Loss(0): 0.0000\tClassification Loss: 2.0316\n","Train Epoch: 3 [1280/110534 (1%)]\tAll Loss: 2.2092\tTriple Loss(1): 0.1537\tClassification Loss: 1.9018\n","Train Epoch: 3 [1440/110534 (1%)]\tAll Loss: 1.5706\tTriple Loss(1): 0.0486\tClassification Loss: 1.4733\n","\n","Test set: Average loss: 1.5449, Accuracy: 278/480 (58%)\n","\n","Train Epoch: 3 [1600/110534 (1%)]\tAll Loss: 1.6491\tTriple Loss(1): 0.2378\tClassification Loss: 1.1735\n","Train Epoch: 3 [1760/110534 (2%)]\tAll Loss: 1.5572\tTriple Loss(1): 0.1241\tClassification Loss: 1.3090\n","Train Epoch: 3 [1920/110534 (2%)]\tAll Loss: 2.1497\tTriple Loss(1): 0.2725\tClassification Loss: 1.6048\n","Train Epoch: 3 [2080/110534 (2%)]\tAll Loss: 2.7899\tTriple Loss(1): 0.3030\tClassification Loss: 2.1838\n","Train Epoch: 3 [2240/110534 (2%)]\tAll Loss: 1.9774\tTriple Loss(1): 0.0927\tClassification Loss: 1.7919\n","Train Epoch: 3 [2400/110534 (2%)]\tAll Loss: 1.7625\tTriple Loss(0): 0.0000\tClassification Loss: 1.7625\n","Train Epoch: 3 [2560/110534 (2%)]\tAll Loss: 1.2008\tTriple Loss(1): 0.1731\tClassification Loss: 0.8546\n","Train Epoch: 3 [2720/110534 (2%)]\tAll Loss: 1.2828\tTriple Loss(0): 0.0000\tClassification Loss: 1.2828\n","Train Epoch: 3 [2880/110534 (3%)]\tAll Loss: 2.6781\tTriple Loss(1): 0.2672\tClassification Loss: 2.1436\n","Train Epoch: 3 [3040/110534 (3%)]\tAll Loss: 1.2089\tTriple Loss(1): 0.0216\tClassification Loss: 1.1658\n","\n","Test set: Average loss: 1.5411, Accuracy: 280/480 (58%)\n","\n","Train Epoch: 3 [3200/110534 (3%)]\tAll Loss: 1.0309\tTriple Loss(0): 0.0000\tClassification Loss: 1.0309\n","Train Epoch: 3 [3360/110534 (3%)]\tAll Loss: 1.7638\tTriple Loss(1): 0.3065\tClassification Loss: 1.1507\n","Train Epoch: 3 [3520/110534 (3%)]\tAll Loss: 2.7711\tTriple Loss(1): 0.4026\tClassification Loss: 1.9659\n","Train Epoch: 3 [3680/110534 (3%)]\tAll Loss: 1.5502\tTriple Loss(0): 0.0000\tClassification Loss: 1.5502\n","Train Epoch: 3 [3840/110534 (3%)]\tAll Loss: 3.4122\tTriple Loss(1): 0.7006\tClassification Loss: 2.0110\n","Train Epoch: 3 [4000/110534 (4%)]\tAll Loss: 1.7203\tTriple Loss(1): 0.1833\tClassification Loss: 1.3537\n","Train Epoch: 3 [4160/110534 (4%)]\tAll Loss: 2.5352\tTriple Loss(1): 0.4038\tClassification Loss: 1.7276\n","Train Epoch: 3 [4320/110534 (4%)]\tAll Loss: 1.9737\tTriple Loss(1): 0.2171\tClassification Loss: 1.5395\n","Train Epoch: 3 [4480/110534 (4%)]\tAll Loss: 2.1974\tTriple Loss(1): 0.1900\tClassification Loss: 1.8174\n","Train Epoch: 3 [4640/110534 (4%)]\tAll Loss: 2.1358\tTriple Loss(1): 0.3094\tClassification Loss: 1.5170\n","\n","Test set: Average loss: 1.5653, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 3 [4800/110534 (4%)]\tAll Loss: 2.2165\tTriple Loss(1): 0.2927\tClassification Loss: 1.6311\n","Train Epoch: 3 [4960/110534 (4%)]\tAll Loss: 1.6574\tTriple Loss(1): 0.3456\tClassification Loss: 0.9661\n","Train Epoch: 3 [5120/110534 (5%)]\tAll Loss: 2.4964\tTriple Loss(1): 0.3120\tClassification Loss: 1.8724\n","Train Epoch: 3 [5280/110534 (5%)]\tAll Loss: 0.9200\tTriple Loss(0): 0.0000\tClassification Loss: 0.9200\n","Train Epoch: 3 [5440/110534 (5%)]\tAll Loss: 1.9791\tTriple Loss(1): 0.3605\tClassification Loss: 1.2582\n","Train Epoch: 3 [5600/110534 (5%)]\tAll Loss: 1.4189\tTriple Loss(1): 0.0000\tClassification Loss: 1.4189\n","Train Epoch: 3 [5760/110534 (5%)]\tAll Loss: 1.2203\tTriple Loss(1): 0.0329\tClassification Loss: 1.1545\n","Train Epoch: 3 [5920/110534 (5%)]\tAll Loss: 1.5329\tTriple Loss(0): 0.0000\tClassification Loss: 1.5329\n","Train Epoch: 3 [6080/110534 (6%)]\tAll Loss: 2.7778\tTriple Loss(1): 0.6954\tClassification Loss: 1.3869\n","Train Epoch: 3 [6240/110534 (6%)]\tAll Loss: 2.7824\tTriple Loss(1): 0.5138\tClassification Loss: 1.7547\n","\n","Test set: Average loss: 1.5547, Accuracy: 278/480 (58%)\n","\n","Train Epoch: 3 [6400/110534 (6%)]\tAll Loss: 2.1747\tTriple Loss(1): 0.5245\tClassification Loss: 1.1257\n","Train Epoch: 3 [6560/110534 (6%)]\tAll Loss: 2.3471\tTriple Loss(1): 0.4294\tClassification Loss: 1.4882\n","Train Epoch: 3 [6720/110534 (6%)]\tAll Loss: 2.0473\tTriple Loss(1): 0.0946\tClassification Loss: 1.8580\n","Train Epoch: 3 [6880/110534 (6%)]\tAll Loss: 1.6249\tTriple Loss(1): 0.2743\tClassification Loss: 1.0763\n","Train Epoch: 3 [7040/110534 (6%)]\tAll Loss: 1.3564\tTriple Loss(0): 0.0000\tClassification Loss: 1.3564\n","Train Epoch: 3 [7200/110534 (7%)]\tAll Loss: 1.8445\tTriple Loss(1): 0.0144\tClassification Loss: 1.8157\n","Train Epoch: 3 [7360/110534 (7%)]\tAll Loss: 1.6426\tTriple Loss(1): 0.0395\tClassification Loss: 1.5636\n","Train Epoch: 3 [7520/110534 (7%)]\tAll Loss: 1.7432\tTriple Loss(1): 0.1176\tClassification Loss: 1.5080\n","Train Epoch: 3 [7680/110534 (7%)]\tAll Loss: 1.9561\tTriple Loss(1): 0.1433\tClassification Loss: 1.6696\n","Train Epoch: 3 [7840/110534 (7%)]\tAll Loss: 1.4193\tTriple Loss(1): 0.0028\tClassification Loss: 1.4138\n","\n","Test set: Average loss: 1.5567, Accuracy: 275/480 (57%)\n","\n","Train Epoch: 3 [8000/110534 (7%)]\tAll Loss: 1.9661\tTriple Loss(1): 0.1320\tClassification Loss: 1.7022\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_500.pth.tar\n","Train Epoch: 3 [8160/110534 (7%)]\tAll Loss: 1.9184\tTriple Loss(0): 0.0000\tClassification Loss: 1.9184\n","Train Epoch: 3 [8320/110534 (8%)]\tAll Loss: 2.4463\tTriple Loss(1): 0.4315\tClassification Loss: 1.5833\n","Train Epoch: 3 [8480/110534 (8%)]\tAll Loss: 2.0291\tTriple Loss(1): 0.1704\tClassification Loss: 1.6883\n","Train Epoch: 3 [8640/110534 (8%)]\tAll Loss: 1.5214\tTriple Loss(1): 0.1314\tClassification Loss: 1.2587\n","Train Epoch: 3 [8800/110534 (8%)]\tAll Loss: 2.0954\tTriple Loss(1): 0.3325\tClassification Loss: 1.4303\n","Train Epoch: 3 [8960/110534 (8%)]\tAll Loss: 1.6507\tTriple Loss(1): 0.0000\tClassification Loss: 1.6507\n","Train Epoch: 3 [9120/110534 (8%)]\tAll Loss: 1.8761\tTriple Loss(1): 0.0798\tClassification Loss: 1.7165\n","Train Epoch: 3 [9280/110534 (8%)]\tAll Loss: 1.7110\tTriple Loss(0): 0.0000\tClassification Loss: 1.7110\n","Train Epoch: 3 [9440/110534 (9%)]\tAll Loss: 2.4535\tTriple Loss(1): 0.3095\tClassification Loss: 1.8344\n","\n","Test set: Average loss: 1.5576, Accuracy: 271/480 (56%)\n","\n","Train Epoch: 3 [9600/110534 (9%)]\tAll Loss: 2.3393\tTriple Loss(1): 0.3130\tClassification Loss: 1.7132\n","Train Epoch: 3 [9760/110534 (9%)]\tAll Loss: 2.7402\tTriple Loss(1): 0.3255\tClassification Loss: 2.0891\n","Train Epoch: 3 [9920/110534 (9%)]\tAll Loss: 1.9837\tTriple Loss(1): 0.1728\tClassification Loss: 1.6382\n","Train Epoch: 3 [10080/110534 (9%)]\tAll Loss: 1.3916\tTriple Loss(1): 0.1441\tClassification Loss: 1.1034\n","Train Epoch: 3 [10240/110534 (9%)]\tAll Loss: 1.9262\tTriple Loss(1): 0.0847\tClassification Loss: 1.7568\n","Train Epoch: 3 [10400/110534 (9%)]\tAll Loss: 1.6907\tTriple Loss(1): 0.1438\tClassification Loss: 1.4031\n","Train Epoch: 3 [10560/110534 (10%)]\tAll Loss: 1.8277\tTriple Loss(1): 0.0016\tClassification Loss: 1.8245\n","Train Epoch: 3 [10720/110534 (10%)]\tAll Loss: 2.0287\tTriple Loss(1): 0.3509\tClassification Loss: 1.3268\n","Train Epoch: 3 [10880/110534 (10%)]\tAll Loss: 1.4574\tTriple Loss(0): 0.0000\tClassification Loss: 1.4574\n","Train Epoch: 3 [11040/110534 (10%)]\tAll Loss: 3.6549\tTriple Loss(1): 0.7535\tClassification Loss: 2.1479\n","\n","Test set: Average loss: 1.5415, Accuracy: 277/480 (58%)\n","\n","Train Epoch: 3 [11200/110534 (10%)]\tAll Loss: 2.0311\tTriple Loss(1): 0.1261\tClassification Loss: 1.7788\n","Train Epoch: 3 [11360/110534 (10%)]\tAll Loss: 2.0086\tTriple Loss(1): 0.0903\tClassification Loss: 1.8281\n","Train Epoch: 3 [11520/110534 (10%)]\tAll Loss: 2.8373\tTriple Loss(1): 0.2833\tClassification Loss: 2.2706\n","Train Epoch: 3 [11680/110534 (11%)]\tAll Loss: 1.6338\tTriple Loss(1): 0.1860\tClassification Loss: 1.2618\n","Train Epoch: 3 [11840/110534 (11%)]\tAll Loss: 1.9175\tTriple Loss(0): 0.0000\tClassification Loss: 1.9175\n","Train Epoch: 3 [12000/110534 (11%)]\tAll Loss: 2.1826\tTriple Loss(1): 0.0079\tClassification Loss: 2.1669\n","Train Epoch: 3 [12160/110534 (11%)]\tAll Loss: 1.7933\tTriple Loss(1): 0.3603\tClassification Loss: 1.0728\n","Train Epoch: 3 [12320/110534 (11%)]\tAll Loss: 2.3541\tTriple Loss(1): 0.2733\tClassification Loss: 1.8074\n","Train Epoch: 3 [12480/110534 (11%)]\tAll Loss: 1.5806\tTriple Loss(0): 0.0000\tClassification Loss: 1.5806\n","Train Epoch: 3 [12640/110534 (11%)]\tAll Loss: 1.9590\tTriple Loss(1): 0.1140\tClassification Loss: 1.7310\n","\n","Test set: Average loss: 1.5506, Accuracy: 280/480 (58%)\n","\n","Train Epoch: 3 [12800/110534 (12%)]\tAll Loss: 1.5644\tTriple Loss(0): 0.0000\tClassification Loss: 1.5644\n","Train Epoch: 3 [12960/110534 (12%)]\tAll Loss: 1.8431\tTriple Loss(1): 0.0307\tClassification Loss: 1.7816\n","Train Epoch: 3 [13120/110534 (12%)]\tAll Loss: 2.1649\tTriple Loss(1): 0.3118\tClassification Loss: 1.5413\n","Train Epoch: 3 [13280/110534 (12%)]\tAll Loss: 2.4902\tTriple Loss(1): 0.2424\tClassification Loss: 2.0053\n","Train Epoch: 3 [13440/110534 (12%)]\tAll Loss: 1.2775\tTriple Loss(0): 0.0000\tClassification Loss: 1.2775\n","Train Epoch: 3 [13600/110534 (12%)]\tAll Loss: 1.6028\tTriple Loss(0): 0.0000\tClassification Loss: 1.6028\n","Train Epoch: 3 [13760/110534 (12%)]\tAll Loss: 1.8882\tTriple Loss(1): 0.1761\tClassification Loss: 1.5360\n","Train Epoch: 3 [13920/110534 (13%)]\tAll Loss: 1.2278\tTriple Loss(1): 0.0145\tClassification Loss: 1.1988\n","Train Epoch: 3 [14080/110534 (13%)]\tAll Loss: 1.3522\tTriple Loss(0): 0.0000\tClassification Loss: 1.3522\n","Train Epoch: 3 [14240/110534 (13%)]\tAll Loss: 1.6204\tTriple Loss(1): 0.1229\tClassification Loss: 1.3745\n","\n","Test set: Average loss: 1.5499, Accuracy: 275/480 (57%)\n","\n","Train Epoch: 3 [14400/110534 (13%)]\tAll Loss: 2.3421\tTriple Loss(1): 0.1617\tClassification Loss: 2.0187\n","Train Epoch: 3 [14560/110534 (13%)]\tAll Loss: 1.5154\tTriple Loss(1): 0.3911\tClassification Loss: 0.7331\n","Train Epoch: 3 [14720/110534 (13%)]\tAll Loss: 2.4047\tTriple Loss(1): 0.3645\tClassification Loss: 1.6757\n","Train Epoch: 3 [14880/110534 (13%)]\tAll Loss: 2.2673\tTriple Loss(1): 0.2775\tClassification Loss: 1.7124\n","Train Epoch: 3 [15040/110534 (14%)]\tAll Loss: 4.8245\tTriple Loss(0): 1.6817\tClassification Loss: 1.4611\n","Train Epoch: 3 [15200/110534 (14%)]\tAll Loss: 1.8408\tTriple Loss(1): 0.0346\tClassification Loss: 1.7716\n","Train Epoch: 3 [15360/110534 (14%)]\tAll Loss: 3.0676\tTriple Loss(1): 0.8303\tClassification Loss: 1.4071\n","Train Epoch: 3 [15520/110534 (14%)]\tAll Loss: 2.0817\tTriple Loss(0): 0.0000\tClassification Loss: 2.0817\n","Train Epoch: 3 [15680/110534 (14%)]\tAll Loss: 2.3233\tTriple Loss(1): 0.1784\tClassification Loss: 1.9665\n","Train Epoch: 3 [15840/110534 (14%)]\tAll Loss: 7.1172\tTriple Loss(0): 2.7223\tClassification Loss: 1.6726\n","\n","Test set: Average loss: 1.5634, Accuracy: 271/480 (56%)\n","\n","Train Epoch: 3 [16000/110534 (14%)]\tAll Loss: 2.0884\tTriple Loss(1): 0.1600\tClassification Loss: 1.7684\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_1000.pth.tar\n","Train Epoch: 3 [16160/110534 (15%)]\tAll Loss: 1.4561\tTriple Loss(1): 0.3108\tClassification Loss: 0.8345\n","Train Epoch: 3 [16320/110534 (15%)]\tAll Loss: 1.0090\tTriple Loss(0): 0.0000\tClassification Loss: 1.0090\n","Train Epoch: 3 [16480/110534 (15%)]\tAll Loss: 1.8678\tTriple Loss(1): 0.1774\tClassification Loss: 1.5129\n","Train Epoch: 3 [16640/110534 (15%)]\tAll Loss: 1.7320\tTriple Loss(1): 0.0000\tClassification Loss: 1.7320\n","Train Epoch: 3 [16800/110534 (15%)]\tAll Loss: 1.1727\tTriple Loss(0): 0.0000\tClassification Loss: 1.1727\n","Train Epoch: 3 [16960/110534 (15%)]\tAll Loss: 2.1979\tTriple Loss(1): 0.0678\tClassification Loss: 2.0623\n","Train Epoch: 3 [17120/110534 (15%)]\tAll Loss: 2.1072\tTriple Loss(1): 0.1696\tClassification Loss: 1.7681\n","Train Epoch: 3 [17280/110534 (16%)]\tAll Loss: 1.6183\tTriple Loss(1): 0.0000\tClassification Loss: 1.6183\n","Train Epoch: 3 [17440/110534 (16%)]\tAll Loss: 1.7426\tTriple Loss(1): 0.1667\tClassification Loss: 1.4092\n","\n","Test set: Average loss: 1.5406, Accuracy: 266/480 (55%)\n","\n","Train Epoch: 3 [17600/110534 (16%)]\tAll Loss: 1.4088\tTriple Loss(1): 0.0992\tClassification Loss: 1.2104\n","Train Epoch: 3 [17760/110534 (16%)]\tAll Loss: 2.0306\tTriple Loss(1): 0.1586\tClassification Loss: 1.7133\n","Train Epoch: 3 [17920/110534 (16%)]\tAll Loss: 1.8384\tTriple Loss(0): 0.0000\tClassification Loss: 1.8384\n","Train Epoch: 3 [18080/110534 (16%)]\tAll Loss: 1.8681\tTriple Loss(1): 0.2141\tClassification Loss: 1.4399\n","Train Epoch: 3 [18240/110534 (17%)]\tAll Loss: 2.5843\tTriple Loss(1): 0.5785\tClassification Loss: 1.4273\n","Train Epoch: 3 [18400/110534 (17%)]\tAll Loss: 1.3962\tTriple Loss(0): 0.0000\tClassification Loss: 1.3962\n","Train Epoch: 3 [18560/110534 (17%)]\tAll Loss: 2.0689\tTriple Loss(0): 0.0000\tClassification Loss: 2.0689\n","Train Epoch: 3 [18720/110534 (17%)]\tAll Loss: 2.2272\tTriple Loss(1): 0.2300\tClassification Loss: 1.7671\n","Train Epoch: 3 [18880/110534 (17%)]\tAll Loss: 1.7580\tTriple Loss(0): 0.0000\tClassification Loss: 1.7580\n","Train Epoch: 3 [19040/110534 (17%)]\tAll Loss: 1.0730\tTriple Loss(0): 0.0000\tClassification Loss: 1.0730\n","\n","Test set: Average loss: 1.5567, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 3 [19200/110534 (17%)]\tAll Loss: 7.4005\tTriple Loss(0): 2.6467\tClassification Loss: 2.1071\n","Train Epoch: 3 [19360/110534 (18%)]\tAll Loss: 1.8734\tTriple Loss(1): 0.1883\tClassification Loss: 1.4969\n","Train Epoch: 3 [19520/110534 (18%)]\tAll Loss: 2.1251\tTriple Loss(1): 0.2459\tClassification Loss: 1.6333\n","Train Epoch: 3 [19680/110534 (18%)]\tAll Loss: 1.1969\tTriple Loss(1): 0.0481\tClassification Loss: 1.1008\n","Train Epoch: 3 [19840/110534 (18%)]\tAll Loss: 1.6439\tTriple Loss(0): 0.0000\tClassification Loss: 1.6439\n","Train Epoch: 3 [20000/110534 (18%)]\tAll Loss: 2.0643\tTriple Loss(1): 0.2172\tClassification Loss: 1.6299\n","Train Epoch: 3 [20160/110534 (18%)]\tAll Loss: 1.7864\tTriple Loss(1): 0.0000\tClassification Loss: 1.7864\n","Train Epoch: 3 [20320/110534 (18%)]\tAll Loss: 2.5112\tTriple Loss(1): 0.2937\tClassification Loss: 1.9238\n","Train Epoch: 3 [20480/110534 (19%)]\tAll Loss: 1.9005\tTriple Loss(1): 0.1477\tClassification Loss: 1.6050\n","Train Epoch: 3 [20640/110534 (19%)]\tAll Loss: 1.6061\tTriple Loss(1): 0.0146\tClassification Loss: 1.5769\n","\n","Test set: Average loss: 1.5362, Accuracy: 281/480 (59%)\n","\n","Train Epoch: 3 [20800/110534 (19%)]\tAll Loss: 1.9420\tTriple Loss(1): 0.1905\tClassification Loss: 1.5609\n","Train Epoch: 3 [20960/110534 (19%)]\tAll Loss: 1.9712\tTriple Loss(1): 0.2835\tClassification Loss: 1.4043\n","Train Epoch: 3 [21120/110534 (19%)]\tAll Loss: 1.5566\tTriple Loss(0): 0.0000\tClassification Loss: 1.5566\n","Train Epoch: 3 [21280/110534 (19%)]\tAll Loss: 1.5853\tTriple Loss(1): 0.0000\tClassification Loss: 1.5853\n","Train Epoch: 3 [21440/110534 (19%)]\tAll Loss: 1.7597\tTriple Loss(1): 0.3426\tClassification Loss: 1.0745\n","Train Epoch: 3 [21600/110534 (20%)]\tAll Loss: 2.5483\tTriple Loss(1): 0.3818\tClassification Loss: 1.7846\n","Train Epoch: 3 [21760/110534 (20%)]\tAll Loss: 1.1673\tTriple Loss(1): 0.0055\tClassification Loss: 1.1563\n","Train Epoch: 3 [21920/110534 (20%)]\tAll Loss: 1.1096\tTriple Loss(1): 0.0489\tClassification Loss: 1.0117\n","Train Epoch: 3 [22080/110534 (20%)]\tAll Loss: 2.1978\tTriple Loss(1): 0.2334\tClassification Loss: 1.7310\n","Train Epoch: 3 [22240/110534 (20%)]\tAll Loss: 1.4843\tTriple Loss(0): 0.0000\tClassification Loss: 1.4843\n","\n","Test set: Average loss: 1.5569, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 3 [22400/110534 (20%)]\tAll Loss: 1.3674\tTriple Loss(0): 0.0000\tClassification Loss: 1.3674\n","Train Epoch: 3 [22560/110534 (20%)]\tAll Loss: 2.0808\tTriple Loss(0): 0.0000\tClassification Loss: 2.0808\n","Train Epoch: 3 [22720/110534 (21%)]\tAll Loss: 2.0906\tTriple Loss(1): 0.0879\tClassification Loss: 1.9148\n","Train Epoch: 3 [22880/110534 (21%)]\tAll Loss: 2.1559\tTriple Loss(1): 0.4805\tClassification Loss: 1.1948\n","Train Epoch: 3 [23040/110534 (21%)]\tAll Loss: 1.3916\tTriple Loss(1): 0.1384\tClassification Loss: 1.1147\n","Train Epoch: 3 [23200/110534 (21%)]\tAll Loss: 2.1495\tTriple Loss(1): 0.2745\tClassification Loss: 1.6005\n","Train Epoch: 3 [23360/110534 (21%)]\tAll Loss: 1.7876\tTriple Loss(1): 0.3959\tClassification Loss: 0.9958\n","Train Epoch: 3 [23520/110534 (21%)]\tAll Loss: 2.6085\tTriple Loss(1): 0.2992\tClassification Loss: 2.0101\n","Train Epoch: 3 [23680/110534 (21%)]\tAll Loss: 1.9123\tTriple Loss(1): 0.0410\tClassification Loss: 1.8304\n","Train Epoch: 3 [23840/110534 (22%)]\tAll Loss: 1.8125\tTriple Loss(1): 0.1389\tClassification Loss: 1.5346\n","\n","Test set: Average loss: 1.5405, Accuracy: 272/480 (57%)\n","\n","Train Epoch: 3 [24000/110534 (22%)]\tAll Loss: 2.0118\tTriple Loss(1): 0.2794\tClassification Loss: 1.4530\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_1500.pth.tar\n","Train Epoch: 3 [24160/110534 (22%)]\tAll Loss: 1.7151\tTriple Loss(1): 0.1424\tClassification Loss: 1.4302\n","Train Epoch: 3 [24320/110534 (22%)]\tAll Loss: 1.6351\tTriple Loss(1): 0.0805\tClassification Loss: 1.4742\n","Train Epoch: 3 [24480/110534 (22%)]\tAll Loss: 2.5295\tTriple Loss(1): 0.4250\tClassification Loss: 1.6796\n","Train Epoch: 3 [24640/110534 (22%)]\tAll Loss: 2.3845\tTriple Loss(1): 0.2298\tClassification Loss: 1.9250\n","Train Epoch: 3 [24800/110534 (22%)]\tAll Loss: 2.9328\tTriple Loss(1): 0.6590\tClassification Loss: 1.6147\n","Train Epoch: 3 [24960/110534 (23%)]\tAll Loss: 2.5867\tTriple Loss(1): 0.3304\tClassification Loss: 1.9259\n","Train Epoch: 3 [25120/110534 (23%)]\tAll Loss: 1.3853\tTriple Loss(1): 0.0540\tClassification Loss: 1.2774\n","Train Epoch: 3 [25280/110534 (23%)]\tAll Loss: 1.6074\tTriple Loss(1): 0.0272\tClassification Loss: 1.5531\n","Train Epoch: 3 [25440/110534 (23%)]\tAll Loss: 1.8038\tTriple Loss(1): 0.1575\tClassification Loss: 1.4888\n","\n","Test set: Average loss: 1.5616, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 3 [25600/110534 (23%)]\tAll Loss: 1.2939\tTriple Loss(1): 0.0000\tClassification Loss: 1.2939\n","Train Epoch: 3 [25760/110534 (23%)]\tAll Loss: 1.9708\tTriple Loss(1): 0.3830\tClassification Loss: 1.2048\n","Train Epoch: 3 [25920/110534 (23%)]\tAll Loss: 1.5024\tTriple Loss(1): 0.2059\tClassification Loss: 1.0907\n","Train Epoch: 3 [26080/110534 (24%)]\tAll Loss: 1.3934\tTriple Loss(0): 0.0000\tClassification Loss: 1.3934\n","Train Epoch: 3 [26240/110534 (24%)]\tAll Loss: 1.7409\tTriple Loss(0): 0.2215\tClassification Loss: 1.2979\n","Train Epoch: 3 [26400/110534 (24%)]\tAll Loss: 1.8925\tTriple Loss(1): 0.1325\tClassification Loss: 1.6275\n","Train Epoch: 3 [26560/110534 (24%)]\tAll Loss: 2.3546\tTriple Loss(1): 0.2741\tClassification Loss: 1.8064\n","Train Epoch: 3 [26720/110534 (24%)]\tAll Loss: 2.1801\tTriple Loss(1): 0.0968\tClassification Loss: 1.9864\n","Train Epoch: 3 [26880/110534 (24%)]\tAll Loss: 1.5440\tTriple Loss(1): 0.0000\tClassification Loss: 1.5440\n","Train Epoch: 3 [27040/110534 (24%)]\tAll Loss: 1.4829\tTriple Loss(1): 0.1611\tClassification Loss: 1.1607\n","\n","Test set: Average loss: 1.5721, Accuracy: 265/480 (55%)\n","\n","Train Epoch: 3 [27200/110534 (25%)]\tAll Loss: 1.7109\tTriple Loss(1): 0.1935\tClassification Loss: 1.3240\n","Train Epoch: 3 [27360/110534 (25%)]\tAll Loss: 2.3478\tTriple Loss(1): 0.4625\tClassification Loss: 1.4228\n","Train Epoch: 3 [27520/110534 (25%)]\tAll Loss: 1.9179\tTriple Loss(1): 0.3255\tClassification Loss: 1.2669\n","Train Epoch: 3 [27680/110534 (25%)]\tAll Loss: 1.7526\tTriple Loss(1): 0.0843\tClassification Loss: 1.5840\n","Train Epoch: 3 [27840/110534 (25%)]\tAll Loss: 1.5177\tTriple Loss(1): 0.1946\tClassification Loss: 1.1284\n","Train Epoch: 3 [28000/110534 (25%)]\tAll Loss: 2.0563\tTriple Loss(1): 0.3401\tClassification Loss: 1.3762\n","Train Epoch: 3 [28160/110534 (25%)]\tAll Loss: 1.2167\tTriple Loss(1): 0.0000\tClassification Loss: 1.2167\n","Train Epoch: 3 [28320/110534 (26%)]\tAll Loss: 1.7971\tTriple Loss(0): 0.0000\tClassification Loss: 1.7971\n","Train Epoch: 3 [28480/110534 (26%)]\tAll Loss: 1.6306\tTriple Loss(0): 0.0000\tClassification Loss: 1.6306\n","Train Epoch: 3 [28640/110534 (26%)]\tAll Loss: 1.4382\tTriple Loss(1): 0.0273\tClassification Loss: 1.3835\n","\n","Test set: Average loss: 1.5419, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 3 [28800/110534 (26%)]\tAll Loss: 1.9792\tTriple Loss(1): 0.2869\tClassification Loss: 1.4055\n","Train Epoch: 3 [28960/110534 (26%)]\tAll Loss: 1.6062\tTriple Loss(1): 0.0915\tClassification Loss: 1.4233\n","Train Epoch: 3 [29120/110534 (26%)]\tAll Loss: 2.0093\tTriple Loss(1): 0.0925\tClassification Loss: 1.8242\n","Train Epoch: 3 [29280/110534 (26%)]\tAll Loss: 2.4808\tTriple Loss(1): 0.2474\tClassification Loss: 1.9859\n","Train Epoch: 3 [29440/110534 (27%)]\tAll Loss: 2.1620\tTriple Loss(1): 0.1229\tClassification Loss: 1.9162\n","Train Epoch: 3 [29600/110534 (27%)]\tAll Loss: 1.6009\tTriple Loss(1): 0.1405\tClassification Loss: 1.3199\n","Train Epoch: 3 [29760/110534 (27%)]\tAll Loss: 2.3627\tTriple Loss(1): 0.3844\tClassification Loss: 1.5939\n","Train Epoch: 3 [29920/110534 (27%)]\tAll Loss: 2.3815\tTriple Loss(1): 0.1851\tClassification Loss: 2.0113\n","Train Epoch: 3 [30080/110534 (27%)]\tAll Loss: 1.8980\tTriple Loss(1): 0.2410\tClassification Loss: 1.4160\n","Train Epoch: 3 [30240/110534 (27%)]\tAll Loss: 1.6143\tTriple Loss(1): 0.1744\tClassification Loss: 1.2655\n","\n","Test set: Average loss: 1.5407, Accuracy: 283/480 (59%)\n","\n","Train Epoch: 3 [30400/110534 (28%)]\tAll Loss: 1.9827\tTriple Loss(1): 0.1559\tClassification Loss: 1.6708\n","Train Epoch: 3 [30560/110534 (28%)]\tAll Loss: 2.3950\tTriple Loss(1): 0.3889\tClassification Loss: 1.6171\n","Train Epoch: 3 [30720/110534 (28%)]\tAll Loss: 1.2424\tTriple Loss(1): 0.0042\tClassification Loss: 1.2341\n","Train Epoch: 3 [30880/110534 (28%)]\tAll Loss: 2.6807\tTriple Loss(1): 0.3954\tClassification Loss: 1.8899\n","Train Epoch: 3 [31040/110534 (28%)]\tAll Loss: 1.8967\tTriple Loss(0): 0.0000\tClassification Loss: 1.8967\n","Train Epoch: 3 [31200/110534 (28%)]\tAll Loss: 1.1157\tTriple Loss(1): 0.0010\tClassification Loss: 1.1137\n","Train Epoch: 3 [31360/110534 (28%)]\tAll Loss: 1.7733\tTriple Loss(1): 0.0228\tClassification Loss: 1.7276\n","Train Epoch: 3 [31520/110534 (29%)]\tAll Loss: 1.3512\tTriple Loss(1): 0.0173\tClassification Loss: 1.3167\n","Train Epoch: 3 [31680/110534 (29%)]\tAll Loss: 1.7021\tTriple Loss(0): 0.0000\tClassification Loss: 1.7021\n","Train Epoch: 3 [31840/110534 (29%)]\tAll Loss: 2.5691\tTriple Loss(1): 0.4494\tClassification Loss: 1.6703\n","\n","Test set: Average loss: 1.5315, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 3 [32000/110534 (29%)]\tAll Loss: 1.6767\tTriple Loss(1): 0.1787\tClassification Loss: 1.3192\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_2000.pth.tar\n","Train Epoch: 3 [32160/110534 (29%)]\tAll Loss: 1.8194\tTriple Loss(1): 0.0000\tClassification Loss: 1.8194\n","Train Epoch: 3 [32320/110534 (29%)]\tAll Loss: 1.9732\tTriple Loss(0): 0.0000\tClassification Loss: 1.9732\n","Train Epoch: 3 [32480/110534 (29%)]\tAll Loss: 1.8741\tTriple Loss(1): 0.2777\tClassification Loss: 1.3188\n","Train Epoch: 3 [32640/110534 (30%)]\tAll Loss: 1.2877\tTriple Loss(1): 0.0258\tClassification Loss: 1.2361\n","Train Epoch: 3 [32800/110534 (30%)]\tAll Loss: 1.9724\tTriple Loss(1): 0.1618\tClassification Loss: 1.6487\n","Train Epoch: 3 [32960/110534 (30%)]\tAll Loss: 2.3010\tTriple Loss(1): 0.5302\tClassification Loss: 1.2405\n","Train Epoch: 3 [33120/110534 (30%)]\tAll Loss: 1.6145\tTriple Loss(1): 0.3324\tClassification Loss: 0.9496\n","Train Epoch: 3 [33280/110534 (30%)]\tAll Loss: 1.7837\tTriple Loss(1): 0.1499\tClassification Loss: 1.4839\n","Train Epoch: 3 [33440/110534 (30%)]\tAll Loss: 4.2338\tTriple Loss(0): 1.3242\tClassification Loss: 1.5854\n","\n","Test set: Average loss: 1.5252, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 3 [33600/110534 (30%)]\tAll Loss: 2.0286\tTriple Loss(1): 0.2899\tClassification Loss: 1.4488\n","Train Epoch: 3 [33760/110534 (31%)]\tAll Loss: 2.0452\tTriple Loss(1): 0.1499\tClassification Loss: 1.7455\n","Train Epoch: 3 [33920/110534 (31%)]\tAll Loss: 1.7776\tTriple Loss(1): 0.1830\tClassification Loss: 1.4116\n","Train Epoch: 3 [34080/110534 (31%)]\tAll Loss: 1.4884\tTriple Loss(1): 0.0484\tClassification Loss: 1.3916\n","Train Epoch: 3 [34240/110534 (31%)]\tAll Loss: 1.3813\tTriple Loss(0): 0.0000\tClassification Loss: 1.3813\n","Train Epoch: 3 [34400/110534 (31%)]\tAll Loss: 1.2732\tTriple Loss(1): 0.0000\tClassification Loss: 1.2732\n","Train Epoch: 3 [34560/110534 (31%)]\tAll Loss: 1.4579\tTriple Loss(0): 0.0000\tClassification Loss: 1.4579\n","Train Epoch: 3 [34720/110534 (31%)]\tAll Loss: 1.9565\tTriple Loss(1): 0.1183\tClassification Loss: 1.7198\n","Train Epoch: 3 [34880/110534 (32%)]\tAll Loss: 1.2553\tTriple Loss(1): 0.0860\tClassification Loss: 1.0834\n","Train Epoch: 3 [35040/110534 (32%)]\tAll Loss: 2.1475\tTriple Loss(1): 0.1443\tClassification Loss: 1.8589\n","\n","Test set: Average loss: 1.5455, Accuracy: 279/480 (58%)\n","\n","Train Epoch: 3 [35200/110534 (32%)]\tAll Loss: 1.9752\tTriple Loss(0): 0.0000\tClassification Loss: 1.9752\n","Train Epoch: 3 [35360/110534 (32%)]\tAll Loss: 1.1823\tTriple Loss(0): 0.0000\tClassification Loss: 1.1823\n","Train Epoch: 3 [35520/110534 (32%)]\tAll Loss: 2.4994\tTriple Loss(1): 0.1916\tClassification Loss: 2.1163\n","Train Epoch: 3 [35680/110534 (32%)]\tAll Loss: 2.6696\tTriple Loss(1): 0.4972\tClassification Loss: 1.6752\n","Train Epoch: 3 [35840/110534 (32%)]\tAll Loss: 1.5792\tTriple Loss(1): 0.1489\tClassification Loss: 1.2815\n","Train Epoch: 3 [36000/110534 (33%)]\tAll Loss: 2.9976\tTriple Loss(1): 0.3583\tClassification Loss: 2.2810\n","Train Epoch: 3 [36160/110534 (33%)]\tAll Loss: 1.2757\tTriple Loss(0): 0.0000\tClassification Loss: 1.2757\n","Train Epoch: 3 [36320/110534 (33%)]\tAll Loss: 2.0995\tTriple Loss(1): 0.0588\tClassification Loss: 1.9818\n","Train Epoch: 3 [36480/110534 (33%)]\tAll Loss: 1.5326\tTriple Loss(0): 0.0000\tClassification Loss: 1.5326\n","Train Epoch: 3 [36640/110534 (33%)]\tAll Loss: 1.9821\tTriple Loss(1): 0.2563\tClassification Loss: 1.4694\n","\n","Test set: Average loss: 1.5497, Accuracy: 271/480 (56%)\n","\n","Train Epoch: 3 [36800/110534 (33%)]\tAll Loss: 1.8980\tTriple Loss(1): 0.3998\tClassification Loss: 1.0984\n","Train Epoch: 3 [36960/110534 (33%)]\tAll Loss: 2.3159\tTriple Loss(1): 0.2526\tClassification Loss: 1.8107\n","Train Epoch: 3 [37120/110534 (34%)]\tAll Loss: 1.8283\tTriple Loss(1): 0.0000\tClassification Loss: 1.8283\n","Train Epoch: 3 [37280/110534 (34%)]\tAll Loss: 2.6663\tTriple Loss(1): 0.3333\tClassification Loss: 1.9997\n","Train Epoch: 3 [37440/110534 (34%)]\tAll Loss: 2.0901\tTriple Loss(0): 0.0000\tClassification Loss: 2.0901\n","Train Epoch: 3 [37600/110534 (34%)]\tAll Loss: 1.3260\tTriple Loss(1): 0.1319\tClassification Loss: 1.0622\n","Train Epoch: 3 [37760/110534 (34%)]\tAll Loss: 3.4743\tTriple Loss(1): 0.6283\tClassification Loss: 2.2176\n","Train Epoch: 3 [37920/110534 (34%)]\tAll Loss: 3.0765\tTriple Loss(1): 0.5929\tClassification Loss: 1.8906\n","Train Epoch: 3 [38080/110534 (34%)]\tAll Loss: 1.5973\tTriple Loss(1): 0.1155\tClassification Loss: 1.3662\n","Train Epoch: 3 [38240/110534 (35%)]\tAll Loss: 1.4586\tTriple Loss(1): 0.1488\tClassification Loss: 1.1609\n","\n","Test set: Average loss: 1.5379, Accuracy: 281/480 (59%)\n","\n","Train Epoch: 3 [38400/110534 (35%)]\tAll Loss: 1.9766\tTriple Loss(1): 0.4726\tClassification Loss: 1.0315\n","Train Epoch: 3 [38560/110534 (35%)]\tAll Loss: 2.0036\tTriple Loss(1): 0.1418\tClassification Loss: 1.7200\n","Train Epoch: 3 [38720/110534 (35%)]\tAll Loss: 2.1873\tTriple Loss(1): 0.1091\tClassification Loss: 1.9691\n","Train Epoch: 3 [38880/110534 (35%)]\tAll Loss: 1.1715\tTriple Loss(1): 0.0396\tClassification Loss: 1.0922\n","Train Epoch: 3 [39040/110534 (35%)]\tAll Loss: 1.6879\tTriple Loss(1): 0.0419\tClassification Loss: 1.6042\n","Train Epoch: 3 [39200/110534 (35%)]\tAll Loss: 2.5625\tTriple Loss(1): 0.4797\tClassification Loss: 1.6032\n","Train Epoch: 3 [39360/110534 (36%)]\tAll Loss: 1.6251\tTriple Loss(1): 0.2561\tClassification Loss: 1.1130\n","Train Epoch: 3 [39520/110534 (36%)]\tAll Loss: 1.9673\tTriple Loss(0): 0.0000\tClassification Loss: 1.9673\n","Train Epoch: 3 [39680/110534 (36%)]\tAll Loss: 1.6796\tTriple Loss(1): 0.0154\tClassification Loss: 1.6488\n","Train Epoch: 3 [39840/110534 (36%)]\tAll Loss: 1.6144\tTriple Loss(1): 0.2811\tClassification Loss: 1.0522\n","\n","Test set: Average loss: 1.5300, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 3 [40000/110534 (36%)]\tAll Loss: 2.0018\tTriple Loss(1): 0.3412\tClassification Loss: 1.3195\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_2500.pth.tar\n","Train Epoch: 3 [40160/110534 (36%)]\tAll Loss: 1.4341\tTriple Loss(1): 0.1826\tClassification Loss: 1.0688\n","Train Epoch: 3 [40320/110534 (36%)]\tAll Loss: 1.2012\tTriple Loss(1): 0.0379\tClassification Loss: 1.1253\n","Train Epoch: 3 [40480/110534 (37%)]\tAll Loss: 1.5722\tTriple Loss(1): 0.0000\tClassification Loss: 1.5722\n","Train Epoch: 3 [40640/110534 (37%)]\tAll Loss: 2.8560\tTriple Loss(1): 0.7571\tClassification Loss: 1.3418\n","Train Epoch: 3 [40800/110534 (37%)]\tAll Loss: 2.2066\tTriple Loss(1): 0.1932\tClassification Loss: 1.8203\n","Train Epoch: 3 [40960/110534 (37%)]\tAll Loss: 2.3173\tTriple Loss(1): 0.2262\tClassification Loss: 1.8648\n","Train Epoch: 3 [41120/110534 (37%)]\tAll Loss: 1.8559\tTriple Loss(1): 0.1126\tClassification Loss: 1.6307\n","Train Epoch: 3 [41280/110534 (37%)]\tAll Loss: 2.4941\tTriple Loss(1): 0.3233\tClassification Loss: 1.8475\n","Train Epoch: 3 [41440/110534 (37%)]\tAll Loss: 1.5888\tTriple Loss(1): 0.0176\tClassification Loss: 1.5537\n","\n","Test set: Average loss: 1.5586, Accuracy: 268/480 (56%)\n","\n","Train Epoch: 3 [41600/110534 (38%)]\tAll Loss: 2.6422\tTriple Loss(1): 0.2246\tClassification Loss: 2.1929\n","Train Epoch: 3 [41760/110534 (38%)]\tAll Loss: 1.4852\tTriple Loss(1): 0.1923\tClassification Loss: 1.1006\n","Train Epoch: 3 [41920/110534 (38%)]\tAll Loss: 1.7432\tTriple Loss(1): 0.1322\tClassification Loss: 1.4788\n","Train Epoch: 3 [42080/110534 (38%)]\tAll Loss: 1.8893\tTriple Loss(1): 0.1911\tClassification Loss: 1.5071\n","Train Epoch: 3 [42240/110534 (38%)]\tAll Loss: 1.6016\tTriple Loss(1): 0.1425\tClassification Loss: 1.3166\n","Train Epoch: 3 [42400/110534 (38%)]\tAll Loss: 1.9282\tTriple Loss(1): 0.1675\tClassification Loss: 1.5931\n","Train Epoch: 3 [42560/110534 (39%)]\tAll Loss: 2.6570\tTriple Loss(1): 0.3985\tClassification Loss: 1.8599\n","Train Epoch: 3 [42720/110534 (39%)]\tAll Loss: 2.5631\tTriple Loss(1): 0.3934\tClassification Loss: 1.7762\n","Train Epoch: 3 [42880/110534 (39%)]\tAll Loss: 1.6262\tTriple Loss(1): 0.3674\tClassification Loss: 0.8914\n","Train Epoch: 3 [43040/110534 (39%)]\tAll Loss: 1.7313\tTriple Loss(1): 0.0496\tClassification Loss: 1.6321\n","\n","Test set: Average loss: 1.5480, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 3 [43200/110534 (39%)]\tAll Loss: 3.1420\tTriple Loss(1): 0.2145\tClassification Loss: 2.7131\n","Train Epoch: 3 [43360/110534 (39%)]\tAll Loss: 1.4507\tTriple Loss(0): 0.0000\tClassification Loss: 1.4507\n","Train Epoch: 3 [43520/110534 (39%)]\tAll Loss: 2.0016\tTriple Loss(1): 0.1605\tClassification Loss: 1.6806\n","Train Epoch: 3 [43680/110534 (40%)]\tAll Loss: 2.6922\tTriple Loss(1): 0.3224\tClassification Loss: 2.0473\n","Train Epoch: 3 [43840/110534 (40%)]\tAll Loss: 2.6116\tTriple Loss(1): 0.4994\tClassification Loss: 1.6127\n","Train Epoch: 3 [44000/110534 (40%)]\tAll Loss: 1.3133\tTriple Loss(1): 0.1656\tClassification Loss: 0.9821\n","Train Epoch: 3 [44160/110534 (40%)]\tAll Loss: 2.1753\tTriple Loss(1): 0.1140\tClassification Loss: 1.9473\n","Train Epoch: 3 [44320/110534 (40%)]\tAll Loss: 1.4995\tTriple Loss(1): 0.0000\tClassification Loss: 1.4995\n","Train Epoch: 3 [44480/110534 (40%)]\tAll Loss: 1.3623\tTriple Loss(0): 0.0000\tClassification Loss: 1.3623\n","Train Epoch: 3 [44640/110534 (40%)]\tAll Loss: 1.6416\tTriple Loss(1): 0.0000\tClassification Loss: 1.6416\n","\n","Test set: Average loss: 1.5497, Accuracy: 267/480 (56%)\n","\n","Train Epoch: 3 [44800/110534 (41%)]\tAll Loss: 1.7515\tTriple Loss(1): 0.0093\tClassification Loss: 1.7329\n","Train Epoch: 3 [44960/110534 (41%)]\tAll Loss: 2.1732\tTriple Loss(1): 0.2434\tClassification Loss: 1.6865\n","Train Epoch: 3 [45120/110534 (41%)]\tAll Loss: 1.0835\tTriple Loss(1): 0.0000\tClassification Loss: 1.0835\n","Train Epoch: 3 [45280/110534 (41%)]\tAll Loss: 2.0266\tTriple Loss(1): 0.2030\tClassification Loss: 1.6206\n","Train Epoch: 3 [45440/110534 (41%)]\tAll Loss: 2.4701\tTriple Loss(1): 0.3588\tClassification Loss: 1.7525\n","Train Epoch: 3 [45600/110534 (41%)]\tAll Loss: 2.2340\tTriple Loss(1): 0.3135\tClassification Loss: 1.6070\n","Train Epoch: 3 [45760/110534 (41%)]\tAll Loss: 2.4590\tTriple Loss(1): 0.2399\tClassification Loss: 1.9792\n","Train Epoch: 3 [45920/110534 (42%)]\tAll Loss: 1.6674\tTriple Loss(1): 0.2275\tClassification Loss: 1.2124\n","Train Epoch: 3 [46080/110534 (42%)]\tAll Loss: 1.6056\tTriple Loss(1): 0.1867\tClassification Loss: 1.2321\n","Train Epoch: 3 [46240/110534 (42%)]\tAll Loss: 1.9226\tTriple Loss(0): 0.0000\tClassification Loss: 1.9226\n","\n","Test set: Average loss: 1.5345, Accuracy: 275/480 (57%)\n","\n","Train Epoch: 3 [46400/110534 (42%)]\tAll Loss: 2.2878\tTriple Loss(1): 0.4499\tClassification Loss: 1.3881\n","Train Epoch: 3 [46560/110534 (42%)]\tAll Loss: 1.9818\tTriple Loss(1): 0.1219\tClassification Loss: 1.7381\n","Train Epoch: 3 [46720/110534 (42%)]\tAll Loss: 1.8683\tTriple Loss(0): 0.0000\tClassification Loss: 1.8683\n","Train Epoch: 3 [46880/110534 (42%)]\tAll Loss: 1.9749\tTriple Loss(1): 0.1502\tClassification Loss: 1.6746\n","Train Epoch: 3 [47040/110534 (43%)]\tAll Loss: 2.0081\tTriple Loss(1): 0.3088\tClassification Loss: 1.3905\n","Train Epoch: 3 [47200/110534 (43%)]\tAll Loss: 2.0162\tTriple Loss(0): 0.0000\tClassification Loss: 2.0162\n","Train Epoch: 3 [47360/110534 (43%)]\tAll Loss: 2.8542\tTriple Loss(1): 0.3676\tClassification Loss: 2.1189\n","Train Epoch: 3 [47520/110534 (43%)]\tAll Loss: 2.4070\tTriple Loss(1): 0.2936\tClassification Loss: 1.8198\n","Train Epoch: 3 [47680/110534 (43%)]\tAll Loss: 1.7817\tTriple Loss(1): 0.2517\tClassification Loss: 1.2783\n","Train Epoch: 3 [47840/110534 (43%)]\tAll Loss: 1.3214\tTriple Loss(0): 0.0000\tClassification Loss: 1.3214\n","\n","Test set: Average loss: 1.5419, Accuracy: 278/480 (58%)\n","\n","Train Epoch: 3 [48000/110534 (43%)]\tAll Loss: 2.0671\tTriple Loss(1): 0.1572\tClassification Loss: 1.7527\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_3000.pth.tar\n","Train Epoch: 3 [48160/110534 (44%)]\tAll Loss: 2.2369\tTriple Loss(1): 0.4772\tClassification Loss: 1.2825\n","Train Epoch: 3 [48320/110534 (44%)]\tAll Loss: 1.9839\tTriple Loss(1): 0.2914\tClassification Loss: 1.4010\n","Train Epoch: 3 [48480/110534 (44%)]\tAll Loss: 1.9319\tTriple Loss(1): 0.2950\tClassification Loss: 1.3419\n","Train Epoch: 3 [48640/110534 (44%)]\tAll Loss: 1.7503\tTriple Loss(0): 0.0000\tClassification Loss: 1.7503\n","Train Epoch: 3 [48800/110534 (44%)]\tAll Loss: 1.6625\tTriple Loss(0): 0.0000\tClassification Loss: 1.6625\n","Train Epoch: 3 [48960/110534 (44%)]\tAll Loss: 2.2333\tTriple Loss(1): 0.3312\tClassification Loss: 1.5709\n","Train Epoch: 3 [49120/110534 (44%)]\tAll Loss: 2.1606\tTriple Loss(1): 0.2173\tClassification Loss: 1.7259\n","Train Epoch: 3 [49280/110534 (45%)]\tAll Loss: 2.8581\tTriple Loss(1): 0.1921\tClassification Loss: 2.4740\n","Train Epoch: 3 [49440/110534 (45%)]\tAll Loss: 2.3797\tTriple Loss(1): 0.2220\tClassification Loss: 1.9358\n","\n","Test set: Average loss: 1.5114, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 3 [49600/110534 (45%)]\tAll Loss: 1.3665\tTriple Loss(0): 0.0000\tClassification Loss: 1.3665\n","Train Epoch: 3 [49760/110534 (45%)]\tAll Loss: 2.8217\tTriple Loss(1): 0.3463\tClassification Loss: 2.1292\n","Train Epoch: 3 [49920/110534 (45%)]\tAll Loss: 0.8483\tTriple Loss(1): 0.0000\tClassification Loss: 0.8483\n","Train Epoch: 3 [50080/110534 (45%)]\tAll Loss: 1.8751\tTriple Loss(0): 0.0000\tClassification Loss: 1.8751\n","Train Epoch: 3 [50240/110534 (45%)]\tAll Loss: 1.8801\tTriple Loss(1): 0.3570\tClassification Loss: 1.1660\n","Train Epoch: 3 [50400/110534 (46%)]\tAll Loss: 1.6187\tTriple Loss(0): 0.0000\tClassification Loss: 1.6187\n","Train Epoch: 3 [50560/110534 (46%)]\tAll Loss: 2.4685\tTriple Loss(1): 0.3307\tClassification Loss: 1.8071\n","Train Epoch: 3 [50720/110534 (46%)]\tAll Loss: 2.2282\tTriple Loss(1): 0.2450\tClassification Loss: 1.7382\n","Train Epoch: 3 [50880/110534 (46%)]\tAll Loss: 1.0749\tTriple Loss(0): 0.0000\tClassification Loss: 1.0749\n","Train Epoch: 3 [51040/110534 (46%)]\tAll Loss: 1.5888\tTriple Loss(0): 0.0000\tClassification Loss: 1.5888\n","\n","Test set: Average loss: 1.5874, Accuracy: 266/480 (55%)\n","\n","Train Epoch: 3 [51200/110534 (46%)]\tAll Loss: 2.3032\tTriple Loss(1): 0.4645\tClassification Loss: 1.3741\n","Train Epoch: 3 [51360/110534 (46%)]\tAll Loss: 1.7590\tTriple Loss(1): 0.0000\tClassification Loss: 1.7590\n","Train Epoch: 3 [51520/110534 (47%)]\tAll Loss: 1.6654\tTriple Loss(1): 0.1319\tClassification Loss: 1.4016\n","Train Epoch: 3 [51680/110534 (47%)]\tAll Loss: 2.2308\tTriple Loss(1): 0.0989\tClassification Loss: 2.0330\n","Train Epoch: 3 [51840/110534 (47%)]\tAll Loss: 2.1353\tTriple Loss(1): 0.1546\tClassification Loss: 1.8261\n","Train Epoch: 3 [52000/110534 (47%)]\tAll Loss: 2.5119\tTriple Loss(1): 0.0977\tClassification Loss: 2.3166\n","Train Epoch: 3 [52160/110534 (47%)]\tAll Loss: 2.4036\tTriple Loss(0): 0.0000\tClassification Loss: 2.4036\n","Train Epoch: 3 [52320/110534 (47%)]\tAll Loss: 1.7072\tTriple Loss(1): 0.2256\tClassification Loss: 1.2560\n","Train Epoch: 3 [52480/110534 (47%)]\tAll Loss: 2.1224\tTriple Loss(1): 0.3036\tClassification Loss: 1.5152\n","Train Epoch: 3 [52640/110534 (48%)]\tAll Loss: 1.7260\tTriple Loss(1): 0.1152\tClassification Loss: 1.4956\n","\n","Test set: Average loss: 1.5866, Accuracy: 267/480 (56%)\n","\n","Train Epoch: 3 [52800/110534 (48%)]\tAll Loss: 2.1530\tTriple Loss(1): 0.1315\tClassification Loss: 1.8900\n","Train Epoch: 3 [52960/110534 (48%)]\tAll Loss: 1.9170\tTriple Loss(1): 0.2045\tClassification Loss: 1.5081\n","Train Epoch: 3 [53120/110534 (48%)]\tAll Loss: 1.5160\tTriple Loss(0): 0.0000\tClassification Loss: 1.5160\n","Train Epoch: 3 [53280/110534 (48%)]\tAll Loss: 1.8700\tTriple Loss(1): 0.4011\tClassification Loss: 1.0678\n","Train Epoch: 3 [53440/110534 (48%)]\tAll Loss: 2.4820\tTriple Loss(1): 0.3742\tClassification Loss: 1.7336\n","Train Epoch: 3 [53600/110534 (48%)]\tAll Loss: 1.7697\tTriple Loss(1): 0.0990\tClassification Loss: 1.5717\n","Train Epoch: 3 [53760/110534 (49%)]\tAll Loss: 1.9870\tTriple Loss(0): 0.0000\tClassification Loss: 1.9870\n","Train Epoch: 3 [53920/110534 (49%)]\tAll Loss: 2.2049\tTriple Loss(1): 0.2619\tClassification Loss: 1.6810\n","Train Epoch: 3 [54080/110534 (49%)]\tAll Loss: 1.2441\tTriple Loss(0): 0.0000\tClassification Loss: 1.2441\n","Train Epoch: 3 [54240/110534 (49%)]\tAll Loss: 1.6553\tTriple Loss(1): 0.1255\tClassification Loss: 1.4043\n","\n","Test set: Average loss: 1.5609, Accuracy: 271/480 (56%)\n","\n","Train Epoch: 3 [54400/110534 (49%)]\tAll Loss: 2.6502\tTriple Loss(1): 0.2685\tClassification Loss: 2.1131\n","Train Epoch: 3 [54560/110534 (49%)]\tAll Loss: 1.9350\tTriple Loss(0): 0.0000\tClassification Loss: 1.9350\n","Train Epoch: 3 [54720/110534 (50%)]\tAll Loss: 1.7824\tTriple Loss(1): 0.2762\tClassification Loss: 1.2300\n","Train Epoch: 3 [54880/110534 (50%)]\tAll Loss: 2.8749\tTriple Loss(1): 0.2522\tClassification Loss: 2.3704\n","Train Epoch: 3 [55040/110534 (50%)]\tAll Loss: 2.7773\tTriple Loss(0): 0.7064\tClassification Loss: 1.3646\n","Train Epoch: 3 [55200/110534 (50%)]\tAll Loss: 1.5587\tTriple Loss(1): 0.1410\tClassification Loss: 1.2767\n","Train Epoch: 3 [55360/110534 (50%)]\tAll Loss: 1.7471\tTriple Loss(1): 0.4282\tClassification Loss: 0.8907\n","Train Epoch: 3 [55520/110534 (50%)]\tAll Loss: 2.0965\tTriple Loss(1): 0.0808\tClassification Loss: 1.9350\n","Train Epoch: 3 [55680/110534 (50%)]\tAll Loss: 1.4594\tTriple Loss(0): 0.0000\tClassification Loss: 1.4594\n","Train Epoch: 3 [55840/110534 (51%)]\tAll Loss: 1.8503\tTriple Loss(1): 0.0803\tClassification Loss: 1.6897\n","\n","Test set: Average loss: 1.5170, Accuracy: 277/480 (58%)\n","\n","Train Epoch: 3 [56000/110534 (51%)]\tAll Loss: 1.4362\tTriple Loss(1): 0.0012\tClassification Loss: 1.4337\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_3500.pth.tar\n","Train Epoch: 3 [56160/110534 (51%)]\tAll Loss: 2.5044\tTriple Loss(1): 0.4214\tClassification Loss: 1.6616\n","Train Epoch: 3 [56320/110534 (51%)]\tAll Loss: 1.3580\tTriple Loss(1): 0.1738\tClassification Loss: 1.0103\n","Train Epoch: 3 [56480/110534 (51%)]\tAll Loss: 1.5101\tTriple Loss(1): 0.1616\tClassification Loss: 1.1870\n","Train Epoch: 3 [56640/110534 (51%)]\tAll Loss: 1.5398\tTriple Loss(1): 0.1866\tClassification Loss: 1.1666\n","Train Epoch: 3 [56800/110534 (51%)]\tAll Loss: 1.5791\tTriple Loss(1): 0.0932\tClassification Loss: 1.3927\n","Train Epoch: 3 [56960/110534 (52%)]\tAll Loss: 2.0381\tTriple Loss(1): 0.0090\tClassification Loss: 2.0201\n","Train Epoch: 3 [57120/110534 (52%)]\tAll Loss: 1.6038\tTriple Loss(1): 0.1309\tClassification Loss: 1.3421\n","Train Epoch: 3 [57280/110534 (52%)]\tAll Loss: 2.4499\tTriple Loss(1): 0.3230\tClassification Loss: 1.8039\n","Train Epoch: 3 [57440/110534 (52%)]\tAll Loss: 1.6237\tTriple Loss(1): 0.1282\tClassification Loss: 1.3672\n","\n","Test set: Average loss: 1.5668, Accuracy: 263/480 (55%)\n","\n","Train Epoch: 3 [57600/110534 (52%)]\tAll Loss: 2.4776\tTriple Loss(1): 0.4181\tClassification Loss: 1.6415\n","Train Epoch: 3 [57760/110534 (52%)]\tAll Loss: 1.3590\tTriple Loss(1): 0.0953\tClassification Loss: 1.1684\n","Train Epoch: 3 [57920/110534 (52%)]\tAll Loss: 2.3130\tTriple Loss(1): 0.4060\tClassification Loss: 1.5010\n","Train Epoch: 3 [58080/110534 (53%)]\tAll Loss: 2.9523\tTriple Loss(1): 0.6125\tClassification Loss: 1.7273\n","Train Epoch: 3 [58240/110534 (53%)]\tAll Loss: 2.7267\tTriple Loss(1): 0.4421\tClassification Loss: 1.8426\n","Train Epoch: 3 [58400/110534 (53%)]\tAll Loss: 2.1325\tTriple Loss(1): 0.4207\tClassification Loss: 1.2910\n","Train Epoch: 3 [58560/110534 (53%)]\tAll Loss: 2.5119\tTriple Loss(1): 0.1182\tClassification Loss: 2.2755\n","Train Epoch: 3 [58720/110534 (53%)]\tAll Loss: 1.8632\tTriple Loss(1): 0.1700\tClassification Loss: 1.5231\n","Train Epoch: 3 [58880/110534 (53%)]\tAll Loss: 1.5719\tTriple Loss(1): 0.0167\tClassification Loss: 1.5386\n","Train Epoch: 3 [59040/110534 (53%)]\tAll Loss: 1.9196\tTriple Loss(1): 0.2597\tClassification Loss: 1.4001\n","\n","Test set: Average loss: 1.5189, Accuracy: 275/480 (57%)\n","\n","Train Epoch: 3 [59200/110534 (54%)]\tAll Loss: 1.6227\tTriple Loss(0): 0.0000\tClassification Loss: 1.6227\n","Train Epoch: 3 [59360/110534 (54%)]\tAll Loss: 1.7233\tTriple Loss(1): 0.0914\tClassification Loss: 1.5404\n","Train Epoch: 3 [59520/110534 (54%)]\tAll Loss: 1.6570\tTriple Loss(0): 0.0000\tClassification Loss: 1.6570\n","Train Epoch: 3 [59680/110534 (54%)]\tAll Loss: 1.9705\tTriple Loss(1): 0.1344\tClassification Loss: 1.7017\n","Train Epoch: 3 [59840/110534 (54%)]\tAll Loss: 3.0663\tTriple Loss(1): 0.4437\tClassification Loss: 2.1789\n","Train Epoch: 3 [60000/110534 (54%)]\tAll Loss: 2.4468\tTriple Loss(1): 0.1006\tClassification Loss: 2.2457\n","Train Epoch: 3 [60160/110534 (54%)]\tAll Loss: 2.1168\tTriple Loss(1): 0.1343\tClassification Loss: 1.8483\n","Train Epoch: 3 [60320/110534 (55%)]\tAll Loss: 1.6430\tTriple Loss(1): 0.0496\tClassification Loss: 1.5438\n","Train Epoch: 3 [60480/110534 (55%)]\tAll Loss: 1.4561\tTriple Loss(1): 0.1716\tClassification Loss: 1.1129\n","Train Epoch: 3 [60640/110534 (55%)]\tAll Loss: 1.3470\tTriple Loss(0): 0.0000\tClassification Loss: 1.3470\n","\n","Test set: Average loss: 1.5122, Accuracy: 282/480 (59%)\n","\n","Train Epoch: 3 [60800/110534 (55%)]\tAll Loss: 0.8886\tTriple Loss(1): 0.0000\tClassification Loss: 0.8886\n","Train Epoch: 3 [60960/110534 (55%)]\tAll Loss: 1.4750\tTriple Loss(1): 0.0812\tClassification Loss: 1.3126\n","Train Epoch: 3 [61120/110534 (55%)]\tAll Loss: 1.5618\tTriple Loss(1): 0.1052\tClassification Loss: 1.3513\n","Train Epoch: 3 [61280/110534 (55%)]\tAll Loss: 1.6153\tTriple Loss(0): 0.0000\tClassification Loss: 1.6153\n","Train Epoch: 3 [61440/110534 (56%)]\tAll Loss: 1.7098\tTriple Loss(1): 0.0034\tClassification Loss: 1.7030\n","Train Epoch: 3 [61600/110534 (56%)]\tAll Loss: 2.5717\tTriple Loss(1): 0.1601\tClassification Loss: 2.2514\n","Train Epoch: 3 [61760/110534 (56%)]\tAll Loss: 1.4334\tTriple Loss(1): 0.0117\tClassification Loss: 1.4099\n","Train Epoch: 3 [61920/110534 (56%)]\tAll Loss: 12.7018\tTriple Loss(0): 5.4895\tClassification Loss: 1.7228\n","Train Epoch: 3 [62080/110534 (56%)]\tAll Loss: 2.1011\tTriple Loss(1): 0.1073\tClassification Loss: 1.8865\n","Train Epoch: 3 [62240/110534 (56%)]\tAll Loss: 1.3591\tTriple Loss(0): 0.0000\tClassification Loss: 1.3591\n","\n","Test set: Average loss: 1.5279, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 3 [62400/110534 (56%)]\tAll Loss: 1.7925\tTriple Loss(1): 0.1024\tClassification Loss: 1.5877\n","Train Epoch: 3 [62560/110534 (57%)]\tAll Loss: 1.7403\tTriple Loss(0): 0.0000\tClassification Loss: 1.7403\n","Train Epoch: 3 [62720/110534 (57%)]\tAll Loss: 1.5512\tTriple Loss(1): 0.1735\tClassification Loss: 1.2043\n","Train Epoch: 3 [62880/110534 (57%)]\tAll Loss: 1.4759\tTriple Loss(1): 0.0959\tClassification Loss: 1.2840\n","Train Epoch: 3 [63040/110534 (57%)]\tAll Loss: 1.8192\tTriple Loss(1): 0.0207\tClassification Loss: 1.7777\n","Train Epoch: 3 [63200/110534 (57%)]\tAll Loss: 1.9722\tTriple Loss(1): 0.2883\tClassification Loss: 1.3956\n","Train Epoch: 3 [63360/110534 (57%)]\tAll Loss: 2.6191\tTriple Loss(1): 0.2014\tClassification Loss: 2.2163\n","Train Epoch: 3 [63520/110534 (57%)]\tAll Loss: 1.9049\tTriple Loss(1): 0.2572\tClassification Loss: 1.3904\n","Train Epoch: 3 [63680/110534 (58%)]\tAll Loss: 1.7711\tTriple Loss(1): 0.1955\tClassification Loss: 1.3801\n","Train Epoch: 3 [63840/110534 (58%)]\tAll Loss: 1.4977\tTriple Loss(1): 0.0601\tClassification Loss: 1.3774\n","\n","Test set: Average loss: 1.5786, Accuracy: 266/480 (55%)\n","\n","Train Epoch: 3 [64000/110534 (58%)]\tAll Loss: 2.2193\tTriple Loss(1): 0.2063\tClassification Loss: 1.8066\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_4000.pth.tar\n","Train Epoch: 3 [64160/110534 (58%)]\tAll Loss: 1.7441\tTriple Loss(1): 0.0957\tClassification Loss: 1.5528\n","Train Epoch: 3 [64320/110534 (58%)]\tAll Loss: 1.1561\tTriple Loss(1): 0.0554\tClassification Loss: 1.0453\n","Train Epoch: 3 [64480/110534 (58%)]\tAll Loss: 2.0557\tTriple Loss(1): 0.2467\tClassification Loss: 1.5623\n","Train Epoch: 3 [64640/110534 (58%)]\tAll Loss: 2.4335\tTriple Loss(1): 0.4230\tClassification Loss: 1.5874\n","Train Epoch: 3 [64800/110534 (59%)]\tAll Loss: 1.2550\tTriple Loss(0): 0.0000\tClassification Loss: 1.2550\n","Train Epoch: 3 [64960/110534 (59%)]\tAll Loss: 1.3602\tTriple Loss(0): 0.0000\tClassification Loss: 1.3602\n","Train Epoch: 3 [65120/110534 (59%)]\tAll Loss: 1.6278\tTriple Loss(1): 0.0000\tClassification Loss: 1.6278\n","Train Epoch: 3 [65280/110534 (59%)]\tAll Loss: 2.3367\tTriple Loss(1): 0.2189\tClassification Loss: 1.8989\n","Train Epoch: 3 [65440/110534 (59%)]\tAll Loss: 1.2893\tTriple Loss(0): 0.0000\tClassification Loss: 1.2893\n","\n","Test set: Average loss: 1.5580, Accuracy: 266/480 (55%)\n","\n","Train Epoch: 3 [65600/110534 (59%)]\tAll Loss: 1.6764\tTriple Loss(0): 0.0000\tClassification Loss: 1.6764\n","Train Epoch: 3 [65760/110534 (59%)]\tAll Loss: 2.1152\tTriple Loss(1): 0.1947\tClassification Loss: 1.7259\n","Train Epoch: 3 [65920/110534 (60%)]\tAll Loss: 2.0810\tTriple Loss(1): 0.2027\tClassification Loss: 1.6756\n","Train Epoch: 3 [66080/110534 (60%)]\tAll Loss: 2.0485\tTriple Loss(1): 0.3600\tClassification Loss: 1.3286\n","Train Epoch: 3 [66240/110534 (60%)]\tAll Loss: 1.4608\tTriple Loss(1): 0.0529\tClassification Loss: 1.3549\n","Train Epoch: 3 [66400/110534 (60%)]\tAll Loss: 2.2203\tTriple Loss(1): 0.4379\tClassification Loss: 1.3446\n","Train Epoch: 3 [66560/110534 (60%)]\tAll Loss: 1.6457\tTriple Loss(1): 0.2900\tClassification Loss: 1.0657\n","Train Epoch: 3 [66720/110534 (60%)]\tAll Loss: 1.7457\tTriple Loss(1): 0.0520\tClassification Loss: 1.6417\n","Train Epoch: 3 [66880/110534 (61%)]\tAll Loss: 2.2998\tTriple Loss(0): 0.0000\tClassification Loss: 2.2998\n","Train Epoch: 3 [67040/110534 (61%)]\tAll Loss: 2.8368\tTriple Loss(1): 0.3439\tClassification Loss: 2.1490\n","\n","Test set: Average loss: 1.5470, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 3 [67200/110534 (61%)]\tAll Loss: 2.6706\tTriple Loss(1): 0.3809\tClassification Loss: 1.9089\n","Train Epoch: 3 [67360/110534 (61%)]\tAll Loss: 0.9414\tTriple Loss(0): 0.0000\tClassification Loss: 0.9414\n","Train Epoch: 3 [67520/110534 (61%)]\tAll Loss: 2.1313\tTriple Loss(1): 0.1411\tClassification Loss: 1.8490\n","Train Epoch: 3 [67680/110534 (61%)]\tAll Loss: 1.0986\tTriple Loss(0): 0.0000\tClassification Loss: 1.0986\n","Train Epoch: 3 [67840/110534 (61%)]\tAll Loss: 1.9158\tTriple Loss(1): 0.0970\tClassification Loss: 1.7218\n","Train Epoch: 3 [68000/110534 (62%)]\tAll Loss: 1.8925\tTriple Loss(1): 0.0335\tClassification Loss: 1.8255\n","Train Epoch: 3 [68160/110534 (62%)]\tAll Loss: 1.8004\tTriple Loss(0): 0.0000\tClassification Loss: 1.8004\n","Train Epoch: 3 [68320/110534 (62%)]\tAll Loss: 1.7848\tTriple Loss(1): 0.3528\tClassification Loss: 1.0792\n","Train Epoch: 3 [68480/110534 (62%)]\tAll Loss: 1.4260\tTriple Loss(0): 0.0000\tClassification Loss: 1.4260\n","Train Epoch: 3 [68640/110534 (62%)]\tAll Loss: 2.0832\tTriple Loss(1): 0.1978\tClassification Loss: 1.6875\n","\n","Test set: Average loss: 1.5544, Accuracy: 275/480 (57%)\n","\n","Train Epoch: 3 [68800/110534 (62%)]\tAll Loss: 2.1198\tTriple Loss(1): 0.0555\tClassification Loss: 2.0088\n","Train Epoch: 3 [68960/110534 (62%)]\tAll Loss: 1.5399\tTriple Loss(0): 0.0000\tClassification Loss: 1.5399\n","Train Epoch: 3 [69120/110534 (63%)]\tAll Loss: 1.3456\tTriple Loss(0): 0.0000\tClassification Loss: 1.3456\n","Train Epoch: 3 [69280/110534 (63%)]\tAll Loss: 1.4971\tTriple Loss(0): 0.0000\tClassification Loss: 1.4971\n","Train Epoch: 3 [69440/110534 (63%)]\tAll Loss: 2.7850\tTriple Loss(1): 0.4386\tClassification Loss: 1.9079\n","Train Epoch: 3 [69600/110534 (63%)]\tAll Loss: 1.0554\tTriple Loss(1): 0.0000\tClassification Loss: 1.0554\n","Train Epoch: 3 [69760/110534 (63%)]\tAll Loss: 1.8819\tTriple Loss(1): 0.3264\tClassification Loss: 1.2291\n","Train Epoch: 3 [69920/110534 (63%)]\tAll Loss: 2.1841\tTriple Loss(1): 0.0951\tClassification Loss: 1.9940\n","Train Epoch: 3 [70080/110534 (63%)]\tAll Loss: 1.7280\tTriple Loss(1): 0.0000\tClassification Loss: 1.7280\n","Train Epoch: 3 [70240/110534 (64%)]\tAll Loss: 2.0787\tTriple Loss(1): 0.0446\tClassification Loss: 1.9896\n","\n","Test set: Average loss: 1.5406, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 3 [70400/110534 (64%)]\tAll Loss: 1.6503\tTriple Loss(0): 0.0000\tClassification Loss: 1.6503\n","Train Epoch: 3 [70560/110534 (64%)]\tAll Loss: 2.7086\tTriple Loss(1): 0.3074\tClassification Loss: 2.0938\n","Train Epoch: 3 [70720/110534 (64%)]\tAll Loss: 1.2303\tTriple Loss(1): 0.0171\tClassification Loss: 1.1961\n","Train Epoch: 3 [70880/110534 (64%)]\tAll Loss: 1.5073\tTriple Loss(0): 0.0000\tClassification Loss: 1.5073\n","Train Epoch: 3 [71040/110534 (64%)]\tAll Loss: 1.1958\tTriple Loss(0): 0.0000\tClassification Loss: 1.1958\n","Train Epoch: 3 [71200/110534 (64%)]\tAll Loss: 2.3445\tTriple Loss(1): 0.3457\tClassification Loss: 1.6532\n","Train Epoch: 3 [71360/110534 (65%)]\tAll Loss: 1.2189\tTriple Loss(0): 0.0000\tClassification Loss: 1.2189\n","Train Epoch: 3 [71520/110534 (65%)]\tAll Loss: 1.5355\tTriple Loss(0): 0.0000\tClassification Loss: 1.5355\n","Train Epoch: 3 [71680/110534 (65%)]\tAll Loss: 1.2967\tTriple Loss(1): 0.1800\tClassification Loss: 0.9367\n","Train Epoch: 3 [71840/110534 (65%)]\tAll Loss: 2.6934\tTriple Loss(1): 0.1908\tClassification Loss: 2.3118\n","\n","Test set: Average loss: 1.5575, Accuracy: 275/480 (57%)\n","\n","Train Epoch: 3 [72000/110534 (65%)]\tAll Loss: 1.9320\tTriple Loss(1): 0.1627\tClassification Loss: 1.6065\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_4500.pth.tar\n","Train Epoch: 3 [72160/110534 (65%)]\tAll Loss: 2.3383\tTriple Loss(1): 0.3353\tClassification Loss: 1.6677\n","Train Epoch: 3 [72320/110534 (65%)]\tAll Loss: 1.8677\tTriple Loss(1): 0.2032\tClassification Loss: 1.4613\n","Train Epoch: 3 [72480/110534 (66%)]\tAll Loss: 2.2452\tTriple Loss(1): 0.2022\tClassification Loss: 1.8408\n","Train Epoch: 3 [72640/110534 (66%)]\tAll Loss: 2.4562\tTriple Loss(1): 0.3715\tClassification Loss: 1.7133\n","Train Epoch: 3 [72800/110534 (66%)]\tAll Loss: 1.5892\tTriple Loss(1): 0.0413\tClassification Loss: 1.5065\n","Train Epoch: 3 [72960/110534 (66%)]\tAll Loss: 1.2054\tTriple Loss(0): 0.0000\tClassification Loss: 1.2054\n","Train Epoch: 3 [73120/110534 (66%)]\tAll Loss: 2.2965\tTriple Loss(1): 0.3065\tClassification Loss: 1.6836\n","Train Epoch: 3 [73280/110534 (66%)]\tAll Loss: 1.5993\tTriple Loss(1): 0.0000\tClassification Loss: 1.5993\n","Train Epoch: 3 [73440/110534 (66%)]\tAll Loss: 2.0286\tTriple Loss(1): 0.1303\tClassification Loss: 1.7679\n","\n","Test set: Average loss: 1.5567, Accuracy: 282/480 (59%)\n","\n","Train Epoch: 3 [73600/110534 (67%)]\tAll Loss: 1.4385\tTriple Loss(1): 0.0196\tClassification Loss: 1.3993\n","Train Epoch: 3 [73760/110534 (67%)]\tAll Loss: 1.9475\tTriple Loss(1): 0.1897\tClassification Loss: 1.5682\n","Train Epoch: 3 [73920/110534 (67%)]\tAll Loss: 2.1285\tTriple Loss(1): 0.3100\tClassification Loss: 1.5086\n","Train Epoch: 3 [74080/110534 (67%)]\tAll Loss: 1.6779\tTriple Loss(1): 0.3013\tClassification Loss: 1.0753\n","Train Epoch: 3 [74240/110534 (67%)]\tAll Loss: 1.6862\tTriple Loss(1): 0.0405\tClassification Loss: 1.6052\n","Train Epoch: 3 [74400/110534 (67%)]\tAll Loss: 1.5297\tTriple Loss(1): 0.0472\tClassification Loss: 1.4353\n","Train Epoch: 3 [74560/110534 (67%)]\tAll Loss: 1.9523\tTriple Loss(1): 0.1937\tClassification Loss: 1.5649\n","Train Epoch: 3 [74720/110534 (68%)]\tAll Loss: 2.4035\tTriple Loss(1): 0.2343\tClassification Loss: 1.9348\n","Train Epoch: 3 [74880/110534 (68%)]\tAll Loss: 2.0694\tTriple Loss(1): 0.2868\tClassification Loss: 1.4958\n","Train Epoch: 3 [75040/110534 (68%)]\tAll Loss: 1.4769\tTriple Loss(0): 0.2384\tClassification Loss: 1.0000\n","\n","Test set: Average loss: 1.5368, Accuracy: 275/480 (57%)\n","\n","Train Epoch: 3 [75200/110534 (68%)]\tAll Loss: 1.2164\tTriple Loss(0): 0.0000\tClassification Loss: 1.2164\n","Train Epoch: 3 [75360/110534 (68%)]\tAll Loss: 3.0260\tTriple Loss(1): 0.3665\tClassification Loss: 2.2929\n","Train Epoch: 3 [75520/110534 (68%)]\tAll Loss: 1.6793\tTriple Loss(1): 0.0911\tClassification Loss: 1.4970\n","Train Epoch: 3 [75680/110534 (68%)]\tAll Loss: 1.9635\tTriple Loss(1): 0.0349\tClassification Loss: 1.8938\n","Train Epoch: 3 [75840/110534 (69%)]\tAll Loss: 2.5491\tTriple Loss(1): 0.2424\tClassification Loss: 2.0642\n","Train Epoch: 3 [76000/110534 (69%)]\tAll Loss: 2.3769\tTriple Loss(1): 0.4043\tClassification Loss: 1.5683\n","Train Epoch: 3 [76160/110534 (69%)]\tAll Loss: 2.0777\tTriple Loss(1): 0.1105\tClassification Loss: 1.8568\n","Train Epoch: 3 [76320/110534 (69%)]\tAll Loss: 2.2036\tTriple Loss(1): 0.2766\tClassification Loss: 1.6503\n","Train Epoch: 3 [76480/110534 (69%)]\tAll Loss: 2.1292\tTriple Loss(1): 0.2976\tClassification Loss: 1.5339\n","Train Epoch: 3 [76640/110534 (69%)]\tAll Loss: 1.7618\tTriple Loss(1): 0.2289\tClassification Loss: 1.3040\n","\n","Test set: Average loss: 1.5288, Accuracy: 279/480 (58%)\n","\n","Train Epoch: 3 [76800/110534 (69%)]\tAll Loss: 1.4981\tTriple Loss(1): 0.0024\tClassification Loss: 1.4933\n","Train Epoch: 3 [76960/110534 (70%)]\tAll Loss: 1.8466\tTriple Loss(1): 0.0481\tClassification Loss: 1.7503\n","Train Epoch: 3 [77120/110534 (70%)]\tAll Loss: 1.6949\tTriple Loss(1): 0.1424\tClassification Loss: 1.4100\n","Train Epoch: 3 [77280/110534 (70%)]\tAll Loss: 1.5212\tTriple Loss(1): 0.0377\tClassification Loss: 1.4459\n","Train Epoch: 3 [77440/110534 (70%)]\tAll Loss: 1.5867\tTriple Loss(1): 0.0454\tClassification Loss: 1.4958\n","Train Epoch: 3 [77600/110534 (70%)]\tAll Loss: 1.8449\tTriple Loss(0): 0.0000\tClassification Loss: 1.8449\n","Train Epoch: 3 [77760/110534 (70%)]\tAll Loss: 2.2397\tTriple Loss(1): 0.1301\tClassification Loss: 1.9796\n","Train Epoch: 3 [77920/110534 (70%)]\tAll Loss: 1.7296\tTriple Loss(1): 0.1188\tClassification Loss: 1.4919\n","Train Epoch: 3 [78080/110534 (71%)]\tAll Loss: 1.5488\tTriple Loss(1): 0.0490\tClassification Loss: 1.4508\n","Train Epoch: 3 [78240/110534 (71%)]\tAll Loss: 2.4517\tTriple Loss(1): 0.3388\tClassification Loss: 1.7742\n","\n","Test set: Average loss: 1.5500, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 3 [78400/110534 (71%)]\tAll Loss: 1.4236\tTriple Loss(0): 0.0000\tClassification Loss: 1.4236\n","Train Epoch: 3 [78560/110534 (71%)]\tAll Loss: 1.9981\tTriple Loss(1): 0.0481\tClassification Loss: 1.9019\n","Train Epoch: 3 [78720/110534 (71%)]\tAll Loss: 1.3652\tTriple Loss(0): 0.0000\tClassification Loss: 1.3652\n","Train Epoch: 3 [78880/110534 (71%)]\tAll Loss: 2.4545\tTriple Loss(1): 0.3416\tClassification Loss: 1.7712\n","Train Epoch: 3 [79040/110534 (72%)]\tAll Loss: 1.6266\tTriple Loss(1): 0.1010\tClassification Loss: 1.4246\n","Train Epoch: 3 [79200/110534 (72%)]\tAll Loss: 1.9032\tTriple Loss(1): 0.1099\tClassification Loss: 1.6833\n","Train Epoch: 3 [79360/110534 (72%)]\tAll Loss: 1.2152\tTriple Loss(0): 0.0000\tClassification Loss: 1.2152\n","Train Epoch: 3 [79520/110534 (72%)]\tAll Loss: 2.2559\tTriple Loss(1): 0.2548\tClassification Loss: 1.7464\n","Train Epoch: 3 [79680/110534 (72%)]\tAll Loss: 1.6376\tTriple Loss(0): 0.0000\tClassification Loss: 1.6376\n","Train Epoch: 3 [79840/110534 (72%)]\tAll Loss: 1.4066\tTriple Loss(1): 0.0201\tClassification Loss: 1.3665\n","\n","Test set: Average loss: 1.5350, Accuracy: 272/480 (57%)\n","\n","Train Epoch: 3 [80000/110534 (72%)]\tAll Loss: 1.7379\tTriple Loss(0): 0.0000\tClassification Loss: 1.7379\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_5000.pth.tar\n","Train Epoch: 3 [80160/110534 (73%)]\tAll Loss: 2.9137\tTriple Loss(1): 0.3483\tClassification Loss: 2.2172\n","Train Epoch: 3 [80320/110534 (73%)]\tAll Loss: 4.6231\tTriple Loss(0): 1.2314\tClassification Loss: 2.1602\n","Train Epoch: 3 [80480/110534 (73%)]\tAll Loss: 3.2015\tTriple Loss(0): 0.8807\tClassification Loss: 1.4401\n","Train Epoch: 3 [80640/110534 (73%)]\tAll Loss: 1.4933\tTriple Loss(1): 0.0385\tClassification Loss: 1.4163\n","Train Epoch: 3 [80800/110534 (73%)]\tAll Loss: 1.6153\tTriple Loss(0): 0.0000\tClassification Loss: 1.6153\n","Train Epoch: 3 [80960/110534 (73%)]\tAll Loss: 2.3751\tTriple Loss(1): 0.5288\tClassification Loss: 1.3174\n","Train Epoch: 3 [81120/110534 (73%)]\tAll Loss: 2.0350\tTriple Loss(1): 0.2771\tClassification Loss: 1.4809\n","Train Epoch: 3 [81280/110534 (74%)]\tAll Loss: 1.6902\tTriple Loss(1): 0.1782\tClassification Loss: 1.3337\n","Train Epoch: 3 [81440/110534 (74%)]\tAll Loss: 1.3467\tTriple Loss(1): 0.1395\tClassification Loss: 1.0677\n","\n","Test set: Average loss: 1.5586, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 3 [81600/110534 (74%)]\tAll Loss: 1.8337\tTriple Loss(1): 0.1632\tClassification Loss: 1.5074\n","Train Epoch: 3 [81760/110534 (74%)]\tAll Loss: 1.2403\tTriple Loss(1): 0.1167\tClassification Loss: 1.0069\n","Train Epoch: 3 [81920/110534 (74%)]\tAll Loss: 2.1483\tTriple Loss(1): 0.0167\tClassification Loss: 2.1149\n","Train Epoch: 3 [82080/110534 (74%)]\tAll Loss: 2.0045\tTriple Loss(1): 0.1473\tClassification Loss: 1.7099\n","Train Epoch: 3 [82240/110534 (74%)]\tAll Loss: 1.5396\tTriple Loss(1): 0.1298\tClassification Loss: 1.2799\n","Train Epoch: 3 [82400/110534 (75%)]\tAll Loss: 2.4135\tTriple Loss(1): 0.3643\tClassification Loss: 1.6849\n","Train Epoch: 3 [82560/110534 (75%)]\tAll Loss: 2.5515\tTriple Loss(1): 0.2566\tClassification Loss: 2.0383\n","Train Epoch: 3 [82720/110534 (75%)]\tAll Loss: 1.0664\tTriple Loss(1): 0.1046\tClassification Loss: 0.8572\n","Train Epoch: 3 [82880/110534 (75%)]\tAll Loss: 2.6780\tTriple Loss(1): 0.6517\tClassification Loss: 1.3747\n","Train Epoch: 3 [83040/110534 (75%)]\tAll Loss: 1.3460\tTriple Loss(0): 0.0000\tClassification Loss: 1.3460\n","\n","Test set: Average loss: 1.5583, Accuracy: 267/480 (56%)\n","\n","Train Epoch: 3 [83200/110534 (75%)]\tAll Loss: 1.4627\tTriple Loss(0): 0.0000\tClassification Loss: 1.4627\n","Train Epoch: 3 [83360/110534 (75%)]\tAll Loss: 2.7874\tTriple Loss(1): 0.4371\tClassification Loss: 1.9132\n","Train Epoch: 3 [83520/110534 (76%)]\tAll Loss: 1.4968\tTriple Loss(1): 0.0000\tClassification Loss: 1.4968\n","Train Epoch: 3 [83680/110534 (76%)]\tAll Loss: 2.5812\tTriple Loss(1): 0.1861\tClassification Loss: 2.2090\n","Train Epoch: 3 [83840/110534 (76%)]\tAll Loss: 1.7076\tTriple Loss(1): 0.0000\tClassification Loss: 1.7076\n","Train Epoch: 3 [84000/110534 (76%)]\tAll Loss: 1.4445\tTriple Loss(0): 0.0000\tClassification Loss: 1.4445\n","Train Epoch: 3 [84160/110534 (76%)]\tAll Loss: 2.4225\tTriple Loss(1): 0.2540\tClassification Loss: 1.9145\n","Train Epoch: 3 [84320/110534 (76%)]\tAll Loss: 1.4407\tTriple Loss(1): 0.1998\tClassification Loss: 1.0412\n","Train Epoch: 3 [84480/110534 (76%)]\tAll Loss: 2.3743\tTriple Loss(1): 0.2738\tClassification Loss: 1.8267\n","Train Epoch: 3 [84640/110534 (77%)]\tAll Loss: 1.9533\tTriple Loss(1): 0.1707\tClassification Loss: 1.6120\n","\n","Test set: Average loss: 1.5188, Accuracy: 284/480 (59%)\n","\n","Train Epoch: 3 [84800/110534 (77%)]\tAll Loss: 2.0339\tTriple Loss(1): 0.1408\tClassification Loss: 1.7524\n","Train Epoch: 3 [84960/110534 (77%)]\tAll Loss: 2.0632\tTriple Loss(1): 0.0652\tClassification Loss: 1.9329\n","Train Epoch: 3 [85120/110534 (77%)]\tAll Loss: 2.2618\tTriple Loss(1): 0.1151\tClassification Loss: 2.0316\n","Train Epoch: 3 [85280/110534 (77%)]\tAll Loss: 1.6739\tTriple Loss(1): 0.0678\tClassification Loss: 1.5384\n","Train Epoch: 3 [85440/110534 (77%)]\tAll Loss: 2.3308\tTriple Loss(1): 0.1984\tClassification Loss: 1.9340\n","Train Epoch: 3 [85600/110534 (77%)]\tAll Loss: 2.2942\tTriple Loss(1): 0.2898\tClassification Loss: 1.7145\n","Train Epoch: 3 [85760/110534 (78%)]\tAll Loss: 2.2989\tTriple Loss(1): 0.3739\tClassification Loss: 1.5511\n","Train Epoch: 3 [85920/110534 (78%)]\tAll Loss: 2.0815\tTriple Loss(1): 0.3372\tClassification Loss: 1.4072\n","Train Epoch: 3 [86080/110534 (78%)]\tAll Loss: 1.7403\tTriple Loss(1): 0.0849\tClassification Loss: 1.5704\n","Train Epoch: 3 [86240/110534 (78%)]\tAll Loss: 1.7213\tTriple Loss(1): 0.0824\tClassification Loss: 1.5565\n","\n","Test set: Average loss: 1.5565, Accuracy: 271/480 (56%)\n","\n","Train Epoch: 3 [86400/110534 (78%)]\tAll Loss: 2.1476\tTriple Loss(1): 0.2557\tClassification Loss: 1.6362\n","Train Epoch: 3 [86560/110534 (78%)]\tAll Loss: 3.0756\tTriple Loss(1): 0.3987\tClassification Loss: 2.2782\n","Train Epoch: 3 [86720/110534 (78%)]\tAll Loss: 2.4931\tTriple Loss(1): 0.3144\tClassification Loss: 1.8643\n","Train Epoch: 3 [86880/110534 (79%)]\tAll Loss: 1.5203\tTriple Loss(1): 0.1106\tClassification Loss: 1.2991\n","Train Epoch: 3 [87040/110534 (79%)]\tAll Loss: 1.4728\tTriple Loss(1): 0.0264\tClassification Loss: 1.4200\n","Train Epoch: 3 [87200/110534 (79%)]\tAll Loss: 2.4938\tTriple Loss(1): 0.2394\tClassification Loss: 2.0151\n","Train Epoch: 3 [87360/110534 (79%)]\tAll Loss: 2.3326\tTriple Loss(1): 0.0000\tClassification Loss: 2.3326\n","Train Epoch: 3 [87520/110534 (79%)]\tAll Loss: 1.5626\tTriple Loss(1): 0.2389\tClassification Loss: 1.0847\n","Train Epoch: 3 [87680/110534 (79%)]\tAll Loss: 2.1124\tTriple Loss(1): 0.0979\tClassification Loss: 1.9165\n","Train Epoch: 3 [87840/110534 (79%)]\tAll Loss: 1.2355\tTriple Loss(1): 0.0354\tClassification Loss: 1.1648\n","\n","Test set: Average loss: 1.5299, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 3 [88000/110534 (80%)]\tAll Loss: 5.0845\tTriple Loss(0): 1.8883\tClassification Loss: 1.3080\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_5500.pth.tar\n","Train Epoch: 3 [88160/110534 (80%)]\tAll Loss: 2.1549\tTriple Loss(0): 0.0000\tClassification Loss: 2.1549\n","Train Epoch: 3 [88320/110534 (80%)]\tAll Loss: 1.8076\tTriple Loss(0): 0.0000\tClassification Loss: 1.8076\n","Train Epoch: 3 [88480/110534 (80%)]\tAll Loss: 1.4845\tTriple Loss(0): 0.0000\tClassification Loss: 1.4845\n","Train Epoch: 3 [88640/110534 (80%)]\tAll Loss: 1.6941\tTriple Loss(1): 0.0730\tClassification Loss: 1.5481\n","Train Epoch: 3 [88800/110534 (80%)]\tAll Loss: 1.5298\tTriple Loss(1): 0.2899\tClassification Loss: 0.9501\n","Train Epoch: 3 [88960/110534 (80%)]\tAll Loss: 1.5925\tTriple Loss(1): 0.0619\tClassification Loss: 1.4686\n","Train Epoch: 3 [89120/110534 (81%)]\tAll Loss: 2.7926\tTriple Loss(1): 0.8748\tClassification Loss: 1.0431\n","Train Epoch: 3 [89280/110534 (81%)]\tAll Loss: 1.3437\tTriple Loss(1): 0.0666\tClassification Loss: 1.2104\n","Train Epoch: 3 [89440/110534 (81%)]\tAll Loss: 2.2907\tTriple Loss(1): 0.4296\tClassification Loss: 1.4316\n","\n","Test set: Average loss: 1.5444, Accuracy: 272/480 (57%)\n","\n","Train Epoch: 3 [89600/110534 (81%)]\tAll Loss: 1.7451\tTriple Loss(1): 0.1851\tClassification Loss: 1.3749\n","Train Epoch: 3 [89760/110534 (81%)]\tAll Loss: 2.1632\tTriple Loss(1): 0.0543\tClassification Loss: 2.0545\n","Train Epoch: 3 [89920/110534 (81%)]\tAll Loss: 1.7725\tTriple Loss(1): 0.0699\tClassification Loss: 1.6327\n","Train Epoch: 3 [90080/110534 (81%)]\tAll Loss: 2.6043\tTriple Loss(1): 0.3644\tClassification Loss: 1.8756\n","Train Epoch: 3 [90240/110534 (82%)]\tAll Loss: 1.7124\tTriple Loss(0): 0.0000\tClassification Loss: 1.7124\n","Train Epoch: 3 [90400/110534 (82%)]\tAll Loss: 1.9976\tTriple Loss(1): 0.1617\tClassification Loss: 1.6741\n","Train Epoch: 3 [90560/110534 (82%)]\tAll Loss: 1.3584\tTriple Loss(1): 0.0000\tClassification Loss: 1.3584\n","Train Epoch: 3 [90720/110534 (82%)]\tAll Loss: 1.7562\tTriple Loss(1): 0.0184\tClassification Loss: 1.7194\n","Train Epoch: 3 [90880/110534 (82%)]\tAll Loss: 1.4264\tTriple Loss(1): 0.1132\tClassification Loss: 1.2001\n","Train Epoch: 3 [91040/110534 (82%)]\tAll Loss: 1.2460\tTriple Loss(1): 0.0000\tClassification Loss: 1.2460\n","\n","Test set: Average loss: 1.5444, Accuracy: 277/480 (58%)\n","\n","Train Epoch: 3 [91200/110534 (83%)]\tAll Loss: 2.2560\tTriple Loss(1): 0.3282\tClassification Loss: 1.5995\n","Train Epoch: 3 [91360/110534 (83%)]\tAll Loss: 1.7370\tTriple Loss(0): 0.0000\tClassification Loss: 1.7370\n","Train Epoch: 3 [91520/110534 (83%)]\tAll Loss: 1.7653\tTriple Loss(1): 0.2162\tClassification Loss: 1.3328\n","Train Epoch: 3 [91680/110534 (83%)]\tAll Loss: 1.7153\tTriple Loss(0): 0.0000\tClassification Loss: 1.7153\n","Train Epoch: 3 [91840/110534 (83%)]\tAll Loss: 1.7978\tTriple Loss(1): 0.0752\tClassification Loss: 1.6473\n","Train Epoch: 3 [92000/110534 (83%)]\tAll Loss: 2.4096\tTriple Loss(1): 0.4992\tClassification Loss: 1.4111\n","Train Epoch: 3 [92160/110534 (83%)]\tAll Loss: 2.2365\tTriple Loss(1): 0.2341\tClassification Loss: 1.7684\n","Train Epoch: 3 [92320/110534 (84%)]\tAll Loss: 1.7692\tTriple Loss(0): 0.0000\tClassification Loss: 1.7692\n","Train Epoch: 3 [92480/110534 (84%)]\tAll Loss: 1.4742\tTriple Loss(0): 0.0000\tClassification Loss: 1.4742\n","Train Epoch: 3 [92640/110534 (84%)]\tAll Loss: 2.1408\tTriple Loss(1): 0.2856\tClassification Loss: 1.5696\n","\n","Test set: Average loss: 1.5076, Accuracy: 278/480 (58%)\n","\n","Train Epoch: 3 [92800/110534 (84%)]\tAll Loss: 6.6095\tTriple Loss(0): 2.4444\tClassification Loss: 1.7206\n","Train Epoch: 3 [92960/110534 (84%)]\tAll Loss: 2.2019\tTriple Loss(1): 0.1180\tClassification Loss: 1.9659\n","Train Epoch: 3 [93120/110534 (84%)]\tAll Loss: 1.9619\tTriple Loss(0): 0.0000\tClassification Loss: 1.9619\n","Train Epoch: 3 [93280/110534 (84%)]\tAll Loss: 1.9389\tTriple Loss(1): 0.2232\tClassification Loss: 1.4926\n","Train Epoch: 3 [93440/110534 (85%)]\tAll Loss: 1.5403\tTriple Loss(1): 0.0000\tClassification Loss: 1.5403\n","Train Epoch: 3 [93600/110534 (85%)]\tAll Loss: 1.9227\tTriple Loss(0): 0.0000\tClassification Loss: 1.9227\n","Train Epoch: 3 [93760/110534 (85%)]\tAll Loss: 1.6391\tTriple Loss(1): 0.1401\tClassification Loss: 1.3589\n","Train Epoch: 3 [93920/110534 (85%)]\tAll Loss: 1.9523\tTriple Loss(0): 0.0000\tClassification Loss: 1.9523\n","Train Epoch: 3 [94080/110534 (85%)]\tAll Loss: 2.3973\tTriple Loss(1): 0.3935\tClassification Loss: 1.6103\n","Train Epoch: 3 [94240/110534 (85%)]\tAll Loss: 1.6779\tTriple Loss(1): 0.1636\tClassification Loss: 1.3508\n","\n","Test set: Average loss: 1.5377, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 3 [94400/110534 (85%)]\tAll Loss: 2.1532\tTriple Loss(1): 0.3176\tClassification Loss: 1.5180\n","Train Epoch: 3 [94560/110534 (86%)]\tAll Loss: 1.9661\tTriple Loss(1): 0.2605\tClassification Loss: 1.4452\n","Train Epoch: 3 [94720/110534 (86%)]\tAll Loss: 2.0003\tTriple Loss(1): 0.2074\tClassification Loss: 1.5855\n","Train Epoch: 3 [94880/110534 (86%)]\tAll Loss: 1.9144\tTriple Loss(1): 0.2641\tClassification Loss: 1.3862\n","Train Epoch: 3 [95040/110534 (86%)]\tAll Loss: 1.6874\tTriple Loss(1): 0.0231\tClassification Loss: 1.6413\n","Train Epoch: 3 [95200/110534 (86%)]\tAll Loss: 1.8146\tTriple Loss(0): 0.0000\tClassification Loss: 1.8146\n","Train Epoch: 3 [95360/110534 (86%)]\tAll Loss: 2.3942\tTriple Loss(1): 0.1340\tClassification Loss: 2.1263\n","Train Epoch: 3 [95520/110534 (86%)]\tAll Loss: 1.9718\tTriple Loss(1): 0.1279\tClassification Loss: 1.7160\n","Train Epoch: 3 [95680/110534 (87%)]\tAll Loss: 1.9587\tTriple Loss(1): 0.1206\tClassification Loss: 1.7174\n","Train Epoch: 3 [95840/110534 (87%)]\tAll Loss: 1.5113\tTriple Loss(0): 0.0000\tClassification Loss: 1.5113\n","\n","Test set: Average loss: 1.5196, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 3 [96000/110534 (87%)]\tAll Loss: 1.5743\tTriple Loss(1): 0.0751\tClassification Loss: 1.4242\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_6000.pth.tar\n","Train Epoch: 3 [96160/110534 (87%)]\tAll Loss: 1.4710\tTriple Loss(1): 0.0000\tClassification Loss: 1.4710\n","Train Epoch: 3 [96320/110534 (87%)]\tAll Loss: 3.8686\tTriple Loss(0): 1.1454\tClassification Loss: 1.5778\n","Train Epoch: 3 [96480/110534 (87%)]\tAll Loss: 2.4185\tTriple Loss(1): 0.2813\tClassification Loss: 1.8560\n","Train Epoch: 3 [96640/110534 (87%)]\tAll Loss: 1.3064\tTriple Loss(1): 0.0000\tClassification Loss: 1.3064\n","Train Epoch: 3 [96800/110534 (88%)]\tAll Loss: 2.1761\tTriple Loss(1): 0.0298\tClassification Loss: 2.1165\n","Train Epoch: 3 [96960/110534 (88%)]\tAll Loss: 1.3710\tTriple Loss(1): 0.1322\tClassification Loss: 1.1066\n","Train Epoch: 3 [97120/110534 (88%)]\tAll Loss: 1.9164\tTriple Loss(1): 0.3059\tClassification Loss: 1.3046\n","Train Epoch: 3 [97280/110534 (88%)]\tAll Loss: 2.9333\tTriple Loss(1): 0.6351\tClassification Loss: 1.6631\n","Train Epoch: 3 [97440/110534 (88%)]\tAll Loss: 1.5456\tTriple Loss(1): 0.1652\tClassification Loss: 1.2151\n","\n","Test set: Average loss: 1.5334, Accuracy: 272/480 (57%)\n","\n","Train Epoch: 3 [97600/110534 (88%)]\tAll Loss: 1.8042\tTriple Loss(1): 0.2815\tClassification Loss: 1.2412\n","Train Epoch: 3 [97760/110534 (88%)]\tAll Loss: 2.8458\tTriple Loss(1): 0.4387\tClassification Loss: 1.9684\n","Train Epoch: 3 [97920/110534 (89%)]\tAll Loss: 1.7378\tTriple Loss(1): 0.0776\tClassification Loss: 1.5827\n","Train Epoch: 3 [98080/110534 (89%)]\tAll Loss: 1.9000\tTriple Loss(1): 0.2251\tClassification Loss: 1.4497\n","Train Epoch: 3 [98240/110534 (89%)]\tAll Loss: 1.7768\tTriple Loss(1): 0.3547\tClassification Loss: 1.0674\n","Train Epoch: 3 [98400/110534 (89%)]\tAll Loss: 3.4754\tTriple Loss(0): 1.1456\tClassification Loss: 1.1842\n","Train Epoch: 3 [98560/110534 (89%)]\tAll Loss: 0.8000\tTriple Loss(0): 0.0000\tClassification Loss: 0.8000\n","Train Epoch: 3 [98720/110534 (89%)]\tAll Loss: 1.1574\tTriple Loss(1): 0.0000\tClassification Loss: 1.1574\n","Train Epoch: 3 [98880/110534 (89%)]\tAll Loss: 2.3728\tTriple Loss(0): 0.6221\tClassification Loss: 1.1285\n","Train Epoch: 3 [99040/110534 (90%)]\tAll Loss: 1.6167\tTriple Loss(0): 0.0000\tClassification Loss: 1.6167\n","\n","Test set: Average loss: 1.5038, Accuracy: 279/480 (58%)\n","\n","Train Epoch: 3 [99200/110534 (90%)]\tAll Loss: 1.2624\tTriple Loss(1): 0.1315\tClassification Loss: 0.9995\n","Train Epoch: 3 [99360/110534 (90%)]\tAll Loss: 1.2137\tTriple Loss(0): 0.0000\tClassification Loss: 1.2137\n","Train Epoch: 3 [99520/110534 (90%)]\tAll Loss: 1.8758\tTriple Loss(1): 0.2760\tClassification Loss: 1.3237\n","Train Epoch: 3 [99680/110534 (90%)]\tAll Loss: 1.5476\tTriple Loss(0): 0.0000\tClassification Loss: 1.5476\n","Train Epoch: 3 [99840/110534 (90%)]\tAll Loss: 2.4782\tTriple Loss(1): 0.1961\tClassification Loss: 2.0860\n","Train Epoch: 3 [100000/110534 (90%)]\tAll Loss: 2.1909\tTriple Loss(1): 0.2603\tClassification Loss: 1.6703\n","Train Epoch: 3 [100160/110534 (91%)]\tAll Loss: 2.4372\tTriple Loss(1): 0.2567\tClassification Loss: 1.9238\n","Train Epoch: 3 [100320/110534 (91%)]\tAll Loss: 1.9023\tTriple Loss(1): 0.2370\tClassification Loss: 1.4284\n","Train Epoch: 3 [100480/110534 (91%)]\tAll Loss: 2.3631\tTriple Loss(1): 0.2757\tClassification Loss: 1.8117\n","Train Epoch: 3 [100640/110534 (91%)]\tAll Loss: 1.9492\tTriple Loss(1): 0.2992\tClassification Loss: 1.3508\n","\n","Test set: Average loss: 1.5162, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 3 [100800/110534 (91%)]\tAll Loss: 2.2537\tTriple Loss(1): 0.0658\tClassification Loss: 2.1222\n","Train Epoch: 3 [100960/110534 (91%)]\tAll Loss: 2.0828\tTriple Loss(1): 0.1451\tClassification Loss: 1.7926\n","Train Epoch: 3 [101120/110534 (91%)]\tAll Loss: 2.3454\tTriple Loss(1): 0.3863\tClassification Loss: 1.5727\n","Train Epoch: 3 [101280/110534 (92%)]\tAll Loss: 1.1784\tTriple Loss(1): 0.0347\tClassification Loss: 1.1090\n","Train Epoch: 3 [101440/110534 (92%)]\tAll Loss: 1.1966\tTriple Loss(1): 0.0223\tClassification Loss: 1.1521\n","Train Epoch: 3 [101600/110534 (92%)]\tAll Loss: 1.0247\tTriple Loss(0): 0.0000\tClassification Loss: 1.0247\n","Train Epoch: 3 [101760/110534 (92%)]\tAll Loss: 1.5442\tTriple Loss(1): 0.0273\tClassification Loss: 1.4897\n","Train Epoch: 3 [101920/110534 (92%)]\tAll Loss: 2.0718\tTriple Loss(1): 0.3154\tClassification Loss: 1.4410\n","Train Epoch: 3 [102080/110534 (92%)]\tAll Loss: 1.7301\tTriple Loss(0): 0.0000\tClassification Loss: 1.7301\n","Train Epoch: 3 [102240/110534 (92%)]\tAll Loss: 1.5935\tTriple Loss(1): 0.1423\tClassification Loss: 1.3088\n","\n","Test set: Average loss: 1.5512, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 3 [102400/110534 (93%)]\tAll Loss: 1.3052\tTriple Loss(1): 0.0860\tClassification Loss: 1.1332\n","Train Epoch: 3 [102560/110534 (93%)]\tAll Loss: 1.1935\tTriple Loss(0): 0.0000\tClassification Loss: 1.1935\n","Train Epoch: 3 [102720/110534 (93%)]\tAll Loss: 1.3421\tTriple Loss(1): 0.0619\tClassification Loss: 1.2183\n","Train Epoch: 3 [102880/110534 (93%)]\tAll Loss: 1.5877\tTriple Loss(1): 0.1496\tClassification Loss: 1.2884\n","Train Epoch: 3 [103040/110534 (93%)]\tAll Loss: 1.7159\tTriple Loss(1): 0.2334\tClassification Loss: 1.2491\n","Train Epoch: 3 [103200/110534 (93%)]\tAll Loss: 2.6606\tTriple Loss(1): 0.1094\tClassification Loss: 2.4418\n","Train Epoch: 3 [103360/110534 (94%)]\tAll Loss: 0.9595\tTriple Loss(0): 0.0000\tClassification Loss: 0.9595\n","Train Epoch: 3 [103520/110534 (94%)]\tAll Loss: 2.1264\tTriple Loss(1): 0.1077\tClassification Loss: 1.9109\n","Train Epoch: 3 [103680/110534 (94%)]\tAll Loss: 1.9487\tTriple Loss(1): 0.2050\tClassification Loss: 1.5387\n","Train Epoch: 3 [103840/110534 (94%)]\tAll Loss: 1.7672\tTriple Loss(1): 0.2477\tClassification Loss: 1.2718\n","\n","Test set: Average loss: 1.5272, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 3 [104000/110534 (94%)]\tAll Loss: 2.3612\tTriple Loss(1): 0.5729\tClassification Loss: 1.2155\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_6500.pth.tar\n","Train Epoch: 3 [104160/110534 (94%)]\tAll Loss: 1.5761\tTriple Loss(1): 0.0786\tClassification Loss: 1.4188\n","Train Epoch: 3 [104320/110534 (94%)]\tAll Loss: 2.0755\tTriple Loss(1): 0.1449\tClassification Loss: 1.7858\n","Train Epoch: 3 [104480/110534 (95%)]\tAll Loss: 2.3447\tTriple Loss(1): 0.2003\tClassification Loss: 1.9441\n","Train Epoch: 3 [104640/110534 (95%)]\tAll Loss: 1.8498\tTriple Loss(1): 0.0780\tClassification Loss: 1.6937\n","Train Epoch: 3 [104800/110534 (95%)]\tAll Loss: 1.5186\tTriple Loss(1): 0.0907\tClassification Loss: 1.3372\n","Train Epoch: 3 [104960/110534 (95%)]\tAll Loss: 1.7085\tTriple Loss(1): 0.0535\tClassification Loss: 1.6014\n","Train Epoch: 3 [105120/110534 (95%)]\tAll Loss: 1.8283\tTriple Loss(1): 0.1413\tClassification Loss: 1.5457\n","Train Epoch: 3 [105280/110534 (95%)]\tAll Loss: 1.7864\tTriple Loss(1): 0.1422\tClassification Loss: 1.5020\n","Train Epoch: 3 [105440/110534 (95%)]\tAll Loss: 2.5368\tTriple Loss(1): 0.5235\tClassification Loss: 1.4898\n","\n","Test set: Average loss: 1.5069, Accuracy: 283/480 (59%)\n","\n","Train Epoch: 3 [105600/110534 (96%)]\tAll Loss: 1.9733\tTriple Loss(1): 0.1961\tClassification Loss: 1.5812\n","Train Epoch: 3 [105760/110534 (96%)]\tAll Loss: 1.3821\tTriple Loss(1): 0.0164\tClassification Loss: 1.3494\n","Train Epoch: 3 [105920/110534 (96%)]\tAll Loss: 2.2991\tTriple Loss(1): 0.2225\tClassification Loss: 1.8542\n","Train Epoch: 3 [106080/110534 (96%)]\tAll Loss: 1.1544\tTriple Loss(1): 0.0078\tClassification Loss: 1.1387\n","Train Epoch: 3 [106240/110534 (96%)]\tAll Loss: 2.3206\tTriple Loss(1): 0.3181\tClassification Loss: 1.6843\n","Train Epoch: 3 [106400/110534 (96%)]\tAll Loss: 2.7550\tTriple Loss(1): 0.4548\tClassification Loss: 1.8453\n","Train Epoch: 3 [106560/110534 (96%)]\tAll Loss: 2.5033\tTriple Loss(1): 0.6871\tClassification Loss: 1.1292\n","Train Epoch: 3 [106720/110534 (97%)]\tAll Loss: 1.5866\tTriple Loss(0): 0.0000\tClassification Loss: 1.5866\n","Train Epoch: 3 [106880/110534 (97%)]\tAll Loss: 2.3351\tTriple Loss(1): 0.4501\tClassification Loss: 1.4350\n","Train Epoch: 3 [107040/110534 (97%)]\tAll Loss: 2.3554\tTriple Loss(1): 0.2749\tClassification Loss: 1.8056\n","\n","Test set: Average loss: 1.5630, Accuracy: 265/480 (55%)\n","\n","Train Epoch: 3 [107200/110534 (97%)]\tAll Loss: 1.7883\tTriple Loss(1): 0.3198\tClassification Loss: 1.1486\n","Train Epoch: 3 [107360/110534 (97%)]\tAll Loss: 1.6495\tTriple Loss(0): 0.0000\tClassification Loss: 1.6495\n","Train Epoch: 3 [107520/110534 (97%)]\tAll Loss: 2.7725\tTriple Loss(1): 0.8178\tClassification Loss: 1.1369\n","Train Epoch: 3 [107680/110534 (97%)]\tAll Loss: 2.3509\tTriple Loss(1): 0.1954\tClassification Loss: 1.9601\n","Train Epoch: 3 [107840/110534 (98%)]\tAll Loss: 7.8898\tTriple Loss(0): 3.3193\tClassification Loss: 1.2512\n","Train Epoch: 3 [108000/110534 (98%)]\tAll Loss: 1.3182\tTriple Loss(1): 0.1775\tClassification Loss: 0.9632\n","Train Epoch: 3 [108160/110534 (98%)]\tAll Loss: 1.6790\tTriple Loss(1): 0.0000\tClassification Loss: 1.6790\n","Train Epoch: 3 [108320/110534 (98%)]\tAll Loss: 1.8883\tTriple Loss(1): 0.0000\tClassification Loss: 1.8883\n","Train Epoch: 3 [108480/110534 (98%)]\tAll Loss: 2.0364\tTriple Loss(1): 0.3922\tClassification Loss: 1.2520\n","Train Epoch: 3 [108640/110534 (98%)]\tAll Loss: 1.3279\tTriple Loss(1): 0.0050\tClassification Loss: 1.3178\n","\n","Test set: Average loss: 1.5283, Accuracy: 277/480 (58%)\n","\n","Train Epoch: 3 [108800/110534 (98%)]\tAll Loss: 2.2910\tTriple Loss(1): 0.0549\tClassification Loss: 2.1812\n","Train Epoch: 3 [108960/110534 (99%)]\tAll Loss: 1.4460\tTriple Loss(1): 0.0261\tClassification Loss: 1.3938\n","Train Epoch: 3 [109120/110534 (99%)]\tAll Loss: 2.5806\tTriple Loss(1): 0.6582\tClassification Loss: 1.2643\n","Train Epoch: 3 [109280/110534 (99%)]\tAll Loss: 2.2690\tTriple Loss(1): 0.0881\tClassification Loss: 2.0928\n","Train Epoch: 3 [109440/110534 (99%)]\tAll Loss: 2.3534\tTriple Loss(1): 0.2000\tClassification Loss: 1.9534\n","Train Epoch: 3 [109600/110534 (99%)]\tAll Loss: 2.4424\tTriple Loss(1): 0.2671\tClassification Loss: 1.9082\n","Train Epoch: 3 [109760/110534 (99%)]\tAll Loss: 1.6170\tTriple Loss(1): 0.1903\tClassification Loss: 1.2363\n","Train Epoch: 3 [109920/110534 (99%)]\tAll Loss: 1.7960\tTriple Loss(1): 0.2870\tClassification Loss: 1.2220\n","Train Epoch: 3 [110080/110534 (100%)]\tAll Loss: 2.5173\tTriple Loss(0): 0.1855\tClassification Loss: 2.1463\n","Train Epoch: 3 [110240/110534 (100%)]\tAll Loss: 2.0751\tTriple Loss(1): 0.2249\tClassification Loss: 1.6252\n","\n","Test set: Average loss: 1.5335, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 3 [110400/110534 (100%)]\tAll Loss: 2.3007\tTriple Loss(0): 0.0000\tClassification Loss: 2.3007\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_final.pth.tar\n","\n","Test set: Average loss: 1.5585, Accuracy: 265/480 (55%)\n","\n","Train Epoch: 4 [0/110534 (0%)]\tAll Loss: 2.1862\tTriple Loss(1): 0.1160\tClassification Loss: 1.9542\n","Train Epoch: 4 [160/110534 (0%)]\tAll Loss: 2.3124\tTriple Loss(1): 0.1880\tClassification Loss: 1.9364\n","Train Epoch: 4 [320/110534 (0%)]\tAll Loss: 2.1385\tTriple Loss(1): 0.1946\tClassification Loss: 1.7493\n","Train Epoch: 4 [480/110534 (0%)]\tAll Loss: 1.4194\tTriple Loss(1): 0.0118\tClassification Loss: 1.3959\n","Train Epoch: 4 [640/110534 (1%)]\tAll Loss: 1.9983\tTriple Loss(1): 0.1437\tClassification Loss: 1.7109\n","Train Epoch: 4 [800/110534 (1%)]\tAll Loss: 1.9384\tTriple Loss(1): 0.1737\tClassification Loss: 1.5909\n","Train Epoch: 4 [960/110534 (1%)]\tAll Loss: 1.6587\tTriple Loss(1): 0.0665\tClassification Loss: 1.5258\n","Train Epoch: 4 [1120/110534 (1%)]\tAll Loss: 2.1489\tTriple Loss(1): 0.0899\tClassification Loss: 1.9692\n","Train Epoch: 4 [1280/110534 (1%)]\tAll Loss: 1.8269\tTriple Loss(0): 0.0000\tClassification Loss: 1.8269\n","Train Epoch: 4 [1440/110534 (1%)]\tAll Loss: 2.0827\tTriple Loss(1): 0.2029\tClassification Loss: 1.6768\n","\n","Test set: Average loss: 1.5229, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 4 [1600/110534 (1%)]\tAll Loss: 1.3427\tTriple Loss(1): 0.1347\tClassification Loss: 1.0732\n","Train Epoch: 4 [1760/110534 (2%)]\tAll Loss: 1.0510\tTriple Loss(1): 0.0436\tClassification Loss: 0.9637\n","Train Epoch: 4 [1920/110534 (2%)]\tAll Loss: 1.6608\tTriple Loss(0): 0.0000\tClassification Loss: 1.6608\n","Train Epoch: 4 [2080/110534 (2%)]\tAll Loss: 2.7018\tTriple Loss(1): 0.2504\tClassification Loss: 2.2010\n","Train Epoch: 4 [2240/110534 (2%)]\tAll Loss: 2.1858\tTriple Loss(1): 0.2608\tClassification Loss: 1.6643\n","Train Epoch: 4 [2400/110534 (2%)]\tAll Loss: 2.1235\tTriple Loss(1): 0.3995\tClassification Loss: 1.3245\n","Train Epoch: 4 [2560/110534 (2%)]\tAll Loss: 2.0274\tTriple Loss(1): 0.4354\tClassification Loss: 1.1566\n","Train Epoch: 4 [2720/110534 (2%)]\tAll Loss: 1.3651\tTriple Loss(0): 0.0000\tClassification Loss: 1.3651\n","Train Epoch: 4 [2880/110534 (3%)]\tAll Loss: 2.4472\tTriple Loss(1): 0.2151\tClassification Loss: 2.0171\n","Train Epoch: 4 [3040/110534 (3%)]\tAll Loss: 2.2278\tTriple Loss(1): 0.4272\tClassification Loss: 1.3734\n","\n","Test set: Average loss: 1.5147, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 4 [3200/110534 (3%)]\tAll Loss: 1.3987\tTriple Loss(1): 0.1132\tClassification Loss: 1.1724\n","Train Epoch: 4 [3360/110534 (3%)]\tAll Loss: 1.2285\tTriple Loss(1): 0.0033\tClassification Loss: 1.2218\n","Train Epoch: 4 [3520/110534 (3%)]\tAll Loss: 1.7773\tTriple Loss(1): 0.0000\tClassification Loss: 1.7773\n","Train Epoch: 4 [3680/110534 (3%)]\tAll Loss: 1.7705\tTriple Loss(1): 0.1530\tClassification Loss: 1.4644\n","Train Epoch: 4 [3840/110534 (3%)]\tAll Loss: 1.7787\tTriple Loss(1): 0.0098\tClassification Loss: 1.7592\n","Train Epoch: 4 [4000/110534 (4%)]\tAll Loss: 1.8951\tTriple Loss(0): 0.0000\tClassification Loss: 1.8951\n","Train Epoch: 4 [4160/110534 (4%)]\tAll Loss: 1.3394\tTriple Loss(0): 0.0000\tClassification Loss: 1.3394\n","Train Epoch: 4 [4320/110534 (4%)]\tAll Loss: 1.7422\tTriple Loss(1): 0.0198\tClassification Loss: 1.7026\n","Train Epoch: 4 [4480/110534 (4%)]\tAll Loss: 2.1239\tTriple Loss(1): 0.1137\tClassification Loss: 1.8965\n","Train Epoch: 4 [4640/110534 (4%)]\tAll Loss: 1.4725\tTriple Loss(1): 0.0273\tClassification Loss: 1.4178\n","\n","Test set: Average loss: 1.5550, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 4 [4800/110534 (4%)]\tAll Loss: 2.7141\tTriple Loss(1): 0.5534\tClassification Loss: 1.6072\n","Train Epoch: 4 [4960/110534 (4%)]\tAll Loss: 1.3575\tTriple Loss(1): 0.0410\tClassification Loss: 1.2755\n","Train Epoch: 4 [5120/110534 (5%)]\tAll Loss: 1.9366\tTriple Loss(1): 0.0799\tClassification Loss: 1.7767\n","Train Epoch: 4 [5280/110534 (5%)]\tAll Loss: 1.0909\tTriple Loss(1): 0.0955\tClassification Loss: 0.9000\n","Train Epoch: 4 [5440/110534 (5%)]\tAll Loss: 1.9658\tTriple Loss(1): 0.2237\tClassification Loss: 1.5183\n","Train Epoch: 4 [5600/110534 (5%)]\tAll Loss: 1.3413\tTriple Loss(1): 0.1330\tClassification Loss: 1.0753\n","Train Epoch: 4 [5760/110534 (5%)]\tAll Loss: 1.9025\tTriple Loss(1): 0.3446\tClassification Loss: 1.2134\n","Train Epoch: 4 [5920/110534 (5%)]\tAll Loss: 1.8565\tTriple Loss(1): 0.0985\tClassification Loss: 1.6595\n","Train Epoch: 4 [6080/110534 (6%)]\tAll Loss: 1.3152\tTriple Loss(0): 0.0000\tClassification Loss: 1.3152\n","Train Epoch: 4 [6240/110534 (6%)]\tAll Loss: 1.5788\tTriple Loss(1): 0.0410\tClassification Loss: 1.4968\n","\n","Test set: Average loss: 1.5307, Accuracy: 280/480 (58%)\n","\n","Train Epoch: 4 [6400/110534 (6%)]\tAll Loss: 1.6903\tTriple Loss(1): 0.1501\tClassification Loss: 1.3901\n","Train Epoch: 4 [6560/110534 (6%)]\tAll Loss: 5.2990\tTriple Loss(0): 2.0185\tClassification Loss: 1.2621\n","Train Epoch: 4 [6720/110534 (6%)]\tAll Loss: 1.9564\tTriple Loss(1): 0.1594\tClassification Loss: 1.6375\n","Train Epoch: 4 [6880/110534 (6%)]\tAll Loss: 1.7820\tTriple Loss(1): 0.2430\tClassification Loss: 1.2961\n","Train Epoch: 4 [7040/110534 (6%)]\tAll Loss: 1.4426\tTriple Loss(1): 0.1193\tClassification Loss: 1.2040\n","Train Epoch: 4 [7200/110534 (7%)]\tAll Loss: 2.8164\tTriple Loss(1): 0.3922\tClassification Loss: 2.0320\n","Train Epoch: 4 [7360/110534 (7%)]\tAll Loss: 1.7150\tTriple Loss(0): 0.0000\tClassification Loss: 1.7150\n","Train Epoch: 4 [7520/110534 (7%)]\tAll Loss: 1.7504\tTriple Loss(1): 0.0660\tClassification Loss: 1.6185\n","Train Epoch: 4 [7680/110534 (7%)]\tAll Loss: 2.4545\tTriple Loss(1): 0.3856\tClassification Loss: 1.6832\n","Train Epoch: 4 [7840/110534 (7%)]\tAll Loss: 1.6865\tTriple Loss(1): 0.2633\tClassification Loss: 1.1600\n","\n","Test set: Average loss: 1.5315, Accuracy: 279/480 (58%)\n","\n","Train Epoch: 4 [8000/110534 (7%)]\tAll Loss: 2.0790\tTriple Loss(0): 0.0000\tClassification Loss: 2.0790\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_500.pth.tar\n","Train Epoch: 4 [8160/110534 (7%)]\tAll Loss: 2.0088\tTriple Loss(1): 0.1646\tClassification Loss: 1.6796\n","Train Epoch: 4 [8320/110534 (8%)]\tAll Loss: 1.6042\tTriple Loss(0): 0.0000\tClassification Loss: 1.6042\n","Train Epoch: 4 [8480/110534 (8%)]\tAll Loss: 2.1858\tTriple Loss(1): 0.1316\tClassification Loss: 1.9227\n","Train Epoch: 4 [8640/110534 (8%)]\tAll Loss: 1.0989\tTriple Loss(0): 0.0000\tClassification Loss: 1.0989\n","Train Epoch: 4 [8800/110534 (8%)]\tAll Loss: 1.3842\tTriple Loss(1): 0.0523\tClassification Loss: 1.2796\n","Train Epoch: 4 [8960/110534 (8%)]\tAll Loss: 1.3344\tTriple Loss(0): 0.0000\tClassification Loss: 1.3344\n","Train Epoch: 4 [9120/110534 (8%)]\tAll Loss: 2.3041\tTriple Loss(1): 0.1864\tClassification Loss: 1.9312\n","Train Epoch: 4 [9280/110534 (8%)]\tAll Loss: 1.5467\tTriple Loss(1): 0.0068\tClassification Loss: 1.5332\n","Train Epoch: 4 [9440/110534 (9%)]\tAll Loss: 2.2225\tTriple Loss(1): 0.1111\tClassification Loss: 2.0002\n","\n","Test set: Average loss: 1.5270, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 4 [9600/110534 (9%)]\tAll Loss: 1.8750\tTriple Loss(1): 0.0000\tClassification Loss: 1.8750\n","Train Epoch: 4 [9760/110534 (9%)]\tAll Loss: 2.7565\tTriple Loss(1): 0.3143\tClassification Loss: 2.1279\n","Train Epoch: 4 [9920/110534 (9%)]\tAll Loss: 1.4747\tTriple Loss(0): 0.0000\tClassification Loss: 1.4747\n","Train Epoch: 4 [10080/110534 (9%)]\tAll Loss: 1.0335\tTriple Loss(0): 0.0000\tClassification Loss: 1.0335\n","Train Epoch: 4 [10240/110534 (9%)]\tAll Loss: 1.8462\tTriple Loss(1): 0.1994\tClassification Loss: 1.4474\n","Train Epoch: 4 [10400/110534 (9%)]\tAll Loss: 1.7881\tTriple Loss(1): 0.0392\tClassification Loss: 1.7098\n","Train Epoch: 4 [10560/110534 (10%)]\tAll Loss: 2.1955\tTriple Loss(1): 0.2317\tClassification Loss: 1.7321\n","Train Epoch: 4 [10720/110534 (10%)]\tAll Loss: 1.5468\tTriple Loss(1): 0.0000\tClassification Loss: 1.5468\n","Train Epoch: 4 [10880/110534 (10%)]\tAll Loss: 1.7595\tTriple Loss(1): 0.1885\tClassification Loss: 1.3825\n","Train Epoch: 4 [11040/110534 (10%)]\tAll Loss: 2.3246\tTriple Loss(1): 0.0478\tClassification Loss: 2.2291\n","\n","Test set: Average loss: 1.4905, Accuracy: 278/480 (58%)\n","\n","Train Epoch: 4 [11200/110534 (10%)]\tAll Loss: 2.1458\tTriple Loss(1): 0.0224\tClassification Loss: 2.1010\n","Train Epoch: 4 [11360/110534 (10%)]\tAll Loss: 2.5121\tTriple Loss(1): 0.3050\tClassification Loss: 1.9021\n","Train Epoch: 4 [11520/110534 (10%)]\tAll Loss: 3.0854\tTriple Loss(1): 0.4106\tClassification Loss: 2.2643\n","Train Epoch: 4 [11680/110534 (11%)]\tAll Loss: 1.7582\tTriple Loss(1): 0.1388\tClassification Loss: 1.4806\n","Train Epoch: 4 [11840/110534 (11%)]\tAll Loss: 1.8659\tTriple Loss(0): 0.0000\tClassification Loss: 1.8659\n","Train Epoch: 4 [12000/110534 (11%)]\tAll Loss: 2.2993\tTriple Loss(1): 0.1984\tClassification Loss: 1.9024\n","Train Epoch: 4 [12160/110534 (11%)]\tAll Loss: 1.9651\tTriple Loss(1): 0.4408\tClassification Loss: 1.0836\n","Train Epoch: 4 [12320/110534 (11%)]\tAll Loss: 1.8624\tTriple Loss(1): 0.1119\tClassification Loss: 1.6386\n","Train Epoch: 4 [12480/110534 (11%)]\tAll Loss: 1.4869\tTriple Loss(1): 0.0000\tClassification Loss: 1.4869\n","Train Epoch: 4 [12640/110534 (11%)]\tAll Loss: 1.7142\tTriple Loss(1): 0.1475\tClassification Loss: 1.4192\n","\n","Test set: Average loss: 1.5161, Accuracy: 280/480 (58%)\n","\n","Train Epoch: 4 [12800/110534 (12%)]\tAll Loss: 2.0851\tTriple Loss(1): 0.0776\tClassification Loss: 1.9300\n","Train Epoch: 4 [12960/110534 (12%)]\tAll Loss: 2.0587\tTriple Loss(1): 0.1697\tClassification Loss: 1.7193\n","Train Epoch: 4 [13120/110534 (12%)]\tAll Loss: 2.2792\tTriple Loss(1): 0.3998\tClassification Loss: 1.4797\n","Train Epoch: 4 [13280/110534 (12%)]\tAll Loss: 2.3303\tTriple Loss(1): 0.0640\tClassification Loss: 2.2024\n","Train Epoch: 4 [13440/110534 (12%)]\tAll Loss: 1.4961\tTriple Loss(1): 0.0170\tClassification Loss: 1.4621\n","Train Epoch: 4 [13600/110534 (12%)]\tAll Loss: 1.2189\tTriple Loss(1): 0.0546\tClassification Loss: 1.1097\n","Train Epoch: 4 [13760/110534 (12%)]\tAll Loss: 1.9781\tTriple Loss(1): 0.0860\tClassification Loss: 1.8061\n","Train Epoch: 4 [13920/110534 (13%)]\tAll Loss: 1.9942\tTriple Loss(1): 0.3235\tClassification Loss: 1.3472\n","Train Epoch: 4 [14080/110534 (13%)]\tAll Loss: 1.9273\tTriple Loss(1): 0.4191\tClassification Loss: 1.0890\n","Train Epoch: 4 [14240/110534 (13%)]\tAll Loss: 1.6882\tTriple Loss(1): 0.2780\tClassification Loss: 1.1321\n","\n","Test set: Average loss: 1.5184, Accuracy: 271/480 (56%)\n","\n","Train Epoch: 4 [14400/110534 (13%)]\tAll Loss: 1.9468\tTriple Loss(1): 0.0784\tClassification Loss: 1.7899\n","Train Epoch: 4 [14560/110534 (13%)]\tAll Loss: 1.3987\tTriple Loss(1): 0.2162\tClassification Loss: 0.9662\n","Train Epoch: 4 [14720/110534 (13%)]\tAll Loss: 2.1417\tTriple Loss(1): 0.1486\tClassification Loss: 1.8445\n","Train Epoch: 4 [14880/110534 (13%)]\tAll Loss: 1.9996\tTriple Loss(1): 0.1533\tClassification Loss: 1.6929\n","Train Epoch: 4 [15040/110534 (14%)]\tAll Loss: 1.6610\tTriple Loss(1): 0.1590\tClassification Loss: 1.3430\n","Train Epoch: 4 [15200/110534 (14%)]\tAll Loss: 2.6764\tTriple Loss(1): 0.3747\tClassification Loss: 1.9270\n","Train Epoch: 4 [15360/110534 (14%)]\tAll Loss: 1.5788\tTriple Loss(1): 0.0739\tClassification Loss: 1.4310\n","Train Epoch: 4 [15520/110534 (14%)]\tAll Loss: 2.2294\tTriple Loss(1): 0.2657\tClassification Loss: 1.6980\n","Train Epoch: 4 [15680/110534 (14%)]\tAll Loss: 1.6369\tTriple Loss(0): 0.0000\tClassification Loss: 1.6369\n","Train Epoch: 4 [15840/110534 (14%)]\tAll Loss: 1.7179\tTriple Loss(1): 0.0323\tClassification Loss: 1.6533\n","\n","Test set: Average loss: 1.5679, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 4 [16000/110534 (14%)]\tAll Loss: 2.2527\tTriple Loss(1): 0.2599\tClassification Loss: 1.7330\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_1000.pth.tar\n","Train Epoch: 4 [16160/110534 (15%)]\tAll Loss: 0.9845\tTriple Loss(1): 0.0807\tClassification Loss: 0.8230\n","Train Epoch: 4 [16320/110534 (15%)]\tAll Loss: 1.9446\tTriple Loss(1): 0.4857\tClassification Loss: 0.9732\n","Train Epoch: 4 [16480/110534 (15%)]\tAll Loss: 1.9093\tTriple Loss(1): 0.3096\tClassification Loss: 1.2901\n","Train Epoch: 4 [16640/110534 (15%)]\tAll Loss: 2.5910\tTriple Loss(1): 0.2266\tClassification Loss: 2.1378\n","Train Epoch: 4 [16800/110534 (15%)]\tAll Loss: 1.4373\tTriple Loss(1): 0.0467\tClassification Loss: 1.3439\n","Train Epoch: 4 [16960/110534 (15%)]\tAll Loss: 1.7881\tTriple Loss(1): 0.0156\tClassification Loss: 1.7570\n","Train Epoch: 4 [17120/110534 (15%)]\tAll Loss: 2.0155\tTriple Loss(1): 0.1542\tClassification Loss: 1.7070\n","Train Epoch: 4 [17280/110534 (16%)]\tAll Loss: 2.3757\tTriple Loss(1): 0.1761\tClassification Loss: 2.0235\n","Train Epoch: 4 [17440/110534 (16%)]\tAll Loss: 1.8203\tTriple Loss(1): 0.2457\tClassification Loss: 1.3290\n","\n","Test set: Average loss: 1.5402, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 4 [17600/110534 (16%)]\tAll Loss: 1.9504\tTriple Loss(1): 0.3442\tClassification Loss: 1.2619\n","Train Epoch: 4 [17760/110534 (16%)]\tAll Loss: 1.8558\tTriple Loss(0): 0.0000\tClassification Loss: 1.8558\n","Train Epoch: 4 [17920/110534 (16%)]\tAll Loss: 2.4957\tTriple Loss(1): 0.2422\tClassification Loss: 2.0114\n","Train Epoch: 4 [18080/110534 (16%)]\tAll Loss: 1.5728\tTriple Loss(1): 0.0984\tClassification Loss: 1.3760\n","Train Epoch: 4 [18240/110534 (17%)]\tAll Loss: 1.7236\tTriple Loss(1): 0.0595\tClassification Loss: 1.6047\n","Train Epoch: 4 [18400/110534 (17%)]\tAll Loss: 2.0988\tTriple Loss(1): 0.3406\tClassification Loss: 1.4176\n","Train Epoch: 4 [18560/110534 (17%)]\tAll Loss: 2.3450\tTriple Loss(0): 0.0000\tClassification Loss: 2.3450\n","Train Epoch: 4 [18720/110534 (17%)]\tAll Loss: 1.8211\tTriple Loss(0): 0.0000\tClassification Loss: 1.8211\n","Train Epoch: 4 [18880/110534 (17%)]\tAll Loss: 1.7999\tTriple Loss(1): 0.1772\tClassification Loss: 1.4455\n","Train Epoch: 4 [19040/110534 (17%)]\tAll Loss: 1.0231\tTriple Loss(1): 0.0335\tClassification Loss: 0.9561\n","\n","Test set: Average loss: 1.5239, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 4 [19200/110534 (17%)]\tAll Loss: 2.3995\tTriple Loss(1): 0.2704\tClassification Loss: 1.8588\n","Train Epoch: 4 [19360/110534 (18%)]\tAll Loss: 1.9674\tTriple Loss(1): 0.2993\tClassification Loss: 1.3688\n","Train Epoch: 4 [19520/110534 (18%)]\tAll Loss: 2.1339\tTriple Loss(1): 0.2994\tClassification Loss: 1.5350\n","Train Epoch: 4 [19680/110534 (18%)]\tAll Loss: 1.5458\tTriple Loss(1): 0.1773\tClassification Loss: 1.1912\n","Train Epoch: 4 [19840/110534 (18%)]\tAll Loss: 2.4198\tTriple Loss(1): 0.3117\tClassification Loss: 1.7964\n","Train Epoch: 4 [20000/110534 (18%)]\tAll Loss: 1.6987\tTriple Loss(1): 0.1974\tClassification Loss: 1.3038\n","Train Epoch: 4 [20160/110534 (18%)]\tAll Loss: 2.0068\tTriple Loss(1): 0.1289\tClassification Loss: 1.7491\n","Train Epoch: 4 [20320/110534 (18%)]\tAll Loss: 2.5351\tTriple Loss(1): 0.3413\tClassification Loss: 1.8526\n","Train Epoch: 4 [20480/110534 (19%)]\tAll Loss: 1.5321\tTriple Loss(0): 0.0000\tClassification Loss: 1.5321\n","Train Epoch: 4 [20640/110534 (19%)]\tAll Loss: 1.6143\tTriple Loss(0): 0.0000\tClassification Loss: 1.6143\n","\n","Test set: Average loss: 1.4927, Accuracy: 280/480 (58%)\n","\n","Train Epoch: 4 [20800/110534 (19%)]\tAll Loss: 1.6260\tTriple Loss(1): 0.0540\tClassification Loss: 1.5180\n","Train Epoch: 4 [20960/110534 (19%)]\tAll Loss: 1.3125\tTriple Loss(1): 0.0000\tClassification Loss: 1.3125\n","Train Epoch: 4 [21120/110534 (19%)]\tAll Loss: 1.7356\tTriple Loss(1): 0.2474\tClassification Loss: 1.2408\n","Train Epoch: 4 [21280/110534 (19%)]\tAll Loss: 1.7525\tTriple Loss(1): 0.0979\tClassification Loss: 1.5567\n","Train Epoch: 4 [21440/110534 (19%)]\tAll Loss: 1.5441\tTriple Loss(1): 0.2525\tClassification Loss: 1.0391\n","Train Epoch: 4 [21600/110534 (20%)]\tAll Loss: 1.6019\tTriple Loss(0): 0.0000\tClassification Loss: 1.6019\n","Train Epoch: 4 [21760/110534 (20%)]\tAll Loss: 1.3595\tTriple Loss(1): 0.0236\tClassification Loss: 1.3123\n","Train Epoch: 4 [21920/110534 (20%)]\tAll Loss: 1.7820\tTriple Loss(1): 0.4317\tClassification Loss: 0.9186\n","Train Epoch: 4 [22080/110534 (20%)]\tAll Loss: 1.6589\tTriple Loss(1): 0.1049\tClassification Loss: 1.4490\n","Train Epoch: 4 [22240/110534 (20%)]\tAll Loss: 1.7287\tTriple Loss(1): 0.0979\tClassification Loss: 1.5329\n","\n","Test set: Average loss: 1.5302, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 4 [22400/110534 (20%)]\tAll Loss: 1.7831\tTriple Loss(1): 0.0751\tClassification Loss: 1.6329\n","Train Epoch: 4 [22560/110534 (20%)]\tAll Loss: 1.8222\tTriple Loss(0): 0.0000\tClassification Loss: 1.8222\n","Train Epoch: 4 [22720/110534 (21%)]\tAll Loss: 2.4761\tTriple Loss(1): 0.2083\tClassification Loss: 2.0596\n","Train Epoch: 4 [22880/110534 (21%)]\tAll Loss: 1.6732\tTriple Loss(1): 0.2037\tClassification Loss: 1.2658\n","Train Epoch: 4 [23040/110534 (21%)]\tAll Loss: 1.4504\tTriple Loss(1): 0.0103\tClassification Loss: 1.4298\n","Train Epoch: 4 [23200/110534 (21%)]\tAll Loss: 2.3645\tTriple Loss(1): 0.1907\tClassification Loss: 1.9831\n","Train Epoch: 4 [23360/110534 (21%)]\tAll Loss: 1.0033\tTriple Loss(0): 0.0000\tClassification Loss: 1.0033\n","Train Epoch: 4 [23520/110534 (21%)]\tAll Loss: 1.7896\tTriple Loss(1): 0.1274\tClassification Loss: 1.5349\n","Train Epoch: 4 [23680/110534 (21%)]\tAll Loss: 1.9305\tTriple Loss(0): 0.0000\tClassification Loss: 1.9305\n","Train Epoch: 4 [23840/110534 (22%)]\tAll Loss: 2.3013\tTriple Loss(1): 0.1433\tClassification Loss: 2.0147\n","\n","Test set: Average loss: 1.5169, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 4 [24000/110534 (22%)]\tAll Loss: 1.6545\tTriple Loss(1): 0.1603\tClassification Loss: 1.3339\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_1500.pth.tar\n","Train Epoch: 4 [24160/110534 (22%)]\tAll Loss: 2.2073\tTriple Loss(1): 0.2558\tClassification Loss: 1.6957\n","Train Epoch: 4 [24320/110534 (22%)]\tAll Loss: 1.8179\tTriple Loss(1): 0.2845\tClassification Loss: 1.2490\n","Train Epoch: 4 [24480/110534 (22%)]\tAll Loss: 1.6820\tTriple Loss(0): 0.0000\tClassification Loss: 1.6820\n","Train Epoch: 4 [24640/110534 (22%)]\tAll Loss: 2.6347\tTriple Loss(1): 0.4192\tClassification Loss: 1.7962\n","Train Epoch: 4 [24800/110534 (22%)]\tAll Loss: 2.9859\tTriple Loss(1): 0.7185\tClassification Loss: 1.5490\n","Train Epoch: 4 [24960/110534 (23%)]\tAll Loss: 1.6770\tTriple Loss(1): 0.0000\tClassification Loss: 1.6770\n","Train Epoch: 4 [25120/110534 (23%)]\tAll Loss: 1.2296\tTriple Loss(1): 0.0329\tClassification Loss: 1.1638\n","Train Epoch: 4 [25280/110534 (23%)]\tAll Loss: 2.2062\tTriple Loss(1): 0.4471\tClassification Loss: 1.3121\n","Train Epoch: 4 [25440/110534 (23%)]\tAll Loss: 1.3245\tTriple Loss(0): 0.0000\tClassification Loss: 1.3245\n","\n","Test set: Average loss: 1.5225, Accuracy: 281/480 (59%)\n","\n","Train Epoch: 4 [25600/110534 (23%)]\tAll Loss: 1.2562\tTriple Loss(0): 0.0000\tClassification Loss: 1.2562\n","Train Epoch: 4 [25760/110534 (23%)]\tAll Loss: 1.5622\tTriple Loss(1): 0.0145\tClassification Loss: 1.5333\n","Train Epoch: 4 [25920/110534 (23%)]\tAll Loss: 1.1957\tTriple Loss(1): 0.0706\tClassification Loss: 1.0545\n","Train Epoch: 4 [26080/110534 (24%)]\tAll Loss: 1.2675\tTriple Loss(0): 0.0000\tClassification Loss: 1.2675\n","Train Epoch: 4 [26240/110534 (24%)]\tAll Loss: 2.1858\tTriple Loss(1): 0.2566\tClassification Loss: 1.6726\n","Train Epoch: 4 [26400/110534 (24%)]\tAll Loss: 1.9788\tTriple Loss(1): 0.3099\tClassification Loss: 1.3589\n","Train Epoch: 4 [26560/110534 (24%)]\tAll Loss: 1.5884\tTriple Loss(1): 0.0000\tClassification Loss: 1.5884\n","Train Epoch: 4 [26720/110534 (24%)]\tAll Loss: 2.0199\tTriple Loss(1): 0.0802\tClassification Loss: 1.8596\n","Train Epoch: 4 [26880/110534 (24%)]\tAll Loss: 1.9806\tTriple Loss(1): 0.2884\tClassification Loss: 1.4038\n","Train Epoch: 4 [27040/110534 (24%)]\tAll Loss: 0.9906\tTriple Loss(0): 0.0000\tClassification Loss: 0.9906\n","\n","Test set: Average loss: 1.5336, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 4 [27200/110534 (25%)]\tAll Loss: 1.1788\tTriple Loss(1): 0.2015\tClassification Loss: 0.7757\n","Train Epoch: 4 [27360/110534 (25%)]\tAll Loss: 2.1888\tTriple Loss(1): 0.4322\tClassification Loss: 1.3243\n","Train Epoch: 4 [27520/110534 (25%)]\tAll Loss: 1.4270\tTriple Loss(1): 0.1754\tClassification Loss: 1.0763\n","Train Epoch: 4 [27680/110534 (25%)]\tAll Loss: 1.2948\tTriple Loss(0): 0.0000\tClassification Loss: 1.2948\n","Train Epoch: 4 [27840/110534 (25%)]\tAll Loss: 1.6468\tTriple Loss(1): 0.1429\tClassification Loss: 1.3609\n","Train Epoch: 4 [28000/110534 (25%)]\tAll Loss: 1.7692\tTriple Loss(1): 0.2363\tClassification Loss: 1.2965\n","Train Epoch: 4 [28160/110534 (25%)]\tAll Loss: 1.4027\tTriple Loss(1): 0.1387\tClassification Loss: 1.1252\n","Train Epoch: 4 [28320/110534 (26%)]\tAll Loss: 1.6539\tTriple Loss(1): 0.0000\tClassification Loss: 1.6539\n","Train Epoch: 4 [28480/110534 (26%)]\tAll Loss: 1.9838\tTriple Loss(1): 0.2120\tClassification Loss: 1.5597\n","Train Epoch: 4 [28640/110534 (26%)]\tAll Loss: 1.0693\tTriple Loss(1): 0.0000\tClassification Loss: 1.0693\n","\n","Test set: Average loss: 1.5187, Accuracy: 281/480 (59%)\n","\n","Train Epoch: 4 [28800/110534 (26%)]\tAll Loss: 1.1969\tTriple Loss(0): 0.0000\tClassification Loss: 1.1969\n","Train Epoch: 4 [28960/110534 (26%)]\tAll Loss: 1.2218\tTriple Loss(1): 0.0275\tClassification Loss: 1.1669\n","Train Epoch: 4 [29120/110534 (26%)]\tAll Loss: 1.6921\tTriple Loss(1): 0.0315\tClassification Loss: 1.6291\n","Train Epoch: 4 [29280/110534 (26%)]\tAll Loss: 1.6082\tTriple Loss(1): 0.0416\tClassification Loss: 1.5250\n","Train Epoch: 4 [29440/110534 (27%)]\tAll Loss: 2.0914\tTriple Loss(1): 0.1909\tClassification Loss: 1.7095\n","Train Epoch: 4 [29600/110534 (27%)]\tAll Loss: 1.9994\tTriple Loss(1): 0.2976\tClassification Loss: 1.4043\n","Train Epoch: 4 [29760/110534 (27%)]\tAll Loss: 2.1655\tTriple Loss(1): 0.2437\tClassification Loss: 1.6781\n","Train Epoch: 4 [29920/110534 (27%)]\tAll Loss: 2.6505\tTriple Loss(1): 0.2055\tClassification Loss: 2.2396\n","Train Epoch: 4 [30080/110534 (27%)]\tAll Loss: 1.7831\tTriple Loss(1): 0.1198\tClassification Loss: 1.5435\n","Train Epoch: 4 [30240/110534 (27%)]\tAll Loss: 1.5332\tTriple Loss(1): 0.1385\tClassification Loss: 1.2563\n","\n","Test set: Average loss: 1.5361, Accuracy: 272/480 (57%)\n","\n","Train Epoch: 4 [30400/110534 (28%)]\tAll Loss: 1.8902\tTriple Loss(1): 0.2213\tClassification Loss: 1.4476\n","Train Epoch: 4 [30560/110534 (28%)]\tAll Loss: 1.4736\tTriple Loss(1): 0.0349\tClassification Loss: 1.4038\n","Train Epoch: 4 [30720/110534 (28%)]\tAll Loss: 1.6517\tTriple Loss(1): 0.2241\tClassification Loss: 1.2036\n","Train Epoch: 4 [30880/110534 (28%)]\tAll Loss: 1.4624\tTriple Loss(1): 0.0830\tClassification Loss: 1.2963\n","Train Epoch: 4 [31040/110534 (28%)]\tAll Loss: 1.9675\tTriple Loss(1): 0.1136\tClassification Loss: 1.7403\n","Train Epoch: 4 [31200/110534 (28%)]\tAll Loss: 2.2525\tTriple Loss(1): 0.4168\tClassification Loss: 1.4189\n","Train Epoch: 4 [31360/110534 (28%)]\tAll Loss: 1.6425\tTriple Loss(1): 0.1359\tClassification Loss: 1.3707\n","Train Epoch: 4 [31520/110534 (29%)]\tAll Loss: 1.3176\tTriple Loss(1): 0.1422\tClassification Loss: 1.0331\n","Train Epoch: 4 [31680/110534 (29%)]\tAll Loss: 1.5156\tTriple Loss(1): 0.0370\tClassification Loss: 1.4416\n","Train Epoch: 4 [31840/110534 (29%)]\tAll Loss: 2.2043\tTriple Loss(1): 0.0520\tClassification Loss: 2.1002\n","\n","Test set: Average loss: 1.5260, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 4 [32000/110534 (29%)]\tAll Loss: 2.3683\tTriple Loss(1): 0.4417\tClassification Loss: 1.4850\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_2000.pth.tar\n","Train Epoch: 4 [32160/110534 (29%)]\tAll Loss: 2.7504\tTriple Loss(1): 0.4936\tClassification Loss: 1.7633\n","Train Epoch: 4 [32320/110534 (29%)]\tAll Loss: 2.3996\tTriple Loss(1): 0.2021\tClassification Loss: 1.9954\n","Train Epoch: 4 [32480/110534 (29%)]\tAll Loss: 2.0316\tTriple Loss(1): 0.2356\tClassification Loss: 1.5603\n","Train Epoch: 4 [32640/110534 (30%)]\tAll Loss: 2.3242\tTriple Loss(1): 0.3554\tClassification Loss: 1.6135\n","Train Epoch: 4 [32800/110534 (30%)]\tAll Loss: 2.2007\tTriple Loss(1): 0.2254\tClassification Loss: 1.7498\n","Train Epoch: 4 [32960/110534 (30%)]\tAll Loss: 1.2753\tTriple Loss(0): 0.0000\tClassification Loss: 1.2753\n","Train Epoch: 4 [33120/110534 (30%)]\tAll Loss: 0.9325\tTriple Loss(0): 0.0000\tClassification Loss: 0.9325\n","Train Epoch: 4 [33280/110534 (30%)]\tAll Loss: 1.2590\tTriple Loss(1): 0.0546\tClassification Loss: 1.1497\n","Train Epoch: 4 [33440/110534 (30%)]\tAll Loss: 1.9495\tTriple Loss(1): 0.0743\tClassification Loss: 1.8009\n","\n","Test set: Average loss: 1.5120, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 4 [33600/110534 (30%)]\tAll Loss: 1.7632\tTriple Loss(1): 0.0762\tClassification Loss: 1.6108\n","Train Epoch: 4 [33760/110534 (31%)]\tAll Loss: 1.4889\tTriple Loss(1): 0.0542\tClassification Loss: 1.3806\n","Train Epoch: 4 [33920/110534 (31%)]\tAll Loss: 1.8043\tTriple Loss(1): 0.0604\tClassification Loss: 1.6836\n","Train Epoch: 4 [34080/110534 (31%)]\tAll Loss: 1.6131\tTriple Loss(0): 0.0000\tClassification Loss: 1.6131\n","Train Epoch: 4 [34240/110534 (31%)]\tAll Loss: 1.8030\tTriple Loss(1): 0.1497\tClassification Loss: 1.5037\n","Train Epoch: 4 [34400/110534 (31%)]\tAll Loss: 1.4595\tTriple Loss(1): 0.2537\tClassification Loss: 0.9521\n","Train Epoch: 4 [34560/110534 (31%)]\tAll Loss: 1.8605\tTriple Loss(1): 0.1954\tClassification Loss: 1.4698\n","Train Epoch: 4 [34720/110534 (31%)]\tAll Loss: 2.4470\tTriple Loss(1): 0.5114\tClassification Loss: 1.4241\n","Train Epoch: 4 [34880/110534 (32%)]\tAll Loss: 1.8916\tTriple Loss(1): 0.3085\tClassification Loss: 1.2746\n","Train Epoch: 4 [35040/110534 (32%)]\tAll Loss: 2.0825\tTriple Loss(1): 0.1599\tClassification Loss: 1.7626\n","\n","Test set: Average loss: 1.5383, Accuracy: 277/480 (58%)\n","\n","Train Epoch: 4 [35200/110534 (32%)]\tAll Loss: 1.8781\tTriple Loss(0): 0.0000\tClassification Loss: 1.8781\n","Train Epoch: 4 [35360/110534 (32%)]\tAll Loss: 9.8550\tTriple Loss(0): 4.2446\tClassification Loss: 1.3659\n","Train Epoch: 4 [35520/110534 (32%)]\tAll Loss: 1.9265\tTriple Loss(1): 0.0507\tClassification Loss: 1.8252\n","Train Epoch: 4 [35680/110534 (32%)]\tAll Loss: 1.8211\tTriple Loss(0): 0.0000\tClassification Loss: 1.8211\n","Train Epoch: 4 [35840/110534 (32%)]\tAll Loss: 1.8711\tTriple Loss(1): 0.3307\tClassification Loss: 1.2097\n","Train Epoch: 4 [36000/110534 (33%)]\tAll Loss: 2.8729\tTriple Loss(1): 0.1197\tClassification Loss: 2.6336\n","Train Epoch: 4 [36160/110534 (33%)]\tAll Loss: 1.4612\tTriple Loss(1): 0.0000\tClassification Loss: 1.4612\n","Train Epoch: 4 [36320/110534 (33%)]\tAll Loss: 1.6870\tTriple Loss(0): 0.0000\tClassification Loss: 1.6870\n","Train Epoch: 4 [36480/110534 (33%)]\tAll Loss: 1.5124\tTriple Loss(1): 0.0150\tClassification Loss: 1.4825\n","Train Epoch: 4 [36640/110534 (33%)]\tAll Loss: 1.9406\tTriple Loss(1): 0.1396\tClassification Loss: 1.6615\n","\n","Test set: Average loss: 1.5354, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 4 [36800/110534 (33%)]\tAll Loss: 1.1680\tTriple Loss(0): 0.0000\tClassification Loss: 1.1680\n","Train Epoch: 4 [36960/110534 (33%)]\tAll Loss: 2.0583\tTriple Loss(1): 0.2099\tClassification Loss: 1.6386\n","Train Epoch: 4 [37120/110534 (34%)]\tAll Loss: 1.8824\tTriple Loss(0): 0.0000\tClassification Loss: 1.8824\n","Train Epoch: 4 [37280/110534 (34%)]\tAll Loss: 2.0292\tTriple Loss(1): 0.1533\tClassification Loss: 1.7225\n","Train Epoch: 4 [37440/110534 (34%)]\tAll Loss: 1.7658\tTriple Loss(0): 0.0000\tClassification Loss: 1.7658\n","Train Epoch: 4 [37600/110534 (34%)]\tAll Loss: 1.5410\tTriple Loss(1): 0.1424\tClassification Loss: 1.2561\n","Train Epoch: 4 [37760/110534 (34%)]\tAll Loss: 2.1512\tTriple Loss(1): 0.1782\tClassification Loss: 1.7947\n","Train Epoch: 4 [37920/110534 (34%)]\tAll Loss: 2.6190\tTriple Loss(1): 0.4949\tClassification Loss: 1.6292\n","Train Epoch: 4 [38080/110534 (34%)]\tAll Loss: 1.8382\tTriple Loss(1): 0.2498\tClassification Loss: 1.3386\n","Train Epoch: 4 [38240/110534 (35%)]\tAll Loss: 2.4218\tTriple Loss(1): 0.5618\tClassification Loss: 1.2982\n","\n","Test set: Average loss: 1.5180, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 4 [38400/110534 (35%)]\tAll Loss: 1.2989\tTriple Loss(1): 0.0461\tClassification Loss: 1.2068\n","Train Epoch: 4 [38560/110534 (35%)]\tAll Loss: 1.8874\tTriple Loss(0): 0.0000\tClassification Loss: 1.8874\n","Train Epoch: 4 [38720/110534 (35%)]\tAll Loss: 2.9659\tTriple Loss(1): 0.4015\tClassification Loss: 2.1630\n","Train Epoch: 4 [38880/110534 (35%)]\tAll Loss: 1.3327\tTriple Loss(1): 0.1080\tClassification Loss: 1.1167\n","Train Epoch: 4 [39040/110534 (35%)]\tAll Loss: 1.7821\tTriple Loss(1): 0.1041\tClassification Loss: 1.5739\n","Train Epoch: 4 [39200/110534 (35%)]\tAll Loss: 2.2182\tTriple Loss(1): 0.1388\tClassification Loss: 1.9407\n","Train Epoch: 4 [39360/110534 (36%)]\tAll Loss: 1.6999\tTriple Loss(1): 0.1533\tClassification Loss: 1.3933\n","Train Epoch: 4 [39520/110534 (36%)]\tAll Loss: 2.0624\tTriple Loss(0): 0.0000\tClassification Loss: 2.0624\n","Train Epoch: 4 [39680/110534 (36%)]\tAll Loss: 1.5835\tTriple Loss(1): 0.2258\tClassification Loss: 1.1319\n","Train Epoch: 4 [39840/110534 (36%)]\tAll Loss: 1.7508\tTriple Loss(1): 0.3494\tClassification Loss: 1.0521\n","\n","Test set: Average loss: 1.5264, Accuracy: 267/480 (56%)\n","\n","Train Epoch: 4 [40000/110534 (36%)]\tAll Loss: 1.2741\tTriple Loss(1): 0.0204\tClassification Loss: 1.2334\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_2500.pth.tar\n","Train Epoch: 4 [40160/110534 (36%)]\tAll Loss: 0.7771\tTriple Loss(0): 0.0000\tClassification Loss: 0.7771\n","Train Epoch: 4 [40320/110534 (36%)]\tAll Loss: 1.4916\tTriple Loss(1): 0.1766\tClassification Loss: 1.1383\n","Train Epoch: 4 [40480/110534 (37%)]\tAll Loss: 2.0882\tTriple Loss(1): 0.3049\tClassification Loss: 1.4785\n","Train Epoch: 4 [40640/110534 (37%)]\tAll Loss: 1.2779\tTriple Loss(0): 0.0000\tClassification Loss: 1.2779\n","Train Epoch: 4 [40800/110534 (37%)]\tAll Loss: 1.5426\tTriple Loss(0): 0.0000\tClassification Loss: 1.5426\n","Train Epoch: 4 [40960/110534 (37%)]\tAll Loss: 1.6055\tTriple Loss(0): 0.0000\tClassification Loss: 1.6055\n","Train Epoch: 4 [41120/110534 (37%)]\tAll Loss: 1.5607\tTriple Loss(0): 0.0000\tClassification Loss: 1.5607\n","Train Epoch: 4 [41280/110534 (37%)]\tAll Loss: 2.7167\tTriple Loss(1): 0.5048\tClassification Loss: 1.7072\n","Train Epoch: 4 [41440/110534 (37%)]\tAll Loss: 1.4347\tTriple Loss(1): 0.0732\tClassification Loss: 1.2883\n","\n","Test set: Average loss: 1.5429, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 4 [41600/110534 (38%)]\tAll Loss: 2.0194\tTriple Loss(1): 0.0479\tClassification Loss: 1.9235\n","Train Epoch: 4 [41760/110534 (38%)]\tAll Loss: 0.9333\tTriple Loss(1): 0.0504\tClassification Loss: 0.8325\n","Train Epoch: 4 [41920/110534 (38%)]\tAll Loss: 1.7652\tTriple Loss(1): 0.2336\tClassification Loss: 1.2980\n","Train Epoch: 4 [42080/110534 (38%)]\tAll Loss: 1.5154\tTriple Loss(0): 0.0000\tClassification Loss: 1.5154\n","Train Epoch: 4 [42240/110534 (38%)]\tAll Loss: 1.8239\tTriple Loss(1): 0.3193\tClassification Loss: 1.1854\n","Train Epoch: 4 [42400/110534 (38%)]\tAll Loss: 2.4598\tTriple Loss(1): 0.2587\tClassification Loss: 1.9424\n","Train Epoch: 4 [42560/110534 (39%)]\tAll Loss: 2.1424\tTriple Loss(1): 0.1846\tClassification Loss: 1.7733\n","Train Epoch: 4 [42720/110534 (39%)]\tAll Loss: 3.4649\tTriple Loss(1): 0.8646\tClassification Loss: 1.7358\n","Train Epoch: 4 [42880/110534 (39%)]\tAll Loss: 1.5410\tTriple Loss(1): 0.3039\tClassification Loss: 0.9332\n","Train Epoch: 4 [43040/110534 (39%)]\tAll Loss: 1.6348\tTriple Loss(0): 0.0000\tClassification Loss: 1.6348\n","\n","Test set: Average loss: 1.5368, Accuracy: 277/480 (58%)\n","\n","Train Epoch: 4 [43200/110534 (39%)]\tAll Loss: 2.6348\tTriple Loss(1): 0.3382\tClassification Loss: 1.9584\n","Train Epoch: 4 [43360/110534 (39%)]\tAll Loss: 1.1373\tTriple Loss(0): 0.0000\tClassification Loss: 1.1373\n","Train Epoch: 4 [43520/110534 (39%)]\tAll Loss: 1.5789\tTriple Loss(0): 0.0000\tClassification Loss: 1.5789\n","Train Epoch: 4 [43680/110534 (40%)]\tAll Loss: 1.5644\tTriple Loss(0): 0.0000\tClassification Loss: 1.5644\n","Train Epoch: 4 [43840/110534 (40%)]\tAll Loss: 1.9686\tTriple Loss(1): 0.1517\tClassification Loss: 1.6652\n","Train Epoch: 4 [44000/110534 (40%)]\tAll Loss: 1.5455\tTriple Loss(1): 0.0786\tClassification Loss: 1.3883\n","Train Epoch: 4 [44160/110534 (40%)]\tAll Loss: 2.4966\tTriple Loss(1): 0.2755\tClassification Loss: 1.9455\n","Train Epoch: 4 [44320/110534 (40%)]\tAll Loss: 1.2800\tTriple Loss(0): 0.0000\tClassification Loss: 1.2800\n","Train Epoch: 4 [44480/110534 (40%)]\tAll Loss: 2.1984\tTriple Loss(1): 0.4061\tClassification Loss: 1.3863\n","Train Epoch: 4 [44640/110534 (40%)]\tAll Loss: 1.7943\tTriple Loss(1): 0.1246\tClassification Loss: 1.5452\n","\n","Test set: Average loss: 1.5351, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 4 [44800/110534 (41%)]\tAll Loss: 1.7083\tTriple Loss(1): 0.0678\tClassification Loss: 1.5728\n","Train Epoch: 4 [44960/110534 (41%)]\tAll Loss: 2.7675\tTriple Loss(1): 0.5013\tClassification Loss: 1.7649\n","Train Epoch: 4 [45120/110534 (41%)]\tAll Loss: 1.7263\tTriple Loss(1): 0.2508\tClassification Loss: 1.2248\n","Train Epoch: 4 [45280/110534 (41%)]\tAll Loss: 1.9876\tTriple Loss(1): 0.2774\tClassification Loss: 1.4327\n","Train Epoch: 4 [45440/110534 (41%)]\tAll Loss: 1.5169\tTriple Loss(1): 0.0333\tClassification Loss: 1.4504\n","Train Epoch: 4 [45600/110534 (41%)]\tAll Loss: 1.9485\tTriple Loss(1): 0.2466\tClassification Loss: 1.4553\n","Train Epoch: 4 [45760/110534 (41%)]\tAll Loss: 2.0861\tTriple Loss(0): 0.0000\tClassification Loss: 2.0861\n","Train Epoch: 4 [45920/110534 (42%)]\tAll Loss: 1.5409\tTriple Loss(1): 0.1407\tClassification Loss: 1.2595\n","Train Epoch: 4 [46080/110534 (42%)]\tAll Loss: 1.7189\tTriple Loss(1): 0.2549\tClassification Loss: 1.2092\n","Train Epoch: 4 [46240/110534 (42%)]\tAll Loss: 1.6914\tTriple Loss(0): 0.0000\tClassification Loss: 1.6914\n","\n","Test set: Average loss: 1.5459, Accuracy: 268/480 (56%)\n","\n","Train Epoch: 4 [46400/110534 (42%)]\tAll Loss: 1.2750\tTriple Loss(0): 0.0000\tClassification Loss: 1.2750\n","Train Epoch: 4 [46560/110534 (42%)]\tAll Loss: 1.7732\tTriple Loss(1): 0.0083\tClassification Loss: 1.7565\n","Train Epoch: 4 [46720/110534 (42%)]\tAll Loss: 2.8111\tTriple Loss(1): 0.5566\tClassification Loss: 1.6978\n","Train Epoch: 4 [46880/110534 (42%)]\tAll Loss: 1.3857\tTriple Loss(1): 0.0231\tClassification Loss: 1.3396\n","Train Epoch: 4 [47040/110534 (43%)]\tAll Loss: 1.7265\tTriple Loss(1): 0.1767\tClassification Loss: 1.3732\n","Train Epoch: 4 [47200/110534 (43%)]\tAll Loss: 2.3978\tTriple Loss(1): 0.2836\tClassification Loss: 1.8307\n","Train Epoch: 4 [47360/110534 (43%)]\tAll Loss: 2.2451\tTriple Loss(0): 0.0000\tClassification Loss: 2.2451\n","Train Epoch: 4 [47520/110534 (43%)]\tAll Loss: 2.3122\tTriple Loss(1): 0.1182\tClassification Loss: 2.0758\n","Train Epoch: 4 [47680/110534 (43%)]\tAll Loss: 1.3427\tTriple Loss(1): 0.1332\tClassification Loss: 1.0763\n","Train Epoch: 4 [47840/110534 (43%)]\tAll Loss: 1.6548\tTriple Loss(1): 0.1120\tClassification Loss: 1.4308\n","\n","Test set: Average loss: 1.5235, Accuracy: 275/480 (57%)\n","\n","Train Epoch: 4 [48000/110534 (43%)]\tAll Loss: 1.6725\tTriple Loss(1): 0.0612\tClassification Loss: 1.5501\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_3000.pth.tar\n","Train Epoch: 4 [48160/110534 (44%)]\tAll Loss: 1.7961\tTriple Loss(1): 0.2840\tClassification Loss: 1.2280\n","Train Epoch: 4 [48320/110534 (44%)]\tAll Loss: 1.3367\tTriple Loss(0): 0.0000\tClassification Loss: 1.3367\n","Train Epoch: 4 [48480/110534 (44%)]\tAll Loss: 1.8393\tTriple Loss(1): 0.0075\tClassification Loss: 1.8243\n","Train Epoch: 4 [48640/110534 (44%)]\tAll Loss: 2.3534\tTriple Loss(1): 0.2990\tClassification Loss: 1.7554\n","Train Epoch: 4 [48800/110534 (44%)]\tAll Loss: 2.3564\tTriple Loss(1): 0.2486\tClassification Loss: 1.8592\n","Train Epoch: 4 [48960/110534 (44%)]\tAll Loss: 1.3066\tTriple Loss(1): 0.0000\tClassification Loss: 1.3066\n","Train Epoch: 4 [49120/110534 (44%)]\tAll Loss: 1.4095\tTriple Loss(1): 0.0704\tClassification Loss: 1.2686\n","Train Epoch: 4 [49280/110534 (45%)]\tAll Loss: 2.0578\tTriple Loss(1): 0.1697\tClassification Loss: 1.7183\n","Train Epoch: 4 [49440/110534 (45%)]\tAll Loss: 1.7499\tTriple Loss(0): 0.0000\tClassification Loss: 1.7499\n","\n","Test set: Average loss: 1.5036, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 4 [49600/110534 (45%)]\tAll Loss: 7.3336\tTriple Loss(0): 2.9772\tClassification Loss: 1.3792\n","Train Epoch: 4 [49760/110534 (45%)]\tAll Loss: 1.8421\tTriple Loss(0): 0.0000\tClassification Loss: 1.8421\n","Train Epoch: 4 [49920/110534 (45%)]\tAll Loss: 0.9543\tTriple Loss(1): 0.0741\tClassification Loss: 0.8061\n","Train Epoch: 4 [50080/110534 (45%)]\tAll Loss: 1.8456\tTriple Loss(1): 0.0763\tClassification Loss: 1.6931\n","Train Epoch: 4 [50240/110534 (45%)]\tAll Loss: 2.2064\tTriple Loss(1): 0.3022\tClassification Loss: 1.6019\n","Train Epoch: 4 [50400/110534 (46%)]\tAll Loss: 1.7418\tTriple Loss(1): 0.1049\tClassification Loss: 1.5321\n","Train Epoch: 4 [50560/110534 (46%)]\tAll Loss: 1.5842\tTriple Loss(1): 0.0526\tClassification Loss: 1.4790\n","Train Epoch: 4 [50720/110534 (46%)]\tAll Loss: 1.9124\tTriple Loss(0): 0.0000\tClassification Loss: 1.9124\n","Train Epoch: 4 [50880/110534 (46%)]\tAll Loss: 1.2903\tTriple Loss(1): 0.0276\tClassification Loss: 1.2351\n","Train Epoch: 4 [51040/110534 (46%)]\tAll Loss: 1.9713\tTriple Loss(1): 0.1718\tClassification Loss: 1.6278\n","\n","Test set: Average loss: 1.5731, Accuracy: 268/480 (56%)\n","\n","Train Epoch: 4 [51200/110534 (46%)]\tAll Loss: 2.0780\tTriple Loss(1): 0.3737\tClassification Loss: 1.3305\n","Train Epoch: 4 [51360/110534 (46%)]\tAll Loss: 2.2994\tTriple Loss(1): 0.2797\tClassification Loss: 1.7400\n","Train Epoch: 4 [51520/110534 (47%)]\tAll Loss: 2.0201\tTriple Loss(1): 0.2339\tClassification Loss: 1.5523\n","Train Epoch: 4 [51680/110534 (47%)]\tAll Loss: 2.3865\tTriple Loss(1): 0.0000\tClassification Loss: 2.3865\n","Train Epoch: 4 [51840/110534 (47%)]\tAll Loss: 2.4241\tTriple Loss(1): 0.3174\tClassification Loss: 1.7893\n","Train Epoch: 4 [52000/110534 (47%)]\tAll Loss: 2.9753\tTriple Loss(1): 0.4900\tClassification Loss: 1.9953\n","Train Epoch: 4 [52160/110534 (47%)]\tAll Loss: 2.4719\tTriple Loss(1): 0.0813\tClassification Loss: 2.3092\n","Train Epoch: 4 [52320/110534 (47%)]\tAll Loss: 1.1627\tTriple Loss(1): 0.0581\tClassification Loss: 1.0466\n","Train Epoch: 4 [52480/110534 (47%)]\tAll Loss: 1.5273\tTriple Loss(1): 0.0000\tClassification Loss: 1.5273\n","Train Epoch: 4 [52640/110534 (48%)]\tAll Loss: 2.6481\tTriple Loss(1): 0.3969\tClassification Loss: 1.8543\n","\n","Test set: Average loss: 1.5516, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 4 [52800/110534 (48%)]\tAll Loss: 2.3081\tTriple Loss(1): 0.2437\tClassification Loss: 1.8207\n","Train Epoch: 4 [52960/110534 (48%)]\tAll Loss: 1.4981\tTriple Loss(1): 0.0749\tClassification Loss: 1.3484\n","Train Epoch: 4 [53120/110534 (48%)]\tAll Loss: 1.5954\tTriple Loss(1): 0.0368\tClassification Loss: 1.5218\n","Train Epoch: 4 [53280/110534 (48%)]\tAll Loss: 0.8352\tTriple Loss(0): 0.0000\tClassification Loss: 0.8352\n","Train Epoch: 4 [53440/110534 (48%)]\tAll Loss: 1.7555\tTriple Loss(0): 0.0000\tClassification Loss: 1.7555\n","Train Epoch: 4 [53600/110534 (48%)]\tAll Loss: 1.5354\tTriple Loss(0): 0.0000\tClassification Loss: 1.5354\n","Train Epoch: 4 [53760/110534 (49%)]\tAll Loss: 1.6904\tTriple Loss(0): 0.0000\tClassification Loss: 1.6904\n","Train Epoch: 4 [53920/110534 (49%)]\tAll Loss: 2.7211\tTriple Loss(1): 0.4007\tClassification Loss: 1.9197\n","Train Epoch: 4 [54080/110534 (49%)]\tAll Loss: 1.7804\tTriple Loss(1): 0.1329\tClassification Loss: 1.5146\n","Train Epoch: 4 [54240/110534 (49%)]\tAll Loss: 1.6683\tTriple Loss(1): 0.2040\tClassification Loss: 1.2604\n","\n","Test set: Average loss: 1.5624, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 4 [54400/110534 (49%)]\tAll Loss: 2.0266\tTriple Loss(1): 0.0916\tClassification Loss: 1.8433\n","Train Epoch: 4 [54560/110534 (49%)]\tAll Loss: 1.6649\tTriple Loss(0): 0.0000\tClassification Loss: 1.6649\n","Train Epoch: 4 [54720/110534 (50%)]\tAll Loss: 1.3548\tTriple Loss(1): 0.0526\tClassification Loss: 1.2496\n","Train Epoch: 4 [54880/110534 (50%)]\tAll Loss: 2.0418\tTriple Loss(1): 0.0062\tClassification Loss: 2.0295\n","Train Epoch: 4 [55040/110534 (50%)]\tAll Loss: 1.4676\tTriple Loss(0): 0.0000\tClassification Loss: 1.4676\n","Train Epoch: 4 [55200/110534 (50%)]\tAll Loss: 1.4521\tTriple Loss(0): 0.0000\tClassification Loss: 1.4521\n","Train Epoch: 4 [55360/110534 (50%)]\tAll Loss: 2.0492\tTriple Loss(1): 0.4756\tClassification Loss: 1.0980\n","Train Epoch: 4 [55520/110534 (50%)]\tAll Loss: 2.3755\tTriple Loss(1): 0.3205\tClassification Loss: 1.7345\n","Train Epoch: 4 [55680/110534 (50%)]\tAll Loss: 1.3247\tTriple Loss(0): 0.0000\tClassification Loss: 1.3247\n","Train Epoch: 4 [55840/110534 (51%)]\tAll Loss: 2.2736\tTriple Loss(1): 0.2004\tClassification Loss: 1.8728\n","\n","Test set: Average loss: 1.5281, Accuracy: 284/480 (59%)\n","\n","Train Epoch: 4 [56000/110534 (51%)]\tAll Loss: 1.5644\tTriple Loss(1): 0.0042\tClassification Loss: 1.5560\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_3500.pth.tar\n","Train Epoch: 4 [56160/110534 (51%)]\tAll Loss: 1.4612\tTriple Loss(0): 0.0000\tClassification Loss: 1.4612\n","Train Epoch: 4 [56320/110534 (51%)]\tAll Loss: 1.3221\tTriple Loss(1): 0.0909\tClassification Loss: 1.1402\n","Train Epoch: 4 [56480/110534 (51%)]\tAll Loss: 2.3402\tTriple Loss(1): 0.4398\tClassification Loss: 1.4606\n","Train Epoch: 4 [56640/110534 (51%)]\tAll Loss: 1.8453\tTriple Loss(1): 0.1402\tClassification Loss: 1.5649\n","Train Epoch: 4 [56800/110534 (51%)]\tAll Loss: 5.4132\tTriple Loss(0): 1.9538\tClassification Loss: 1.5056\n","Train Epoch: 4 [56960/110534 (52%)]\tAll Loss: 1.9650\tTriple Loss(0): 0.0000\tClassification Loss: 1.9650\n","Train Epoch: 4 [57120/110534 (52%)]\tAll Loss: 1.3744\tTriple Loss(0): 0.0000\tClassification Loss: 1.3744\n","Train Epoch: 4 [57280/110534 (52%)]\tAll Loss: 1.6240\tTriple Loss(1): 0.0000\tClassification Loss: 1.6240\n","Train Epoch: 4 [57440/110534 (52%)]\tAll Loss: 1.2395\tTriple Loss(1): 0.0847\tClassification Loss: 1.0701\n","\n","Test set: Average loss: 1.5558, Accuracy: 261/480 (54%)\n","\n","Train Epoch: 4 [57600/110534 (52%)]\tAll Loss: 1.4707\tTriple Loss(1): 0.1114\tClassification Loss: 1.2479\n","Train Epoch: 4 [57760/110534 (52%)]\tAll Loss: 2.2353\tTriple Loss(1): 0.4588\tClassification Loss: 1.3177\n","Train Epoch: 4 [57920/110534 (52%)]\tAll Loss: 1.5271\tTriple Loss(1): 0.0272\tClassification Loss: 1.4726\n","Train Epoch: 4 [58080/110534 (53%)]\tAll Loss: 1.4827\tTriple Loss(0): 0.0000\tClassification Loss: 1.4827\n","Train Epoch: 4 [58240/110534 (53%)]\tAll Loss: 1.6350\tTriple Loss(1): 0.0147\tClassification Loss: 1.6056\n","Train Epoch: 4 [58400/110534 (53%)]\tAll Loss: 1.5997\tTriple Loss(1): 0.1503\tClassification Loss: 1.2991\n","Train Epoch: 4 [58560/110534 (53%)]\tAll Loss: 2.3093\tTriple Loss(1): 0.0732\tClassification Loss: 2.1629\n","Train Epoch: 4 [58720/110534 (53%)]\tAll Loss: 1.8939\tTriple Loss(1): 0.2422\tClassification Loss: 1.4094\n","Train Epoch: 4 [58880/110534 (53%)]\tAll Loss: 1.7570\tTriple Loss(1): 0.1599\tClassification Loss: 1.4372\n","Train Epoch: 4 [59040/110534 (53%)]\tAll Loss: 1.7423\tTriple Loss(1): 0.1622\tClassification Loss: 1.4179\n","\n","Test set: Average loss: 1.5083, Accuracy: 275/480 (57%)\n","\n","Train Epoch: 4 [59200/110534 (54%)]\tAll Loss: 1.4370\tTriple Loss(1): 0.0000\tClassification Loss: 1.4370\n","Train Epoch: 4 [59360/110534 (54%)]\tAll Loss: 3.0042\tTriple Loss(0): 0.9402\tClassification Loss: 1.1237\n","Train Epoch: 4 [59520/110534 (54%)]\tAll Loss: 2.4114\tTriple Loss(1): 0.1892\tClassification Loss: 2.0329\n","Train Epoch: 4 [59680/110534 (54%)]\tAll Loss: 1.5817\tTriple Loss(1): 0.0304\tClassification Loss: 1.5209\n","Train Epoch: 4 [59840/110534 (54%)]\tAll Loss: 2.6053\tTriple Loss(1): 0.2289\tClassification Loss: 2.1474\n","Train Epoch: 4 [60000/110534 (54%)]\tAll Loss: 2.1871\tTriple Loss(1): 0.1399\tClassification Loss: 1.9073\n","Train Epoch: 4 [60160/110534 (54%)]\tAll Loss: 2.0629\tTriple Loss(1): 0.1582\tClassification Loss: 1.7465\n","Train Epoch: 4 [60320/110534 (55%)]\tAll Loss: 2.1399\tTriple Loss(1): 0.1798\tClassification Loss: 1.7803\n","Train Epoch: 4 [60480/110534 (55%)]\tAll Loss: 1.2932\tTriple Loss(1): 0.1037\tClassification Loss: 1.0859\n","Train Epoch: 4 [60640/110534 (55%)]\tAll Loss: 1.2205\tTriple Loss(0): 0.0000\tClassification Loss: 1.2205\n","\n","Test set: Average loss: 1.4980, Accuracy: 285/480 (59%)\n","\n","Train Epoch: 4 [60800/110534 (55%)]\tAll Loss: 1.3345\tTriple Loss(1): 0.2104\tClassification Loss: 0.9136\n","Train Epoch: 4 [60960/110534 (55%)]\tAll Loss: 1.2647\tTriple Loss(1): 0.0632\tClassification Loss: 1.1383\n","Train Epoch: 4 [61120/110534 (55%)]\tAll Loss: 1.3144\tTriple Loss(0): 0.0000\tClassification Loss: 1.3144\n","Train Epoch: 4 [61280/110534 (55%)]\tAll Loss: 2.4456\tTriple Loss(1): 0.6153\tClassification Loss: 1.2150\n","Train Epoch: 4 [61440/110534 (56%)]\tAll Loss: 1.8778\tTriple Loss(1): 0.0864\tClassification Loss: 1.7050\n","Train Epoch: 4 [61600/110534 (56%)]\tAll Loss: 2.2285\tTriple Loss(1): 0.0185\tClassification Loss: 2.1916\n","Train Epoch: 4 [61760/110534 (56%)]\tAll Loss: 1.6543\tTriple Loss(1): 0.1009\tClassification Loss: 1.4525\n","Train Epoch: 4 [61920/110534 (56%)]\tAll Loss: 2.2204\tTriple Loss(1): 0.2657\tClassification Loss: 1.6891\n","Train Epoch: 4 [62080/110534 (56%)]\tAll Loss: 2.2604\tTriple Loss(1): 0.2275\tClassification Loss: 1.8055\n","Train Epoch: 4 [62240/110534 (56%)]\tAll Loss: 1.5329\tTriple Loss(1): 0.1778\tClassification Loss: 1.1773\n","\n","Test set: Average loss: 1.5006, Accuracy: 280/480 (58%)\n","\n","Train Epoch: 4 [62400/110534 (56%)]\tAll Loss: 1.2898\tTriple Loss(1): 0.0000\tClassification Loss: 1.2898\n","Train Epoch: 4 [62560/110534 (57%)]\tAll Loss: 1.9100\tTriple Loss(1): 0.1023\tClassification Loss: 1.7055\n","Train Epoch: 4 [62720/110534 (57%)]\tAll Loss: 1.7102\tTriple Loss(1): 0.1946\tClassification Loss: 1.3210\n","Train Epoch: 4 [62880/110534 (57%)]\tAll Loss: 1.1451\tTriple Loss(0): 0.0000\tClassification Loss: 1.1451\n","Train Epoch: 4 [63040/110534 (57%)]\tAll Loss: 1.7629\tTriple Loss(1): 0.1597\tClassification Loss: 1.4435\n","Train Epoch: 4 [63200/110534 (57%)]\tAll Loss: 1.7088\tTriple Loss(1): 0.0782\tClassification Loss: 1.5524\n","Train Epoch: 4 [63360/110534 (57%)]\tAll Loss: 2.1447\tTriple Loss(1): 0.0566\tClassification Loss: 2.0314\n","Train Epoch: 4 [63520/110534 (57%)]\tAll Loss: 2.4323\tTriple Loss(1): 0.3887\tClassification Loss: 1.6550\n","Train Epoch: 4 [63680/110534 (58%)]\tAll Loss: 1.4463\tTriple Loss(0): 0.0000\tClassification Loss: 1.4463\n","Train Epoch: 4 [63840/110534 (58%)]\tAll Loss: 1.6107\tTriple Loss(0): 0.0000\tClassification Loss: 1.6107\n","\n","Test set: Average loss: 1.5363, Accuracy: 275/480 (57%)\n","\n","Train Epoch: 4 [64000/110534 (58%)]\tAll Loss: 1.4538\tTriple Loss(0): 0.0000\tClassification Loss: 1.4538\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_4000.pth.tar\n","Train Epoch: 4 [64160/110534 (58%)]\tAll Loss: 2.1101\tTriple Loss(1): 0.0623\tClassification Loss: 1.9854\n","Train Epoch: 4 [64320/110534 (58%)]\tAll Loss: 0.9218\tTriple Loss(0): 0.0000\tClassification Loss: 0.9218\n","Train Epoch: 4 [64480/110534 (58%)]\tAll Loss: 2.2090\tTriple Loss(1): 0.2802\tClassification Loss: 1.6485\n","Train Epoch: 4 [64640/110534 (58%)]\tAll Loss: 1.9824\tTriple Loss(1): 0.1695\tClassification Loss: 1.6435\n","Train Epoch: 4 [64800/110534 (59%)]\tAll Loss: 1.3778\tTriple Loss(0): 0.0000\tClassification Loss: 1.3778\n","Train Epoch: 4 [64960/110534 (59%)]\tAll Loss: 1.3947\tTriple Loss(1): 0.0000\tClassification Loss: 1.3947\n","Train Epoch: 4 [65120/110534 (59%)]\tAll Loss: 3.1455\tTriple Loss(1): 0.5872\tClassification Loss: 1.9710\n","Train Epoch: 4 [65280/110534 (59%)]\tAll Loss: 1.6415\tTriple Loss(1): 0.0039\tClassification Loss: 1.6337\n","Train Epoch: 4 [65440/110534 (59%)]\tAll Loss: 1.2613\tTriple Loss(1): 0.0955\tClassification Loss: 1.0703\n","\n","Test set: Average loss: 1.5125, Accuracy: 278/480 (58%)\n","\n","Train Epoch: 4 [65600/110534 (59%)]\tAll Loss: 1.7576\tTriple Loss(1): 0.2036\tClassification Loss: 1.3504\n","Train Epoch: 4 [65760/110534 (59%)]\tAll Loss: 2.0317\tTriple Loss(1): 0.1722\tClassification Loss: 1.6873\n","Train Epoch: 4 [65920/110534 (60%)]\tAll Loss: 1.3964\tTriple Loss(0): 0.0000\tClassification Loss: 1.3964\n","Train Epoch: 4 [66080/110534 (60%)]\tAll Loss: 3.4172\tTriple Loss(0): 1.1108\tClassification Loss: 1.1956\n","Train Epoch: 4 [66240/110534 (60%)]\tAll Loss: 1.2695\tTriple Loss(0): 0.0000\tClassification Loss: 1.2695\n","Train Epoch: 4 [66400/110534 (60%)]\tAll Loss: 1.9841\tTriple Loss(1): 0.1999\tClassification Loss: 1.5843\n","Train Epoch: 4 [66560/110534 (60%)]\tAll Loss: 1.4876\tTriple Loss(1): 0.1081\tClassification Loss: 1.2713\n","Train Epoch: 4 [66720/110534 (60%)]\tAll Loss: 2.1134\tTriple Loss(1): 0.2409\tClassification Loss: 1.6317\n","Train Epoch: 4 [66880/110534 (61%)]\tAll Loss: 1.8361\tTriple Loss(1): 0.0000\tClassification Loss: 1.8361\n","Train Epoch: 4 [67040/110534 (61%)]\tAll Loss: 2.1526\tTriple Loss(0): 0.0000\tClassification Loss: 2.1526\n","\n","Test set: Average loss: 1.4980, Accuracy: 288/480 (60%)\n","\n","Train Epoch: 4 [67200/110534 (61%)]\tAll Loss: 1.8238\tTriple Loss(1): 0.1596\tClassification Loss: 1.5046\n","Train Epoch: 4 [67360/110534 (61%)]\tAll Loss: 1.7248\tTriple Loss(1): 0.3555\tClassification Loss: 1.0138\n","Train Epoch: 4 [67520/110534 (61%)]\tAll Loss: 1.8669\tTriple Loss(1): 0.1980\tClassification Loss: 1.4710\n","Train Epoch: 4 [67680/110534 (61%)]\tAll Loss: 0.9980\tTriple Loss(1): 0.0086\tClassification Loss: 0.9807\n","Train Epoch: 4 [67840/110534 (61%)]\tAll Loss: 1.9372\tTriple Loss(1): 0.1832\tClassification Loss: 1.5708\n","Train Epoch: 4 [68000/110534 (62%)]\tAll Loss: 1.6971\tTriple Loss(1): 0.0149\tClassification Loss: 1.6674\n","Train Epoch: 4 [68160/110534 (62%)]\tAll Loss: 2.0101\tTriple Loss(1): 0.0000\tClassification Loss: 2.0101\n","Train Epoch: 4 [68320/110534 (62%)]\tAll Loss: 1.2325\tTriple Loss(1): 0.1542\tClassification Loss: 0.9241\n","Train Epoch: 4 [68480/110534 (62%)]\tAll Loss: 2.0640\tTriple Loss(1): 0.2627\tClassification Loss: 1.5386\n","Train Epoch: 4 [68640/110534 (62%)]\tAll Loss: 1.5100\tTriple Loss(0): 0.0000\tClassification Loss: 1.5100\n","\n","Test set: Average loss: 1.5174, Accuracy: 289/480 (60%)\n","\n","Train Epoch: 4 [68800/110534 (62%)]\tAll Loss: 2.4102\tTriple Loss(1): 0.1634\tClassification Loss: 2.0834\n","Train Epoch: 4 [68960/110534 (62%)]\tAll Loss: 1.7220\tTriple Loss(1): 0.0657\tClassification Loss: 1.5906\n","Train Epoch: 4 [69120/110534 (63%)]\tAll Loss: 2.1285\tTriple Loss(1): 0.3572\tClassification Loss: 1.4140\n","Train Epoch: 4 [69280/110534 (63%)]\tAll Loss: 1.7056\tTriple Loss(1): 0.0000\tClassification Loss: 1.7056\n","Train Epoch: 4 [69440/110534 (63%)]\tAll Loss: 2.3374\tTriple Loss(1): 0.3070\tClassification Loss: 1.7235\n","Train Epoch: 4 [69600/110534 (63%)]\tAll Loss: 1.3411\tTriple Loss(0): 0.0000\tClassification Loss: 1.3411\n","Train Epoch: 4 [69760/110534 (63%)]\tAll Loss: 2.0214\tTriple Loss(1): 0.3715\tClassification Loss: 1.2784\n","Train Epoch: 4 [69920/110534 (63%)]\tAll Loss: 1.8322\tTriple Loss(0): 0.0000\tClassification Loss: 1.8322\n","Train Epoch: 4 [70080/110534 (63%)]\tAll Loss: 2.0737\tTriple Loss(1): 0.0544\tClassification Loss: 1.9648\n","Train Epoch: 4 [70240/110534 (64%)]\tAll Loss: 2.1953\tTriple Loss(1): 0.0776\tClassification Loss: 2.0401\n","\n","Test set: Average loss: 1.5087, Accuracy: 283/480 (59%)\n","\n","Train Epoch: 4 [70400/110534 (64%)]\tAll Loss: 2.1481\tTriple Loss(1): 0.0498\tClassification Loss: 2.0484\n","Train Epoch: 4 [70560/110534 (64%)]\tAll Loss: 2.0496\tTriple Loss(1): 0.0698\tClassification Loss: 1.9100\n","Train Epoch: 4 [70720/110534 (64%)]\tAll Loss: 1.7453\tTriple Loss(1): 0.2608\tClassification Loss: 1.2237\n","Train Epoch: 4 [70880/110534 (64%)]\tAll Loss: 1.6564\tTriple Loss(1): 0.0222\tClassification Loss: 1.6120\n","Train Epoch: 4 [71040/110534 (64%)]\tAll Loss: 1.0855\tTriple Loss(0): 0.0000\tClassification Loss: 1.0855\n","Train Epoch: 4 [71200/110534 (64%)]\tAll Loss: 1.2091\tTriple Loss(1): 0.0405\tClassification Loss: 1.1281\n","Train Epoch: 4 [71360/110534 (65%)]\tAll Loss: 1.3632\tTriple Loss(1): 0.0739\tClassification Loss: 1.2155\n","Train Epoch: 4 [71520/110534 (65%)]\tAll Loss: 1.8162\tTriple Loss(1): 0.0887\tClassification Loss: 1.6387\n","Train Epoch: 4 [71680/110534 (65%)]\tAll Loss: 1.2150\tTriple Loss(0): 0.0000\tClassification Loss: 1.2150\n","Train Epoch: 4 [71840/110534 (65%)]\tAll Loss: 2.6007\tTriple Loss(1): 0.1835\tClassification Loss: 2.2336\n","\n","Test set: Average loss: 1.5274, Accuracy: 282/480 (59%)\n","\n","Train Epoch: 4 [72000/110534 (65%)]\tAll Loss: 1.6157\tTriple Loss(0): 0.0000\tClassification Loss: 1.6157\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_4500.pth.tar\n","Train Epoch: 4 [72160/110534 (65%)]\tAll Loss: 2.1582\tTriple Loss(1): 0.2122\tClassification Loss: 1.7338\n","Train Epoch: 4 [72320/110534 (65%)]\tAll Loss: 2.8466\tTriple Loss(1): 0.4865\tClassification Loss: 1.8736\n","Train Epoch: 4 [72480/110534 (66%)]\tAll Loss: 2.9529\tTriple Loss(1): 0.5185\tClassification Loss: 1.9159\n","Train Epoch: 4 [72640/110534 (66%)]\tAll Loss: 1.7026\tTriple Loss(1): 0.0000\tClassification Loss: 1.7026\n","Train Epoch: 4 [72800/110534 (66%)]\tAll Loss: 1.4307\tTriple Loss(0): 0.0000\tClassification Loss: 1.4307\n","Train Epoch: 4 [72960/110534 (66%)]\tAll Loss: 1.0422\tTriple Loss(0): 0.0000\tClassification Loss: 1.0422\n","Train Epoch: 4 [73120/110534 (66%)]\tAll Loss: 1.5882\tTriple Loss(1): 0.1269\tClassification Loss: 1.3345\n","Train Epoch: 4 [73280/110534 (66%)]\tAll Loss: 1.9848\tTriple Loss(1): 0.1975\tClassification Loss: 1.5897\n","Train Epoch: 4 [73440/110534 (66%)]\tAll Loss: 1.3662\tTriple Loss(1): 0.1025\tClassification Loss: 1.1613\n","\n","Test set: Average loss: 1.5456, Accuracy: 278/480 (58%)\n","\n","Train Epoch: 4 [73600/110534 (67%)]\tAll Loss: 1.4303\tTriple Loss(0): 0.0000\tClassification Loss: 1.4303\n","Train Epoch: 4 [73760/110534 (67%)]\tAll Loss: 1.9560\tTriple Loss(1): 0.2202\tClassification Loss: 1.5155\n","Train Epoch: 4 [73920/110534 (67%)]\tAll Loss: 2.2823\tTriple Loss(1): 0.3600\tClassification Loss: 1.5622\n","Train Epoch: 4 [74080/110534 (67%)]\tAll Loss: 1.5160\tTriple Loss(1): 0.1063\tClassification Loss: 1.3034\n","Train Epoch: 4 [74240/110534 (67%)]\tAll Loss: 1.7063\tTriple Loss(0): 0.0000\tClassification Loss: 1.7063\n","Train Epoch: 4 [74400/110534 (67%)]\tAll Loss: 1.6818\tTriple Loss(0): 0.0000\tClassification Loss: 1.6818\n","Train Epoch: 4 [74560/110534 (67%)]\tAll Loss: 1.8501\tTriple Loss(1): 0.1853\tClassification Loss: 1.4795\n","Train Epoch: 4 [74720/110534 (68%)]\tAll Loss: 2.3642\tTriple Loss(1): 0.3797\tClassification Loss: 1.6048\n","Train Epoch: 4 [74880/110534 (68%)]\tAll Loss: 1.7759\tTriple Loss(0): 0.0000\tClassification Loss: 1.7759\n","Train Epoch: 4 [75040/110534 (68%)]\tAll Loss: 1.5022\tTriple Loss(1): 0.1646\tClassification Loss: 1.1731\n","\n","Test set: Average loss: 1.5299, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 4 [75200/110534 (68%)]\tAll Loss: 1.3047\tTriple Loss(0): 0.0000\tClassification Loss: 1.3047\n","Train Epoch: 4 [75360/110534 (68%)]\tAll Loss: 2.3776\tTriple Loss(0): 0.0000\tClassification Loss: 2.3776\n","Train Epoch: 4 [75520/110534 (68%)]\tAll Loss: 2.4093\tTriple Loss(1): 0.5126\tClassification Loss: 1.3840\n","Train Epoch: 4 [75680/110534 (68%)]\tAll Loss: 2.2431\tTriple Loss(1): 0.1579\tClassification Loss: 1.9274\n","Train Epoch: 4 [75840/110534 (69%)]\tAll Loss: 9.0630\tTriple Loss(0): 3.5920\tClassification Loss: 1.8791\n","Train Epoch: 4 [76000/110534 (69%)]\tAll Loss: 1.9044\tTriple Loss(1): 0.0234\tClassification Loss: 1.8577\n","Train Epoch: 4 [76160/110534 (69%)]\tAll Loss: 1.6741\tTriple Loss(0): 0.0000\tClassification Loss: 1.6741\n","Train Epoch: 4 [76320/110534 (69%)]\tAll Loss: 2.3618\tTriple Loss(1): 0.3560\tClassification Loss: 1.6497\n","Train Epoch: 4 [76480/110534 (69%)]\tAll Loss: 2.3925\tTriple Loss(1): 0.5108\tClassification Loss: 1.3708\n","Train Epoch: 4 [76640/110534 (69%)]\tAll Loss: 1.4275\tTriple Loss(1): 0.0000\tClassification Loss: 1.4275\n","\n","Test set: Average loss: 1.5202, Accuracy: 280/480 (58%)\n","\n","Train Epoch: 4 [76800/110534 (69%)]\tAll Loss: 1.4476\tTriple Loss(0): 0.0000\tClassification Loss: 1.4476\n","Train Epoch: 4 [76960/110534 (70%)]\tAll Loss: 1.8104\tTriple Loss(1): 0.2140\tClassification Loss: 1.3824\n","Train Epoch: 4 [77120/110534 (70%)]\tAll Loss: 1.5959\tTriple Loss(0): 0.0000\tClassification Loss: 1.5959\n","Train Epoch: 4 [77280/110534 (70%)]\tAll Loss: 1.6681\tTriple Loss(1): 0.2039\tClassification Loss: 1.2604\n","Train Epoch: 4 [77440/110534 (70%)]\tAll Loss: 1.6930\tTriple Loss(1): 0.0884\tClassification Loss: 1.5162\n","Train Epoch: 4 [77600/110534 (70%)]\tAll Loss: 2.1753\tTriple Loss(1): 0.0564\tClassification Loss: 2.0625\n","Train Epoch: 4 [77760/110534 (70%)]\tAll Loss: 1.9060\tTriple Loss(1): 0.0000\tClassification Loss: 1.9060\n","Train Epoch: 4 [77920/110534 (70%)]\tAll Loss: 2.6026\tTriple Loss(1): 0.2520\tClassification Loss: 2.0985\n","Train Epoch: 4 [78080/110534 (71%)]\tAll Loss: 1.8893\tTriple Loss(1): 0.1234\tClassification Loss: 1.6425\n","Train Epoch: 4 [78240/110534 (71%)]\tAll Loss: 1.6000\tTriple Loss(1): 0.1023\tClassification Loss: 1.3954\n","\n","Test set: Average loss: 1.5476, Accuracy: 267/480 (56%)\n","\n","Train Epoch: 4 [78400/110534 (71%)]\tAll Loss: 1.5741\tTriple Loss(1): 0.1364\tClassification Loss: 1.3014\n","Train Epoch: 4 [78560/110534 (71%)]\tAll Loss: 2.2817\tTriple Loss(0): 0.0000\tClassification Loss: 2.2817\n","Train Epoch: 4 [78720/110534 (71%)]\tAll Loss: 1.7767\tTriple Loss(1): 0.1681\tClassification Loss: 1.4404\n","Train Epoch: 4 [78880/110534 (71%)]\tAll Loss: 2.3153\tTriple Loss(1): 0.3882\tClassification Loss: 1.5389\n","Train Epoch: 4 [79040/110534 (72%)]\tAll Loss: 1.2814\tTriple Loss(0): 0.0000\tClassification Loss: 1.2814\n","Train Epoch: 4 [79200/110534 (72%)]\tAll Loss: 2.0246\tTriple Loss(1): 0.1518\tClassification Loss: 1.7209\n","Train Epoch: 4 [79360/110534 (72%)]\tAll Loss: 1.8532\tTriple Loss(1): 0.1388\tClassification Loss: 1.5756\n","Train Epoch: 4 [79520/110534 (72%)]\tAll Loss: 1.8021\tTriple Loss(1): 0.2799\tClassification Loss: 1.2423\n","Train Epoch: 4 [79680/110534 (72%)]\tAll Loss: 1.8080\tTriple Loss(1): 0.1571\tClassification Loss: 1.4939\n","Train Epoch: 4 [79840/110534 (72%)]\tAll Loss: 1.6277\tTriple Loss(1): 0.0993\tClassification Loss: 1.4292\n","\n","Test set: Average loss: 1.5372, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 4 [80000/110534 (72%)]\tAll Loss: 1.6178\tTriple Loss(1): 0.0078\tClassification Loss: 1.6022\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_5000.pth.tar\n","Train Epoch: 4 [80160/110534 (73%)]\tAll Loss: 2.3739\tTriple Loss(1): 0.2498\tClassification Loss: 1.8743\n","Train Epoch: 4 [80320/110534 (73%)]\tAll Loss: 2.4267\tTriple Loss(1): 0.0891\tClassification Loss: 2.2484\n","Train Epoch: 4 [80480/110534 (73%)]\tAll Loss: 2.0737\tTriple Loss(1): 0.5365\tClassification Loss: 1.0008\n","Train Epoch: 4 [80640/110534 (73%)]\tAll Loss: 1.5302\tTriple Loss(1): 0.1123\tClassification Loss: 1.3055\n","Train Epoch: 4 [80800/110534 (73%)]\tAll Loss: 2.0258\tTriple Loss(1): 0.1124\tClassification Loss: 1.8009\n","Train Epoch: 4 [80960/110534 (73%)]\tAll Loss: 1.5258\tTriple Loss(1): 0.1056\tClassification Loss: 1.3146\n","Train Epoch: 4 [81120/110534 (73%)]\tAll Loss: 1.6185\tTriple Loss(1): 0.0965\tClassification Loss: 1.4256\n","Train Epoch: 4 [81280/110534 (74%)]\tAll Loss: 2.3444\tTriple Loss(1): 0.4697\tClassification Loss: 1.4050\n","Train Epoch: 4 [81440/110534 (74%)]\tAll Loss: 1.5495\tTriple Loss(1): 0.2197\tClassification Loss: 1.1102\n","\n","Test set: Average loss: 1.5546, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 4 [81600/110534 (74%)]\tAll Loss: 1.8871\tTriple Loss(1): 0.2005\tClassification Loss: 1.4861\n","Train Epoch: 4 [81760/110534 (74%)]\tAll Loss: 1.3514\tTriple Loss(1): 0.1145\tClassification Loss: 1.1225\n","Train Epoch: 4 [81920/110534 (74%)]\tAll Loss: 1.3986\tTriple Loss(1): 0.0000\tClassification Loss: 1.3986\n","Train Epoch: 4 [82080/110534 (74%)]\tAll Loss: 1.2222\tTriple Loss(1): 0.0000\tClassification Loss: 1.2222\n","Train Epoch: 4 [82240/110534 (74%)]\tAll Loss: 1.0279\tTriple Loss(0): 0.0000\tClassification Loss: 1.0279\n","Train Epoch: 4 [82400/110534 (75%)]\tAll Loss: 1.9932\tTriple Loss(0): 0.0000\tClassification Loss: 1.9932\n","Train Epoch: 4 [82560/110534 (75%)]\tAll Loss: 2.1425\tTriple Loss(1): 0.0809\tClassification Loss: 1.9807\n","Train Epoch: 4 [82720/110534 (75%)]\tAll Loss: 1.5412\tTriple Loss(1): 0.0478\tClassification Loss: 1.4456\n","Train Epoch: 4 [82880/110534 (75%)]\tAll Loss: 2.2009\tTriple Loss(1): 0.3510\tClassification Loss: 1.4990\n","Train Epoch: 4 [83040/110534 (75%)]\tAll Loss: 1.9749\tTriple Loss(1): 0.2775\tClassification Loss: 1.4198\n","\n","Test set: Average loss: 1.5646, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 4 [83200/110534 (75%)]\tAll Loss: 1.8643\tTriple Loss(1): 0.3242\tClassification Loss: 1.2159\n","Train Epoch: 4 [83360/110534 (75%)]\tAll Loss: 2.4455\tTriple Loss(1): 0.1878\tClassification Loss: 2.0699\n","Train Epoch: 4 [83520/110534 (76%)]\tAll Loss: 2.6009\tTriple Loss(1): 0.4284\tClassification Loss: 1.7440\n","Train Epoch: 4 [83680/110534 (76%)]\tAll Loss: 1.9637\tTriple Loss(1): 0.0754\tClassification Loss: 1.8128\n","Train Epoch: 4 [83840/110534 (76%)]\tAll Loss: 1.3345\tTriple Loss(0): 0.0000\tClassification Loss: 1.3345\n","Train Epoch: 4 [84000/110534 (76%)]\tAll Loss: 1.1580\tTriple Loss(1): 0.0000\tClassification Loss: 1.1580\n","Train Epoch: 4 [84160/110534 (76%)]\tAll Loss: 1.9238\tTriple Loss(0): 0.0000\tClassification Loss: 1.9238\n","Train Epoch: 4 [84320/110534 (76%)]\tAll Loss: 1.3451\tTriple Loss(1): 0.1210\tClassification Loss: 1.1031\n","Train Epoch: 4 [84480/110534 (76%)]\tAll Loss: 1.8475\tTriple Loss(1): 0.0902\tClassification Loss: 1.6672\n","Train Epoch: 4 [84640/110534 (77%)]\tAll Loss: 1.7658\tTriple Loss(0): 0.0000\tClassification Loss: 1.7658\n","\n","Test set: Average loss: 1.5198, Accuracy: 282/480 (59%)\n","\n","Train Epoch: 4 [84800/110534 (77%)]\tAll Loss: 1.7237\tTriple Loss(1): 0.0477\tClassification Loss: 1.6282\n","Train Epoch: 4 [84960/110534 (77%)]\tAll Loss: 2.9612\tTriple Loss(1): 0.5329\tClassification Loss: 1.8953\n","Train Epoch: 4 [85120/110534 (77%)]\tAll Loss: 1.8287\tTriple Loss(1): 0.0930\tClassification Loss: 1.6428\n","Train Epoch: 4 [85280/110534 (77%)]\tAll Loss: 1.2277\tTriple Loss(0): 0.0000\tClassification Loss: 1.2277\n","Train Epoch: 4 [85440/110534 (77%)]\tAll Loss: 5.9101\tTriple Loss(0): 2.0636\tClassification Loss: 1.7830\n","Train Epoch: 4 [85600/110534 (77%)]\tAll Loss: 2.2970\tTriple Loss(1): 0.2407\tClassification Loss: 1.8156\n","Train Epoch: 4 [85760/110534 (78%)]\tAll Loss: 1.4015\tTriple Loss(1): 0.0302\tClassification Loss: 1.3412\n","Train Epoch: 4 [85920/110534 (78%)]\tAll Loss: 1.3401\tTriple Loss(1): 0.0000\tClassification Loss: 1.3401\n","Train Epoch: 4 [86080/110534 (78%)]\tAll Loss: 1.5983\tTriple Loss(1): 0.0349\tClassification Loss: 1.5285\n","Train Epoch: 4 [86240/110534 (78%)]\tAll Loss: 2.3484\tTriple Loss(1): 0.3747\tClassification Loss: 1.5990\n","\n","Test set: Average loss: 1.5500, Accuracy: 269/480 (56%)\n","\n","Train Epoch: 4 [86400/110534 (78%)]\tAll Loss: 1.5401\tTriple Loss(1): 0.1684\tClassification Loss: 1.2033\n","Train Epoch: 4 [86560/110534 (78%)]\tAll Loss: 2.1741\tTriple Loss(1): 0.1481\tClassification Loss: 1.8779\n","Train Epoch: 4 [86720/110534 (78%)]\tAll Loss: 2.0573\tTriple Loss(1): 0.0537\tClassification Loss: 1.9499\n","Train Epoch: 4 [86880/110534 (79%)]\tAll Loss: 1.9447\tTriple Loss(1): 0.2818\tClassification Loss: 1.3810\n","Train Epoch: 4 [87040/110534 (79%)]\tAll Loss: 1.3797\tTriple Loss(1): 0.0403\tClassification Loss: 1.2992\n","Train Epoch: 4 [87200/110534 (79%)]\tAll Loss: 1.9628\tTriple Loss(0): 0.0000\tClassification Loss: 1.9628\n","Train Epoch: 4 [87360/110534 (79%)]\tAll Loss: 2.2183\tTriple Loss(1): 0.0000\tClassification Loss: 2.2183\n","Train Epoch: 4 [87520/110534 (79%)]\tAll Loss: 1.3152\tTriple Loss(0): 0.0000\tClassification Loss: 1.3152\n","Train Epoch: 4 [87680/110534 (79%)]\tAll Loss: 2.9453\tTriple Loss(1): 0.6331\tClassification Loss: 1.6791\n","Train Epoch: 4 [87840/110534 (79%)]\tAll Loss: 1.1639\tTriple Loss(0): 0.0000\tClassification Loss: 1.1639\n","\n","Test set: Average loss: 1.5226, Accuracy: 273/480 (57%)\n","\n","Train Epoch: 4 [88000/110534 (80%)]\tAll Loss: 1.4867\tTriple Loss(1): 0.0000\tClassification Loss: 1.4867\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_5500.pth.tar\n","Train Epoch: 4 [88160/110534 (80%)]\tAll Loss: 2.2221\tTriple Loss(1): 0.1304\tClassification Loss: 1.9614\n","Train Epoch: 4 [88320/110534 (80%)]\tAll Loss: 2.2345\tTriple Loss(1): 0.3210\tClassification Loss: 1.5925\n","Train Epoch: 4 [88480/110534 (80%)]\tAll Loss: 1.2246\tTriple Loss(1): 0.0129\tClassification Loss: 1.1987\n","Train Epoch: 4 [88640/110534 (80%)]\tAll Loss: 1.6976\tTriple Loss(1): 0.0545\tClassification Loss: 1.5885\n","Train Epoch: 4 [88800/110534 (80%)]\tAll Loss: 0.8179\tTriple Loss(0): 0.0000\tClassification Loss: 0.8179\n","Train Epoch: 4 [88960/110534 (80%)]\tAll Loss: 1.5774\tTriple Loss(1): 0.0721\tClassification Loss: 1.4333\n","Train Epoch: 4 [89120/110534 (81%)]\tAll Loss: 1.3501\tTriple Loss(1): 0.1157\tClassification Loss: 1.1186\n","Train Epoch: 4 [89280/110534 (81%)]\tAll Loss: 1.6056\tTriple Loss(1): 0.1548\tClassification Loss: 1.2960\n","Train Epoch: 4 [89440/110534 (81%)]\tAll Loss: 1.2790\tTriple Loss(0): 0.0000\tClassification Loss: 1.2790\n","\n","Test set: Average loss: 1.5598, Accuracy: 263/480 (55%)\n","\n","Train Epoch: 4 [89600/110534 (81%)]\tAll Loss: 2.2944\tTriple Loss(1): 0.2316\tClassification Loss: 1.8311\n","Train Epoch: 4 [89760/110534 (81%)]\tAll Loss: 2.8050\tTriple Loss(1): 0.4762\tClassification Loss: 1.8525\n","Train Epoch: 4 [89920/110534 (81%)]\tAll Loss: 1.5798\tTriple Loss(1): 0.0000\tClassification Loss: 1.5798\n","Train Epoch: 4 [90080/110534 (81%)]\tAll Loss: 2.2540\tTriple Loss(1): 0.1192\tClassification Loss: 2.0156\n","Train Epoch: 4 [90240/110534 (82%)]\tAll Loss: 2.3167\tTriple Loss(1): 0.0835\tClassification Loss: 2.1497\n","Train Epoch: 4 [90400/110534 (82%)]\tAll Loss: 2.8544\tTriple Loss(1): 0.3994\tClassification Loss: 2.0556\n","Train Epoch: 4 [90560/110534 (82%)]\tAll Loss: 1.8863\tTriple Loss(1): 0.1721\tClassification Loss: 1.5421\n","Train Epoch: 4 [90720/110534 (82%)]\tAll Loss: 1.6403\tTriple Loss(1): 0.0295\tClassification Loss: 1.5813\n","Train Epoch: 4 [90880/110534 (82%)]\tAll Loss: 1.8245\tTriple Loss(1): 0.2120\tClassification Loss: 1.4004\n","Train Epoch: 4 [91040/110534 (82%)]\tAll Loss: 1.6096\tTriple Loss(1): 0.0618\tClassification Loss: 1.4861\n","\n","Test set: Average loss: 1.5710, Accuracy: 270/480 (56%)\n","\n","Train Epoch: 4 [91200/110534 (83%)]\tAll Loss: 2.1034\tTriple Loss(1): 0.0633\tClassification Loss: 1.9767\n","Train Epoch: 4 [91360/110534 (83%)]\tAll Loss: 1.5916\tTriple Loss(1): 0.2388\tClassification Loss: 1.1140\n","Train Epoch: 4 [91520/110534 (83%)]\tAll Loss: 1.5970\tTriple Loss(1): 0.1091\tClassification Loss: 1.3787\n","Train Epoch: 4 [91680/110534 (83%)]\tAll Loss: 1.5969\tTriple Loss(0): 0.0000\tClassification Loss: 1.5969\n","Train Epoch: 4 [91840/110534 (83%)]\tAll Loss: 1.3680\tTriple Loss(0): 0.0000\tClassification Loss: 1.3680\n","Train Epoch: 4 [92000/110534 (83%)]\tAll Loss: 1.3254\tTriple Loss(1): 0.0000\tClassification Loss: 1.3254\n","Train Epoch: 4 [92160/110534 (83%)]\tAll Loss: 2.0958\tTriple Loss(1): 0.0751\tClassification Loss: 1.9456\n","Train Epoch: 4 [92320/110534 (84%)]\tAll Loss: 2.0711\tTriple Loss(1): 0.1627\tClassification Loss: 1.7458\n","Train Epoch: 4 [92480/110534 (84%)]\tAll Loss: 1.8619\tTriple Loss(1): 0.1328\tClassification Loss: 1.5963\n","Train Epoch: 4 [92640/110534 (84%)]\tAll Loss: 1.7644\tTriple Loss(1): 0.0997\tClassification Loss: 1.5651\n","\n","Test set: Average loss: 1.5220, Accuracy: 277/480 (58%)\n","\n","Train Epoch: 4 [92800/110534 (84%)]\tAll Loss: 2.0920\tTriple Loss(1): 0.1379\tClassification Loss: 1.8162\n","Train Epoch: 4 [92960/110534 (84%)]\tAll Loss: 3.1315\tTriple Loss(1): 0.5466\tClassification Loss: 2.0383\n","Train Epoch: 4 [93120/110534 (84%)]\tAll Loss: 2.7848\tTriple Loss(1): 0.3226\tClassification Loss: 2.1396\n","Train Epoch: 4 [93280/110534 (84%)]\tAll Loss: 1.5537\tTriple Loss(1): 0.0288\tClassification Loss: 1.4960\n","Train Epoch: 4 [93440/110534 (85%)]\tAll Loss: 2.0692\tTriple Loss(1): 0.2949\tClassification Loss: 1.4794\n","Train Epoch: 4 [93600/110534 (85%)]\tAll Loss: 1.6296\tTriple Loss(0): 0.0000\tClassification Loss: 1.6296\n","Train Epoch: 4 [93760/110534 (85%)]\tAll Loss: 1.5704\tTriple Loss(1): 0.1172\tClassification Loss: 1.3359\n","Train Epoch: 4 [93920/110534 (85%)]\tAll Loss: 1.9951\tTriple Loss(1): 0.0335\tClassification Loss: 1.9280\n","Train Epoch: 4 [94080/110534 (85%)]\tAll Loss: 2.0653\tTriple Loss(1): 0.3894\tClassification Loss: 1.2865\n","Train Epoch: 4 [94240/110534 (85%)]\tAll Loss: 1.6632\tTriple Loss(1): 0.0153\tClassification Loss: 1.6325\n","\n","Test set: Average loss: 1.5536, Accuracy: 272/480 (57%)\n","\n","Train Epoch: 4 [94400/110534 (85%)]\tAll Loss: 1.8910\tTriple Loss(0): 0.0000\tClassification Loss: 1.8910\n","Train Epoch: 4 [94560/110534 (86%)]\tAll Loss: 2.2251\tTriple Loss(1): 0.3701\tClassification Loss: 1.4849\n","Train Epoch: 4 [94720/110534 (86%)]\tAll Loss: 1.4581\tTriple Loss(1): 0.0249\tClassification Loss: 1.4083\n","Train Epoch: 4 [94880/110534 (86%)]\tAll Loss: 1.1483\tTriple Loss(0): 0.0000\tClassification Loss: 1.1483\n","Train Epoch: 4 [95040/110534 (86%)]\tAll Loss: 1.9263\tTriple Loss(1): 0.1941\tClassification Loss: 1.5381\n","Train Epoch: 4 [95200/110534 (86%)]\tAll Loss: 2.3529\tTriple Loss(1): 0.2531\tClassification Loss: 1.8466\n","Train Epoch: 4 [95360/110534 (86%)]\tAll Loss: 1.9820\tTriple Loss(1): 0.0237\tClassification Loss: 1.9346\n","Train Epoch: 4 [95520/110534 (86%)]\tAll Loss: 1.6085\tTriple Loss(1): 0.0974\tClassification Loss: 1.4137\n","Train Epoch: 4 [95680/110534 (87%)]\tAll Loss: 1.8915\tTriple Loss(1): 0.0770\tClassification Loss: 1.7376\n","Train Epoch: 4 [95840/110534 (87%)]\tAll Loss: 2.1702\tTriple Loss(1): 0.2790\tClassification Loss: 1.6122\n","\n","Test set: Average loss: 1.5313, Accuracy: 265/480 (55%)\n","\n","Train Epoch: 4 [96000/110534 (87%)]\tAll Loss: 10.5046\tTriple Loss(0): 4.5845\tClassification Loss: 1.3356\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_6000.pth.tar\n","Train Epoch: 4 [96160/110534 (87%)]\tAll Loss: 2.1074\tTriple Loss(1): 0.2843\tClassification Loss: 1.5389\n","Train Epoch: 4 [96320/110534 (87%)]\tAll Loss: 2.1050\tTriple Loss(1): 0.1467\tClassification Loss: 1.8116\n","Train Epoch: 4 [96480/110534 (87%)]\tAll Loss: 1.7666\tTriple Loss(1): 0.0567\tClassification Loss: 1.6531\n","Train Epoch: 4 [96640/110534 (87%)]\tAll Loss: 1.9783\tTriple Loss(1): 0.2139\tClassification Loss: 1.5506\n","Train Epoch: 4 [96800/110534 (88%)]\tAll Loss: 2.8676\tTriple Loss(1): 0.3481\tClassification Loss: 2.1714\n","Train Epoch: 4 [96960/110534 (88%)]\tAll Loss: 1.8735\tTriple Loss(1): 0.2365\tClassification Loss: 1.4005\n","Train Epoch: 4 [97120/110534 (88%)]\tAll Loss: 2.3763\tTriple Loss(1): 0.1503\tClassification Loss: 2.0756\n","Train Epoch: 4 [97280/110534 (88%)]\tAll Loss: 1.7978\tTriple Loss(1): 0.2058\tClassification Loss: 1.3862\n","Train Epoch: 4 [97440/110534 (88%)]\tAll Loss: 1.6852\tTriple Loss(1): 0.2648\tClassification Loss: 1.1556\n","\n","Test set: Average loss: 1.5213, Accuracy: 268/480 (56%)\n","\n","Train Epoch: 4 [97600/110534 (88%)]\tAll Loss: 2.1500\tTriple Loss(1): 0.2720\tClassification Loss: 1.6059\n","Train Epoch: 4 [97760/110534 (88%)]\tAll Loss: 2.0218\tTriple Loss(1): 0.2517\tClassification Loss: 1.5184\n","Train Epoch: 4 [97920/110534 (89%)]\tAll Loss: 1.5643\tTriple Loss(1): 0.0552\tClassification Loss: 1.4538\n","Train Epoch: 4 [98080/110534 (89%)]\tAll Loss: 1.2884\tTriple Loss(1): 0.0233\tClassification Loss: 1.2417\n","Train Epoch: 4 [98240/110534 (89%)]\tAll Loss: 0.9464\tTriple Loss(0): 0.0000\tClassification Loss: 0.9464\n","Train Epoch: 4 [98400/110534 (89%)]\tAll Loss: 1.0804\tTriple Loss(0): 0.0000\tClassification Loss: 1.0804\n","Train Epoch: 4 [98560/110534 (89%)]\tAll Loss: 1.5171\tTriple Loss(1): 0.3646\tClassification Loss: 0.7880\n","Train Epoch: 4 [98720/110534 (89%)]\tAll Loss: 2.0165\tTriple Loss(1): 0.3982\tClassification Loss: 1.2201\n","Train Epoch: 4 [98880/110534 (89%)]\tAll Loss: 1.4959\tTriple Loss(1): 0.3048\tClassification Loss: 0.8862\n","Train Epoch: 4 [99040/110534 (90%)]\tAll Loss: 2.2769\tTriple Loss(1): 0.3373\tClassification Loss: 1.6022\n","\n","Test set: Average loss: 1.5242, Accuracy: 271/480 (56%)\n","\n","Train Epoch: 4 [99200/110534 (90%)]\tAll Loss: 1.5855\tTriple Loss(0): 0.0000\tClassification Loss: 1.5855\n","Train Epoch: 4 [99360/110534 (90%)]\tAll Loss: 1.6552\tTriple Loss(1): 0.1599\tClassification Loss: 1.3354\n","Train Epoch: 4 [99520/110534 (90%)]\tAll Loss: 2.0529\tTriple Loss(0): 0.0000\tClassification Loss: 2.0529\n","Train Epoch: 4 [99680/110534 (90%)]\tAll Loss: 2.1862\tTriple Loss(1): 0.3179\tClassification Loss: 1.5504\n","Train Epoch: 4 [99840/110534 (90%)]\tAll Loss: 2.5321\tTriple Loss(1): 0.3119\tClassification Loss: 1.9083\n","Train Epoch: 4 [100000/110534 (90%)]\tAll Loss: 1.5720\tTriple Loss(1): 0.0222\tClassification Loss: 1.5276\n","Train Epoch: 4 [100160/110534 (91%)]\tAll Loss: 2.7198\tTriple Loss(1): 0.2780\tClassification Loss: 2.1639\n","Train Epoch: 4 [100320/110534 (91%)]\tAll Loss: 1.7467\tTriple Loss(1): 0.0911\tClassification Loss: 1.5645\n","Train Epoch: 4 [100480/110534 (91%)]\tAll Loss: 2.1018\tTriple Loss(0): 0.0000\tClassification Loss: 2.1018\n","Train Epoch: 4 [100640/110534 (91%)]\tAll Loss: 2.7834\tTriple Loss(1): 0.5081\tClassification Loss: 1.7672\n","\n","Test set: Average loss: 1.5039, Accuracy: 278/480 (58%)\n","\n","Train Epoch: 4 [100800/110534 (91%)]\tAll Loss: 2.2636\tTriple Loss(1): 0.1093\tClassification Loss: 2.0451\n","Train Epoch: 4 [100960/110534 (91%)]\tAll Loss: 1.7049\tTriple Loss(1): 0.1319\tClassification Loss: 1.4411\n","Train Epoch: 4 [101120/110534 (91%)]\tAll Loss: 1.5892\tTriple Loss(0): 0.0000\tClassification Loss: 1.5892\n","Train Epoch: 4 [101280/110534 (92%)]\tAll Loss: 1.7933\tTriple Loss(1): 0.2299\tClassification Loss: 1.3335\n","Train Epoch: 4 [101440/110534 (92%)]\tAll Loss: 1.8088\tTriple Loss(1): 0.2174\tClassification Loss: 1.3739\n","Train Epoch: 4 [101600/110534 (92%)]\tAll Loss: 2.1533\tTriple Loss(1): 0.2800\tClassification Loss: 1.5932\n","Train Epoch: 4 [101760/110534 (92%)]\tAll Loss: 1.5250\tTriple Loss(0): 0.0000\tClassification Loss: 1.5250\n","Train Epoch: 4 [101920/110534 (92%)]\tAll Loss: 1.7844\tTriple Loss(1): 0.2909\tClassification Loss: 1.2026\n","Train Epoch: 4 [102080/110534 (92%)]\tAll Loss: 2.0597\tTriple Loss(1): 0.2828\tClassification Loss: 1.4940\n","Train Epoch: 4 [102240/110534 (92%)]\tAll Loss: 1.5316\tTriple Loss(1): 0.0498\tClassification Loss: 1.4321\n","\n","Test set: Average loss: 1.5463, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 4 [102400/110534 (93%)]\tAll Loss: 1.3441\tTriple Loss(1): 0.0323\tClassification Loss: 1.2795\n","Train Epoch: 4 [102560/110534 (93%)]\tAll Loss: 7.3939\tTriple Loss(0): 3.0272\tClassification Loss: 1.3396\n","Train Epoch: 4 [102720/110534 (93%)]\tAll Loss: 1.1761\tTriple Loss(1): 0.0607\tClassification Loss: 1.0548\n","Train Epoch: 4 [102880/110534 (93%)]\tAll Loss: 1.3264\tTriple Loss(1): 0.1010\tClassification Loss: 1.1243\n","Train Epoch: 4 [103040/110534 (93%)]\tAll Loss: 1.9849\tTriple Loss(1): 0.2271\tClassification Loss: 1.5308\n","Train Epoch: 4 [103200/110534 (93%)]\tAll Loss: 3.3384\tTriple Loss(1): 0.4209\tClassification Loss: 2.4965\n","Train Epoch: 4 [103360/110534 (94%)]\tAll Loss: 1.1706\tTriple Loss(1): 0.0000\tClassification Loss: 1.1706\n","Train Epoch: 4 [103520/110534 (94%)]\tAll Loss: 2.0099\tTriple Loss(0): 0.0000\tClassification Loss: 2.0099\n","Train Epoch: 4 [103680/110534 (94%)]\tAll Loss: 1.7047\tTriple Loss(1): 0.1582\tClassification Loss: 1.3882\n","Train Epoch: 4 [103840/110534 (94%)]\tAll Loss: 2.0075\tTriple Loss(1): 0.5368\tClassification Loss: 0.9339\n","\n","Test set: Average loss: 1.4878, Accuracy: 282/480 (59%)\n","\n","Train Epoch: 4 [104000/110534 (94%)]\tAll Loss: 1.1788\tTriple Loss(0): 0.0000\tClassification Loss: 1.1788\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_6500.pth.tar\n","Train Epoch: 4 [104160/110534 (94%)]\tAll Loss: 1.5127\tTriple Loss(1): 0.0511\tClassification Loss: 1.4104\n","Train Epoch: 4 [104320/110534 (94%)]\tAll Loss: 2.6548\tTriple Loss(1): 0.3699\tClassification Loss: 1.9150\n","Train Epoch: 4 [104480/110534 (95%)]\tAll Loss: 1.6726\tTriple Loss(0): 0.0000\tClassification Loss: 1.6726\n","Train Epoch: 4 [104640/110534 (95%)]\tAll Loss: 1.7360\tTriple Loss(1): 0.0705\tClassification Loss: 1.5951\n","Train Epoch: 4 [104800/110534 (95%)]\tAll Loss: 2.0476\tTriple Loss(1): 0.1524\tClassification Loss: 1.7429\n","Train Epoch: 4 [104960/110534 (95%)]\tAll Loss: 1.9766\tTriple Loss(1): 0.0910\tClassification Loss: 1.7945\n","Train Epoch: 4 [105120/110534 (95%)]\tAll Loss: 1.6224\tTriple Loss(0): 0.0000\tClassification Loss: 1.6224\n","Train Epoch: 4 [105280/110534 (95%)]\tAll Loss: 1.7951\tTriple Loss(1): 0.1296\tClassification Loss: 1.5360\n","Train Epoch: 4 [105440/110534 (95%)]\tAll Loss: 1.5536\tTriple Loss(1): 0.0369\tClassification Loss: 1.4798\n","\n","Test set: Average loss: 1.5488, Accuracy: 271/480 (56%)\n","\n","Train Epoch: 4 [105600/110534 (96%)]\tAll Loss: 1.8591\tTriple Loss(1): 0.0635\tClassification Loss: 1.7322\n","Train Epoch: 4 [105760/110534 (96%)]\tAll Loss: 1.8469\tTriple Loss(1): 0.2031\tClassification Loss: 1.4408\n","Train Epoch: 4 [105920/110534 (96%)]\tAll Loss: 2.1973\tTriple Loss(1): 0.1360\tClassification Loss: 1.9252\n","Train Epoch: 4 [106080/110534 (96%)]\tAll Loss: 0.9706\tTriple Loss(0): 0.0000\tClassification Loss: 0.9706\n","Train Epoch: 4 [106240/110534 (96%)]\tAll Loss: 2.2396\tTriple Loss(1): 0.1487\tClassification Loss: 1.9423\n","Train Epoch: 4 [106400/110534 (96%)]\tAll Loss: 1.6645\tTriple Loss(1): 0.0134\tClassification Loss: 1.6378\n","Train Epoch: 4 [106560/110534 (96%)]\tAll Loss: 1.7975\tTriple Loss(1): 0.3536\tClassification Loss: 1.0904\n","Train Epoch: 4 [106720/110534 (97%)]\tAll Loss: 2.0128\tTriple Loss(1): 0.0881\tClassification Loss: 1.8366\n","Train Epoch: 4 [106880/110534 (97%)]\tAll Loss: 2.0311\tTriple Loss(1): 0.2224\tClassification Loss: 1.5863\n","Train Epoch: 4 [107040/110534 (97%)]\tAll Loss: 1.8494\tTriple Loss(0): 0.0000\tClassification Loss: 1.8494\n","\n","Test set: Average loss: 1.5696, Accuracy: 264/480 (55%)\n","\n","Train Epoch: 4 [107200/110534 (97%)]\tAll Loss: 2.1020\tTriple Loss(1): 0.4212\tClassification Loss: 1.2596\n","Train Epoch: 4 [107360/110534 (97%)]\tAll Loss: 2.3275\tTriple Loss(1): 0.4108\tClassification Loss: 1.5060\n","Train Epoch: 4 [107520/110534 (97%)]\tAll Loss: 1.1743\tTriple Loss(1): 0.0120\tClassification Loss: 1.1503\n","Train Epoch: 4 [107680/110534 (97%)]\tAll Loss: 1.8868\tTriple Loss(0): 0.0000\tClassification Loss: 1.8868\n","Train Epoch: 4 [107840/110534 (98%)]\tAll Loss: 1.8442\tTriple Loss(1): 0.1866\tClassification Loss: 1.4710\n","Train Epoch: 4 [108000/110534 (98%)]\tAll Loss: 1.4860\tTriple Loss(1): 0.0577\tClassification Loss: 1.3706\n","Train Epoch: 4 [108160/110534 (98%)]\tAll Loss: 1.9517\tTriple Loss(1): 0.0232\tClassification Loss: 1.9053\n","Train Epoch: 4 [108320/110534 (98%)]\tAll Loss: 1.8764\tTriple Loss(1): 0.1102\tClassification Loss: 1.6560\n","Train Epoch: 4 [108480/110534 (98%)]\tAll Loss: 1.3815\tTriple Loss(1): 0.1166\tClassification Loss: 1.1483\n","Train Epoch: 4 [108640/110534 (98%)]\tAll Loss: 0.9989\tTriple Loss(0): 0.0000\tClassification Loss: 0.9989\n","\n","Test set: Average loss: 1.5504, Accuracy: 285/480 (59%)\n","\n","Train Epoch: 4 [108800/110534 (98%)]\tAll Loss: 2.0547\tTriple Loss(1): 0.0117\tClassification Loss: 2.0313\n","Train Epoch: 4 [108960/110534 (99%)]\tAll Loss: 1.8881\tTriple Loss(1): 0.2926\tClassification Loss: 1.3028\n","Train Epoch: 4 [109120/110534 (99%)]\tAll Loss: 1.5950\tTriple Loss(1): 0.2056\tClassification Loss: 1.1838\n","Train Epoch: 4 [109280/110534 (99%)]\tAll Loss: 2.2166\tTriple Loss(1): 0.3156\tClassification Loss: 1.5854\n","Train Epoch: 4 [109440/110534 (99%)]\tAll Loss: 1.6775\tTriple Loss(0): 0.0000\tClassification Loss: 1.6775\n","Train Epoch: 4 [109600/110534 (99%)]\tAll Loss: 2.3045\tTriple Loss(1): 0.2051\tClassification Loss: 1.8943\n","Train Epoch: 4 [109760/110534 (99%)]\tAll Loss: 5.1711\tTriple Loss(0): 1.8747\tClassification Loss: 1.4217\n","Train Epoch: 4 [109920/110534 (99%)]\tAll Loss: 2.9421\tTriple Loss(1): 0.5682\tClassification Loss: 1.8058\n","Train Epoch: 4 [110080/110534 (100%)]\tAll Loss: 2.7372\tTriple Loss(1): 0.3729\tClassification Loss: 1.9914\n","Train Epoch: 4 [110240/110534 (100%)]\tAll Loss: 1.5150\tTriple Loss(0): 0.0000\tClassification Loss: 1.5150\n","\n","Test set: Average loss: 1.5209, Accuracy: 286/480 (60%)\n","\n","Train Epoch: 4 [110400/110534 (100%)]\tAll Loss: 2.4398\tTriple Loss(0): 0.0000\tClassification Loss: 2.4398\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_final.pth.tar\n","\n","Test set: Average loss: 1.5246, Accuracy: 278/480 (58%)\n","\n","Train Epoch: 5 [0/110534 (0%)]\tAll Loss: 1.8554\tTriple Loss(1): 0.0093\tClassification Loss: 1.8368\n","Train Epoch: 5 [160/110534 (0%)]\tAll Loss: 2.1073\tTriple Loss(1): 0.2220\tClassification Loss: 1.6633\n","Train Epoch: 5 [320/110534 (0%)]\tAll Loss: 1.9273\tTriple Loss(1): 0.1888\tClassification Loss: 1.5497\n","Train Epoch: 5 [480/110534 (0%)]\tAll Loss: 2.0110\tTriple Loss(1): 0.1065\tClassification Loss: 1.7981\n","Train Epoch: 5 [640/110534 (1%)]\tAll Loss: 2.4645\tTriple Loss(1): 0.5342\tClassification Loss: 1.3962\n","Train Epoch: 5 [800/110534 (1%)]\tAll Loss: 1.5757\tTriple Loss(1): 0.1437\tClassification Loss: 1.2883\n","Train Epoch: 5 [960/110534 (1%)]\tAll Loss: 1.0016\tTriple Loss(1): 0.0000\tClassification Loss: 1.0016\n","Train Epoch: 5 [1120/110534 (1%)]\tAll Loss: 2.6002\tTriple Loss(1): 0.2008\tClassification Loss: 2.1987\n","Train Epoch: 5 [1280/110534 (1%)]\tAll Loss: 1.9827\tTriple Loss(1): 0.0557\tClassification Loss: 1.8713\n","Train Epoch: 5 [1440/110534 (1%)]\tAll Loss: 2.1212\tTriple Loss(1): 0.3542\tClassification Loss: 1.4127\n","\n","Test set: Average loss: 1.5277, Accuracy: 284/480 (59%)\n","\n","Train Epoch: 5 [1600/110534 (1%)]\tAll Loss: 1.5366\tTriple Loss(1): 0.2945\tClassification Loss: 0.9477\n","Train Epoch: 5 [1760/110534 (2%)]\tAll Loss: 1.9174\tTriple Loss(1): 0.3945\tClassification Loss: 1.1284\n","Train Epoch: 5 [1920/110534 (2%)]\tAll Loss: 1.8438\tTriple Loss(1): 0.1583\tClassification Loss: 1.5272\n","Train Epoch: 5 [2080/110534 (2%)]\tAll Loss: 2.7038\tTriple Loss(1): 0.1082\tClassification Loss: 2.4873\n","Train Epoch: 5 [2240/110534 (2%)]\tAll Loss: 2.1225\tTriple Loss(1): 0.1436\tClassification Loss: 1.8352\n","Train Epoch: 5 [2400/110534 (2%)]\tAll Loss: 1.8434\tTriple Loss(1): 0.1255\tClassification Loss: 1.5924\n","Train Epoch: 5 [2560/110534 (2%)]\tAll Loss: 0.9767\tTriple Loss(1): 0.0420\tClassification Loss: 0.8927\n","Train Epoch: 5 [2720/110534 (2%)]\tAll Loss: 1.5469\tTriple Loss(1): 0.1895\tClassification Loss: 1.1678\n","Train Epoch: 5 [2880/110534 (3%)]\tAll Loss: 2.8214\tTriple Loss(1): 0.2729\tClassification Loss: 2.2756\n","Train Epoch: 5 [3040/110534 (3%)]\tAll Loss: 1.5443\tTriple Loss(1): 0.0553\tClassification Loss: 1.4338\n","\n","Test set: Average loss: 1.5232, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 5 [3200/110534 (3%)]\tAll Loss: 1.6344\tTriple Loss(1): 0.2210\tClassification Loss: 1.1923\n","Train Epoch: 5 [3360/110534 (3%)]\tAll Loss: 1.3481\tTriple Loss(0): 0.0000\tClassification Loss: 1.3481\n","Train Epoch: 5 [3520/110534 (3%)]\tAll Loss: 1.8348\tTriple Loss(1): 0.0000\tClassification Loss: 1.8348\n","Train Epoch: 5 [3680/110534 (3%)]\tAll Loss: 1.9691\tTriple Loss(1): 0.1337\tClassification Loss: 1.7017\n","Train Epoch: 5 [3840/110534 (3%)]\tAll Loss: 1.9581\tTriple Loss(1): 0.1878\tClassification Loss: 1.5824\n","Train Epoch: 5 [4000/110534 (4%)]\tAll Loss: 2.3728\tTriple Loss(1): 0.3820\tClassification Loss: 1.6088\n","Train Epoch: 5 [4160/110534 (4%)]\tAll Loss: 1.5254\tTriple Loss(0): 0.0000\tClassification Loss: 1.5254\n","Train Epoch: 5 [4320/110534 (4%)]\tAll Loss: 1.7217\tTriple Loss(1): 0.0787\tClassification Loss: 1.5642\n","Train Epoch: 5 [4480/110534 (4%)]\tAll Loss: 3.0488\tTriple Loss(1): 0.3792\tClassification Loss: 2.2905\n","Train Epoch: 5 [4640/110534 (4%)]\tAll Loss: 1.5194\tTriple Loss(1): 0.0000\tClassification Loss: 1.5194\n","\n","Test set: Average loss: 1.5323, Accuracy: 275/480 (57%)\n","\n","Train Epoch: 5 [4800/110534 (4%)]\tAll Loss: 1.6738\tTriple Loss(1): 0.0000\tClassification Loss: 1.6738\n","Train Epoch: 5 [4960/110534 (4%)]\tAll Loss: 1.1833\tTriple Loss(1): 0.0000\tClassification Loss: 1.1833\n","Train Epoch: 5 [5120/110534 (5%)]\tAll Loss: 1.6209\tTriple Loss(0): 0.0000\tClassification Loss: 1.6209\n","Train Epoch: 5 [5280/110534 (5%)]\tAll Loss: 0.9623\tTriple Loss(1): 0.0302\tClassification Loss: 0.9020\n","Train Epoch: 5 [5440/110534 (5%)]\tAll Loss: 1.2863\tTriple Loss(0): 0.0000\tClassification Loss: 1.2863\n","Train Epoch: 5 [5600/110534 (5%)]\tAll Loss: 1.5087\tTriple Loss(1): 0.0397\tClassification Loss: 1.4292\n","Train Epoch: 5 [5760/110534 (5%)]\tAll Loss: 1.6891\tTriple Loss(1): 0.3153\tClassification Loss: 1.0585\n","Train Epoch: 5 [5920/110534 (5%)]\tAll Loss: 2.6278\tTriple Loss(1): 0.4713\tClassification Loss: 1.6851\n","Train Epoch: 5 [6080/110534 (6%)]\tAll Loss: 1.6573\tTriple Loss(1): 0.0823\tClassification Loss: 1.4927\n","Train Epoch: 5 [6240/110534 (6%)]\tAll Loss: 1.8540\tTriple Loss(1): 0.2866\tClassification Loss: 1.2808\n","\n","Test set: Average loss: 1.5252, Accuracy: 272/480 (57%)\n","\n","Train Epoch: 5 [6400/110534 (6%)]\tAll Loss: 1.6251\tTriple Loss(1): 0.3393\tClassification Loss: 0.9465\n","Train Epoch: 5 [6560/110534 (6%)]\tAll Loss: 1.5718\tTriple Loss(1): 0.1490\tClassification Loss: 1.2738\n","Train Epoch: 5 [6720/110534 (6%)]\tAll Loss: 2.1389\tTriple Loss(0): 0.0000\tClassification Loss: 2.1389\n","Train Epoch: 5 [6880/110534 (6%)]\tAll Loss: 1.3975\tTriple Loss(1): 0.1247\tClassification Loss: 1.1481\n","Train Epoch: 5 [7040/110534 (6%)]\tAll Loss: 1.1697\tTriple Loss(0): 0.0000\tClassification Loss: 1.1697\n","Train Epoch: 5 [7200/110534 (7%)]\tAll Loss: 1.6467\tTriple Loss(1): 0.0730\tClassification Loss: 1.5008\n","Train Epoch: 5 [7360/110534 (7%)]\tAll Loss: 2.2055\tTriple Loss(1): 0.2515\tClassification Loss: 1.7025\n","Train Epoch: 5 [7520/110534 (7%)]\tAll Loss: 1.4787\tTriple Loss(0): 0.0000\tClassification Loss: 1.4787\n","Train Epoch: 5 [7680/110534 (7%)]\tAll Loss: 2.1746\tTriple Loss(1): 0.1087\tClassification Loss: 1.9572\n","Train Epoch: 5 [7840/110534 (7%)]\tAll Loss: 2.0076\tTriple Loss(1): 0.2442\tClassification Loss: 1.5193\n","\n","Test set: Average loss: 1.5265, Accuracy: 281/480 (59%)\n","\n","Train Epoch: 5 [8000/110534 (7%)]\tAll Loss: 1.9997\tTriple Loss(1): 0.0022\tClassification Loss: 1.9953\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_5_500.pth.tar\n","Train Epoch: 5 [8160/110534 (7%)]\tAll Loss: 1.7911\tTriple Loss(1): 0.1029\tClassification Loss: 1.5852\n","Train Epoch: 5 [8320/110534 (8%)]\tAll Loss: 1.6614\tTriple Loss(0): 0.0000\tClassification Loss: 1.6614\n","Train Epoch: 5 [8480/110534 (8%)]\tAll Loss: 2.0789\tTriple Loss(1): 0.1532\tClassification Loss: 1.7725\n","Train Epoch: 5 [8640/110534 (8%)]\tAll Loss: 1.1207\tTriple Loss(1): 0.0474\tClassification Loss: 1.0259\n","Train Epoch: 5 [8800/110534 (8%)]\tAll Loss: 1.9721\tTriple Loss(1): 0.0946\tClassification Loss: 1.7828\n","Train Epoch: 5 [8960/110534 (8%)]\tAll Loss: 1.5844\tTriple Loss(1): 0.0608\tClassification Loss: 1.4627\n","Train Epoch: 5 [9120/110534 (8%)]\tAll Loss: 2.2073\tTriple Loss(1): 0.1568\tClassification Loss: 1.8937\n","Train Epoch: 5 [9280/110534 (8%)]\tAll Loss: 1.4496\tTriple Loss(1): 0.0000\tClassification Loss: 1.4496\n","Train Epoch: 5 [9440/110534 (9%)]\tAll Loss: 2.0085\tTriple Loss(1): 0.2073\tClassification Loss: 1.5939\n","\n","Test set: Average loss: 1.5101, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 5 [9600/110534 (9%)]\tAll Loss: 1.6473\tTriple Loss(1): 0.0091\tClassification Loss: 1.6292\n","Train Epoch: 5 [9760/110534 (9%)]\tAll Loss: 2.1860\tTriple Loss(1): 0.2139\tClassification Loss: 1.7582\n","Train Epoch: 5 [9920/110534 (9%)]\tAll Loss: 1.7187\tTriple Loss(1): 0.0619\tClassification Loss: 1.5949\n","Train Epoch: 5 [10080/110534 (9%)]\tAll Loss: 1.7832\tTriple Loss(1): 0.3960\tClassification Loss: 0.9911\n","Train Epoch: 5 [10240/110534 (9%)]\tAll Loss: 1.9264\tTriple Loss(1): 0.0374\tClassification Loss: 1.8516\n","Train Epoch: 5 [10400/110534 (9%)]\tAll Loss: 1.7317\tTriple Loss(1): 0.0393\tClassification Loss: 1.6530\n","Train Epoch: 5 [10560/110534 (10%)]\tAll Loss: 2.2040\tTriple Loss(1): 0.2849\tClassification Loss: 1.6343\n","Train Epoch: 5 [10720/110534 (10%)]\tAll Loss: 1.3813\tTriple Loss(1): 0.1251\tClassification Loss: 1.1311\n","Train Epoch: 5 [10880/110534 (10%)]\tAll Loss: 1.4055\tTriple Loss(0): 0.0000\tClassification Loss: 1.4055\n","Train Epoch: 5 [11040/110534 (10%)]\tAll Loss: 2.3859\tTriple Loss(1): 0.0750\tClassification Loss: 2.2359\n","\n","Test set: Average loss: 1.4882, Accuracy: 282/480 (59%)\n","\n","Train Epoch: 5 [11200/110534 (10%)]\tAll Loss: 2.1727\tTriple Loss(1): 0.0000\tClassification Loss: 2.1727\n","Train Epoch: 5 [11360/110534 (10%)]\tAll Loss: 1.7812\tTriple Loss(1): 0.0496\tClassification Loss: 1.6821\n","Train Epoch: 5 [11520/110534 (10%)]\tAll Loss: 2.7070\tTriple Loss(1): 0.3656\tClassification Loss: 1.9758\n","Train Epoch: 5 [11680/110534 (11%)]\tAll Loss: 1.6755\tTriple Loss(1): 0.2016\tClassification Loss: 1.2724\n","Train Epoch: 5 [11840/110534 (11%)]\tAll Loss: 2.3661\tTriple Loss(1): 0.1984\tClassification Loss: 1.9693\n","Train Epoch: 5 [12000/110534 (11%)]\tAll Loss: 2.2357\tTriple Loss(1): 0.0899\tClassification Loss: 2.0559\n","Train Epoch: 5 [12160/110534 (11%)]\tAll Loss: 1.7756\tTriple Loss(1): 0.2260\tClassification Loss: 1.3236\n","Train Epoch: 5 [12320/110534 (11%)]\tAll Loss: 1.5232\tTriple Loss(1): 0.0746\tClassification Loss: 1.3740\n","Train Epoch: 5 [12480/110534 (11%)]\tAll Loss: 1.8680\tTriple Loss(1): 0.2213\tClassification Loss: 1.4253\n","Train Epoch: 5 [12640/110534 (11%)]\tAll Loss: 2.0106\tTriple Loss(1): 0.2142\tClassification Loss: 1.5823\n","\n","Test set: Average loss: 1.5070, Accuracy: 286/480 (60%)\n","\n","Train Epoch: 5 [12800/110534 (12%)]\tAll Loss: 2.3058\tTriple Loss(1): 0.4783\tClassification Loss: 1.3492\n","Train Epoch: 5 [12960/110534 (12%)]\tAll Loss: 1.9923\tTriple Loss(1): 0.1920\tClassification Loss: 1.6083\n","Train Epoch: 5 [13120/110534 (12%)]\tAll Loss: 2.4127\tTriple Loss(1): 0.4884\tClassification Loss: 1.4358\n","Train Epoch: 5 [13280/110534 (12%)]\tAll Loss: 1.7159\tTriple Loss(0): 0.0000\tClassification Loss: 1.7159\n","Train Epoch: 5 [13440/110534 (12%)]\tAll Loss: 2.0780\tTriple Loss(1): 0.1182\tClassification Loss: 1.8416\n","Train Epoch: 5 [13600/110534 (12%)]\tAll Loss: 1.5825\tTriple Loss(1): 0.0725\tClassification Loss: 1.4375\n","Train Epoch: 5 [13760/110534 (12%)]\tAll Loss: 1.6817\tTriple Loss(1): 0.0056\tClassification Loss: 1.6705\n","Train Epoch: 5 [13920/110534 (13%)]\tAll Loss: 1.9344\tTriple Loss(1): 0.2667\tClassification Loss: 1.4011\n","Train Epoch: 5 [14080/110534 (13%)]\tAll Loss: 3.6220\tTriple Loss(0): 1.0998\tClassification Loss: 1.4225\n","Train Epoch: 5 [14240/110534 (13%)]\tAll Loss: 1.2368\tTriple Loss(1): 0.0059\tClassification Loss: 1.2250\n","\n","Test set: Average loss: 1.5198, Accuracy: 282/480 (59%)\n","\n","Train Epoch: 5 [14400/110534 (13%)]\tAll Loss: 2.2726\tTriple Loss(0): 0.0000\tClassification Loss: 2.2726\n","Train Epoch: 5 [14560/110534 (13%)]\tAll Loss: 1.5330\tTriple Loss(1): 0.1751\tClassification Loss: 1.1828\n","Train Epoch: 5 [14720/110534 (13%)]\tAll Loss: 1.5202\tTriple Loss(0): 0.0000\tClassification Loss: 1.5202\n","Train Epoch: 5 [14880/110534 (13%)]\tAll Loss: 2.1245\tTriple Loss(1): 0.2022\tClassification Loss: 1.7201\n","Train Epoch: 5 [15040/110534 (14%)]\tAll Loss: 1.7795\tTriple Loss(1): 0.2564\tClassification Loss: 1.2668\n","Train Epoch: 5 [15200/110534 (14%)]\tAll Loss: 2.2899\tTriple Loss(1): 0.1413\tClassification Loss: 2.0073\n","Train Epoch: 5 [15360/110534 (14%)]\tAll Loss: 1.6442\tTriple Loss(1): 0.2528\tClassification Loss: 1.1387\n","Train Epoch: 5 [15520/110534 (14%)]\tAll Loss: 3.2465\tTriple Loss(1): 0.6941\tClassification Loss: 1.8584\n","Train Epoch: 5 [15680/110534 (14%)]\tAll Loss: 1.7577\tTriple Loss(0): 0.0000\tClassification Loss: 1.7577\n","Train Epoch: 5 [15840/110534 (14%)]\tAll Loss: 1.8471\tTriple Loss(0): 0.0000\tClassification Loss: 1.8471\n","\n","Test set: Average loss: 1.5205, Accuracy: 272/480 (57%)\n","\n","Train Epoch: 5 [16000/110534 (14%)]\tAll Loss: 1.7368\tTriple Loss(1): 0.1214\tClassification Loss: 1.4941\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_5_1000.pth.tar\n","Train Epoch: 5 [16160/110534 (15%)]\tAll Loss: 1.2452\tTriple Loss(1): 0.1671\tClassification Loss: 0.9109\n","Train Epoch: 5 [16320/110534 (15%)]\tAll Loss: 0.8658\tTriple Loss(0): 0.0000\tClassification Loss: 0.8658\n","Train Epoch: 5 [16480/110534 (15%)]\tAll Loss: 2.4573\tTriple Loss(1): 0.5284\tClassification Loss: 1.4005\n","Train Epoch: 5 [16640/110534 (15%)]\tAll Loss: 1.4747\tTriple Loss(1): 0.0044\tClassification Loss: 1.4659\n","Train Epoch: 5 [16800/110534 (15%)]\tAll Loss: 1.4528\tTriple Loss(1): 0.0458\tClassification Loss: 1.3611\n","Train Epoch: 5 [16960/110534 (15%)]\tAll Loss: 2.1234\tTriple Loss(1): 0.0000\tClassification Loss: 2.1234\n","Train Epoch: 5 [17120/110534 (15%)]\tAll Loss: 1.8326\tTriple Loss(1): 0.0659\tClassification Loss: 1.7008\n","Train Epoch: 5 [17280/110534 (16%)]\tAll Loss: 1.9253\tTriple Loss(0): 0.0000\tClassification Loss: 1.9253\n","Train Epoch: 5 [17440/110534 (16%)]\tAll Loss: 1.4359\tTriple Loss(0): 0.0000\tClassification Loss: 1.4359\n","\n","Test set: Average loss: 1.5021, Accuracy: 279/480 (58%)\n","\n","Train Epoch: 5 [17600/110534 (16%)]\tAll Loss: 1.6604\tTriple Loss(1): 0.3089\tClassification Loss: 1.0425\n","Train Epoch: 5 [17760/110534 (16%)]\tAll Loss: 1.9144\tTriple Loss(1): 0.1592\tClassification Loss: 1.5960\n","Train Epoch: 5 [17920/110534 (16%)]\tAll Loss: 1.9132\tTriple Loss(1): 0.1929\tClassification Loss: 1.5275\n","Train Epoch: 5 [18080/110534 (16%)]\tAll Loss: 1.3277\tTriple Loss(0): 0.0000\tClassification Loss: 1.3277\n","Train Epoch: 5 [18240/110534 (17%)]\tAll Loss: 1.9552\tTriple Loss(1): 0.2424\tClassification Loss: 1.4705\n","Train Epoch: 5 [18400/110534 (17%)]\tAll Loss: 2.8600\tTriple Loss(1): 0.5725\tClassification Loss: 1.7149\n","Train Epoch: 5 [18560/110534 (17%)]\tAll Loss: 2.4702\tTriple Loss(1): 0.1380\tClassification Loss: 2.1942\n","Train Epoch: 5 [18720/110534 (17%)]\tAll Loss: 1.8968\tTriple Loss(1): 0.0685\tClassification Loss: 1.7598\n","Train Epoch: 5 [18880/110534 (17%)]\tAll Loss: 1.5619\tTriple Loss(1): 0.1513\tClassification Loss: 1.2593\n","Train Epoch: 5 [19040/110534 (17%)]\tAll Loss: 0.8464\tTriple Loss(1): 0.0228\tClassification Loss: 0.8008\n","\n","Test set: Average loss: 1.5026, Accuracy: 275/480 (57%)\n","\n","Train Epoch: 5 [19200/110534 (17%)]\tAll Loss: 2.1472\tTriple Loss(1): 0.1922\tClassification Loss: 1.7629\n","Train Epoch: 5 [19360/110534 (18%)]\tAll Loss: 2.2394\tTriple Loss(1): 0.3040\tClassification Loss: 1.6314\n","Train Epoch: 5 [19520/110534 (18%)]\tAll Loss: 1.7490\tTriple Loss(1): 0.0744\tClassification Loss: 1.6001\n","Train Epoch: 5 [19680/110534 (18%)]\tAll Loss: 1.3465\tTriple Loss(1): 0.0000\tClassification Loss: 1.3465\n","Train Epoch: 5 [19840/110534 (18%)]\tAll Loss: 1.5700\tTriple Loss(1): 0.0813\tClassification Loss: 1.4074\n","Train Epoch: 5 [20000/110534 (18%)]\tAll Loss: 1.5929\tTriple Loss(0): 0.0000\tClassification Loss: 1.5929\n","Train Epoch: 5 [20160/110534 (18%)]\tAll Loss: 1.8165\tTriple Loss(0): 0.0000\tClassification Loss: 1.8165\n","Train Epoch: 5 [20320/110534 (18%)]\tAll Loss: 2.1322\tTriple Loss(1): 0.1350\tClassification Loss: 1.8622\n","Train Epoch: 5 [20480/110534 (19%)]\tAll Loss: 1.4652\tTriple Loss(1): 0.0915\tClassification Loss: 1.2823\n","Train Epoch: 5 [20640/110534 (19%)]\tAll Loss: 1.4405\tTriple Loss(1): 0.0192\tClassification Loss: 1.4021\n","\n","Test set: Average loss: 1.4971, Accuracy: 280/480 (58%)\n","\n","Train Epoch: 5 [20800/110534 (19%)]\tAll Loss: 1.4377\tTriple Loss(1): 0.0608\tClassification Loss: 1.3162\n","Train Epoch: 5 [20960/110534 (19%)]\tAll Loss: 1.3898\tTriple Loss(1): 0.0406\tClassification Loss: 1.3087\n","Train Epoch: 5 [21120/110534 (19%)]\tAll Loss: 1.6405\tTriple Loss(1): 0.2341\tClassification Loss: 1.1724\n","Train Epoch: 5 [21280/110534 (19%)]\tAll Loss: 1.8706\tTriple Loss(1): 0.1362\tClassification Loss: 1.5982\n","Train Epoch: 5 [21440/110534 (19%)]\tAll Loss: 1.6828\tTriple Loss(1): 0.2445\tClassification Loss: 1.1939\n","Train Epoch: 5 [21600/110534 (20%)]\tAll Loss: 2.5567\tTriple Loss(1): 0.2313\tClassification Loss: 2.0942\n","Train Epoch: 5 [21760/110534 (20%)]\tAll Loss: 4.3814\tTriple Loss(0): 1.3718\tClassification Loss: 1.6379\n","Train Epoch: 5 [21920/110534 (20%)]\tAll Loss: 1.5667\tTriple Loss(1): 0.2719\tClassification Loss: 1.0228\n","Train Epoch: 5 [22080/110534 (20%)]\tAll Loss: 1.4467\tTriple Loss(1): 0.0000\tClassification Loss: 1.4467\n","Train Epoch: 5 [22240/110534 (20%)]\tAll Loss: 1.2923\tTriple Loss(0): 0.0000\tClassification Loss: 1.2923\n","\n","Test set: Average loss: 1.4918, Accuracy: 277/480 (58%)\n","\n","Train Epoch: 5 [22400/110534 (20%)]\tAll Loss: 1.3464\tTriple Loss(1): 0.1001\tClassification Loss: 1.1463\n","Train Epoch: 5 [22560/110534 (20%)]\tAll Loss: 1.7588\tTriple Loss(0): 0.0000\tClassification Loss: 1.7588\n","Train Epoch: 5 [22720/110534 (21%)]\tAll Loss: 2.1118\tTriple Loss(1): 0.0433\tClassification Loss: 2.0251\n","Train Epoch: 5 [22880/110534 (21%)]\tAll Loss: 1.9085\tTriple Loss(1): 0.2891\tClassification Loss: 1.3303\n","Train Epoch: 5 [23040/110534 (21%)]\tAll Loss: 1.4058\tTriple Loss(1): 0.2257\tClassification Loss: 0.9543\n","Train Epoch: 5 [23200/110534 (21%)]\tAll Loss: 2.2413\tTriple Loss(1): 0.2028\tClassification Loss: 1.8358\n","Train Epoch: 5 [23360/110534 (21%)]\tAll Loss: 1.0999\tTriple Loss(1): 0.0000\tClassification Loss: 1.0999\n","Train Epoch: 5 [23520/110534 (21%)]\tAll Loss: 1.8506\tTriple Loss(1): 0.1590\tClassification Loss: 1.5327\n","Train Epoch: 5 [23680/110534 (21%)]\tAll Loss: 2.3419\tTriple Loss(1): 0.2606\tClassification Loss: 1.8206\n","Train Epoch: 5 [23840/110534 (22%)]\tAll Loss: 1.7716\tTriple Loss(1): 0.1846\tClassification Loss: 1.4024\n","\n","Test set: Average loss: 1.5155, Accuracy: 277/480 (58%)\n","\n","Train Epoch: 5 [24000/110534 (22%)]\tAll Loss: 1.7567\tTriple Loss(1): 0.3316\tClassification Loss: 1.0935\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_5_1500.pth.tar\n","Train Epoch: 5 [24160/110534 (22%)]\tAll Loss: 1.7286\tTriple Loss(1): 0.1909\tClassification Loss: 1.3468\n","Train Epoch: 5 [24320/110534 (22%)]\tAll Loss: 1.7331\tTriple Loss(1): 0.0755\tClassification Loss: 1.5821\n","Train Epoch: 5 [24480/110534 (22%)]\tAll Loss: 1.5951\tTriple Loss(0): 0.0000\tClassification Loss: 1.5951\n","Train Epoch: 5 [24640/110534 (22%)]\tAll Loss: 2.1959\tTriple Loss(1): 0.1066\tClassification Loss: 1.9827\n","Train Epoch: 5 [24800/110534 (22%)]\tAll Loss: 2.8061\tTriple Loss(1): 0.7221\tClassification Loss: 1.3618\n","Train Epoch: 5 [24960/110534 (23%)]\tAll Loss: 1.9719\tTriple Loss(1): 0.0821\tClassification Loss: 1.8076\n","Train Epoch: 5 [25120/110534 (23%)]\tAll Loss: 1.4991\tTriple Loss(1): 0.1380\tClassification Loss: 1.2231\n","Train Epoch: 5 [25280/110534 (23%)]\tAll Loss: 1.6888\tTriple Loss(1): 0.0984\tClassification Loss: 1.4920\n","Train Epoch: 5 [25440/110534 (23%)]\tAll Loss: 1.4455\tTriple Loss(1): 0.0068\tClassification Loss: 1.4319\n","\n","Test set: Average loss: 1.5312, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 5 [25600/110534 (23%)]\tAll Loss: 1.0605\tTriple Loss(1): 0.0000\tClassification Loss: 1.0605\n","Train Epoch: 5 [25760/110534 (23%)]\tAll Loss: 1.5156\tTriple Loss(1): 0.2018\tClassification Loss: 1.1119\n","Train Epoch: 5 [25920/110534 (23%)]\tAll Loss: 1.4138\tTriple Loss(1): 0.0000\tClassification Loss: 1.4138\n","Train Epoch: 5 [26080/110534 (24%)]\tAll Loss: 0.9524\tTriple Loss(1): 0.0645\tClassification Loss: 0.8234\n","Train Epoch: 5 [26240/110534 (24%)]\tAll Loss: 1.3539\tTriple Loss(1): 0.0000\tClassification Loss: 1.3539\n","Train Epoch: 5 [26400/110534 (24%)]\tAll Loss: 1.9379\tTriple Loss(1): 0.1844\tClassification Loss: 1.5692\n","Train Epoch: 5 [26560/110534 (24%)]\tAll Loss: 2.7877\tTriple Loss(1): 0.5237\tClassification Loss: 1.7403\n","Train Epoch: 5 [26720/110534 (24%)]\tAll Loss: 1.7553\tTriple Loss(1): 0.0422\tClassification Loss: 1.6709\n","Train Epoch: 5 [26880/110534 (24%)]\tAll Loss: 1.9015\tTriple Loss(1): 0.2492\tClassification Loss: 1.4030\n","Train Epoch: 5 [27040/110534 (24%)]\tAll Loss: 1.7118\tTriple Loss(1): 0.1401\tClassification Loss: 1.4316\n","\n","Test set: Average loss: 1.5283, Accuracy: 274/480 (57%)\n","\n","Train Epoch: 5 [27200/110534 (25%)]\tAll Loss: 1.1016\tTriple Loss(1): 0.1669\tClassification Loss: 0.7679\n","Train Epoch: 5 [27360/110534 (25%)]\tAll Loss: 1.1667\tTriple Loss(0): 0.0000\tClassification Loss: 1.1667\n","Train Epoch: 5 [27520/110534 (25%)]\tAll Loss: 1.1683\tTriple Loss(1): 0.0851\tClassification Loss: 0.9980\n","Train Epoch: 5 [27680/110534 (25%)]\tAll Loss: 1.6740\tTriple Loss(1): 0.0944\tClassification Loss: 1.4852\n","Train Epoch: 5 [27840/110534 (25%)]\tAll Loss: 1.2936\tTriple Loss(1): 0.0710\tClassification Loss: 1.1516\n","Train Epoch: 5 [28000/110534 (25%)]\tAll Loss: 1.9716\tTriple Loss(1): 0.3682\tClassification Loss: 1.2351\n","Train Epoch: 5 [28160/110534 (25%)]\tAll Loss: 1.4750\tTriple Loss(0): 0.0000\tClassification Loss: 1.4750\n","Train Epoch: 5 [28320/110534 (26%)]\tAll Loss: 2.2855\tTriple Loss(1): 0.0857\tClassification Loss: 2.1141\n","Train Epoch: 5 [28480/110534 (26%)]\tAll Loss: 1.3197\tTriple Loss(1): 0.0000\tClassification Loss: 1.3197\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oFYsssLDaSDJ","colab_type":"code","outputId":"c8b0c1e4-9039-4b20-cef8-c971efa437aa","executionInfo":{"status":"ok","timestamp":1584954545220,"user_tz":-300,"elapsed":3922822,"user":{"displayName":"Muhammad Ali","photoUrl":"","userId":"15673831022739340207"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# model_5_1500.pth.tar. Freeze=True. LR=0.03\n","! python train.py"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n","  \"please use transforms.Resize instead.\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:704: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n","  \"please use transforms.RandomResizedCrop instead.\")\n","Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n","100% 97.8M/97.8M [00:01<00:00, 62.9MB/s]\n","Loading model model_5_1500.pth.tar\n","/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n","train.py:132: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(data, volatile=True), Variable(target)\n","\n","Test set: Average loss: 1.4784, Accuracy: 280/480 (58%)\n","\n","Train Epoch: 1 [0/110534 (0%)]\tAll Loss: 2.1485\tTriple Loss(1): 0.2993\tClassification Loss: 1.5500\n","Train Epoch: 1 [160/110534 (0%)]\tAll Loss: 1.5572\tTriple Loss(0): 0.0000\tClassification Loss: 1.5572\n","Train Epoch: 1 [320/110534 (0%)]\tAll Loss: 2.2011\tTriple Loss(1): 0.3139\tClassification Loss: 1.5733\n","Train Epoch: 1 [480/110534 (0%)]\tAll Loss: 1.4629\tTriple Loss(1): 0.2042\tClassification Loss: 1.0544\n","Train Epoch: 1 [640/110534 (1%)]\tAll Loss: 0.8739\tTriple Loss(1): 0.0252\tClassification Loss: 0.8235\n","Train Epoch: 1 [800/110534 (1%)]\tAll Loss: 2.8054\tTriple Loss(1): 0.4463\tClassification Loss: 1.9128\n","Train Epoch: 1 [960/110534 (1%)]\tAll Loss: 1.1585\tTriple Loss(0): 0.0000\tClassification Loss: 1.1585\n","Train Epoch: 1 [1120/110534 (1%)]\tAll Loss: 1.9586\tTriple Loss(1): 0.0987\tClassification Loss: 1.7612\n","Train Epoch: 1 [1280/110534 (1%)]\tAll Loss: 1.3543\tTriple Loss(1): 0.0641\tClassification Loss: 1.2262\n","Train Epoch: 1 [1440/110534 (1%)]\tAll Loss: 1.9506\tTriple Loss(1): 0.1715\tClassification Loss: 1.6075\n","\n","Test set: Average loss: 1.4848, Accuracy: 284/480 (59%)\n","\n","Train Epoch: 1 [1600/110534 (1%)]\tAll Loss: 2.2874\tTriple Loss(1): 0.5040\tClassification Loss: 1.2794\n","Train Epoch: 1 [1760/110534 (2%)]\tAll Loss: 2.0542\tTriple Loss(0): 0.0000\tClassification Loss: 2.0542\n","Train Epoch: 1 [1920/110534 (2%)]\tAll Loss: 1.7936\tTriple Loss(1): 0.1969\tClassification Loss: 1.3998\n","Train Epoch: 1 [2080/110534 (2%)]\tAll Loss: 1.9291\tTriple Loss(1): 0.0973\tClassification Loss: 1.7345\n","Train Epoch: 1 [2240/110534 (2%)]\tAll Loss: 1.7137\tTriple Loss(1): 0.0563\tClassification Loss: 1.6012\n","Train Epoch: 1 [2400/110534 (2%)]\tAll Loss: 1.3432\tTriple Loss(0): 0.0000\tClassification Loss: 1.3432\n","Train Epoch: 1 [2560/110534 (2%)]\tAll Loss: 1.3121\tTriple Loss(0): 0.0000\tClassification Loss: 1.3121\n","Train Epoch: 1 [2720/110534 (2%)]\tAll Loss: 1.6070\tTriple Loss(1): 0.1249\tClassification Loss: 1.3571\n","Train Epoch: 1 [2880/110534 (3%)]\tAll Loss: 1.9302\tTriple Loss(1): 0.0000\tClassification Loss: 1.9302\n","Train Epoch: 1 [3040/110534 (3%)]\tAll Loss: 1.9748\tTriple Loss(1): 0.1526\tClassification Loss: 1.6696\n","\n","Test set: Average loss: 1.4676, Accuracy: 278/480 (58%)\n","\n","Train Epoch: 1 [3200/110534 (3%)]\tAll Loss: 2.5632\tTriple Loss(1): 0.3726\tClassification Loss: 1.8181\n","Train Epoch: 1 [3360/110534 (3%)]\tAll Loss: 1.6722\tTriple Loss(0): 0.0000\tClassification Loss: 1.6722\n","Train Epoch: 1 [3520/110534 (3%)]\tAll Loss: 1.6611\tTriple Loss(1): 0.0245\tClassification Loss: 1.6121\n","Train Epoch: 1 [3680/110534 (3%)]\tAll Loss: 1.4897\tTriple Loss(0): 0.0000\tClassification Loss: 1.4897\n","Train Epoch: 1 [3840/110534 (3%)]\tAll Loss: 1.4168\tTriple Loss(1): 0.0571\tClassification Loss: 1.3026\n","Train Epoch: 1 [4000/110534 (4%)]\tAll Loss: 2.1130\tTriple Loss(1): 0.1193\tClassification Loss: 1.8744\n","Train Epoch: 1 [4160/110534 (4%)]\tAll Loss: 1.8125\tTriple Loss(1): 0.0955\tClassification Loss: 1.6216\n","Train Epoch: 1 [4320/110534 (4%)]\tAll Loss: 5.3454\tTriple Loss(0): 2.2449\tClassification Loss: 0.8556\n","Train Epoch: 1 [4480/110534 (4%)]\tAll Loss: 2.0839\tTriple Loss(1): 0.2863\tClassification Loss: 1.5113\n","Train Epoch: 1 [4640/110534 (4%)]\tAll Loss: 1.6224\tTriple Loss(1): 0.2456\tClassification Loss: 1.1312\n","\n","Test set: Average loss: 1.4687, Accuracy: 279/480 (58%)\n","\n","Train Epoch: 1 [4800/110534 (4%)]\tAll Loss: 1.4135\tTriple Loss(1): 0.0139\tClassification Loss: 1.3856\n","Train Epoch: 1 [4960/110534 (4%)]\tAll Loss: 2.1947\tTriple Loss(1): 0.2180\tClassification Loss: 1.7586\n","Train Epoch: 1 [5120/110534 (5%)]\tAll Loss: 2.0731\tTriple Loss(1): 0.3403\tClassification Loss: 1.3924\n","Train Epoch: 1 [5280/110534 (5%)]\tAll Loss: 1.2558\tTriple Loss(1): 0.0465\tClassification Loss: 1.1628\n","Train Epoch: 1 [5440/110534 (5%)]\tAll Loss: 1.3919\tTriple Loss(0): 0.0000\tClassification Loss: 1.3919\n","Train Epoch: 1 [5600/110534 (5%)]\tAll Loss: 2.7083\tTriple Loss(1): 0.4737\tClassification Loss: 1.7609\n","Train Epoch: 1 [5760/110534 (5%)]\tAll Loss: 2.4983\tTriple Loss(1): 0.3686\tClassification Loss: 1.7612\n","Train Epoch: 1 [5920/110534 (5%)]\tAll Loss: 2.2455\tTriple Loss(0): 0.5020\tClassification Loss: 1.2415\n","Train Epoch: 1 [6080/110534 (6%)]\tAll Loss: 2.5902\tTriple Loss(1): 0.3992\tClassification Loss: 1.7918\n","Train Epoch: 1 [6240/110534 (6%)]\tAll Loss: 1.5443\tTriple Loss(1): 0.0369\tClassification Loss: 1.4706\n","\n","Test set: Average loss: 1.4641, Accuracy: 282/480 (59%)\n","\n","Train Epoch: 1 [6400/110534 (6%)]\tAll Loss: 1.7944\tTriple Loss(1): 0.0691\tClassification Loss: 1.6561\n","Train Epoch: 1 [6560/110534 (6%)]\tAll Loss: 2.3275\tTriple Loss(1): 0.4020\tClassification Loss: 1.5236\n","Train Epoch: 1 [6720/110534 (6%)]\tAll Loss: 1.5509\tTriple Loss(0): 0.0000\tClassification Loss: 1.5509\n","Train Epoch: 1 [6880/110534 (6%)]\tAll Loss: 1.7928\tTriple Loss(1): 0.2394\tClassification Loss: 1.3141\n","Train Epoch: 1 [7040/110534 (6%)]\tAll Loss: 2.6062\tTriple Loss(1): 0.4534\tClassification Loss: 1.6994\n","Train Epoch: 1 [7200/110534 (7%)]\tAll Loss: 1.9709\tTriple Loss(1): 0.1490\tClassification Loss: 1.6729\n","Train Epoch: 1 [7360/110534 (7%)]\tAll Loss: 2.1860\tTriple Loss(1): 0.2535\tClassification Loss: 1.6791\n","Train Epoch: 1 [7520/110534 (7%)]\tAll Loss: 1.7795\tTriple Loss(1): 0.1393\tClassification Loss: 1.5010\n","Train Epoch: 1 [7680/110534 (7%)]\tAll Loss: 1.8702\tTriple Loss(1): 0.1276\tClassification Loss: 1.6149\n","Train Epoch: 1 [7840/110534 (7%)]\tAll Loss: 1.9531\tTriple Loss(1): 0.3214\tClassification Loss: 1.3104\n","\n","Test set: Average loss: 1.4802, Accuracy: 278/480 (58%)\n","\n","Train Epoch: 1 [8000/110534 (7%)]\tAll Loss: 2.2457\tTriple Loss(1): 0.3598\tClassification Loss: 1.5261\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_500.pth.tar\n","Train Epoch: 1 [8160/110534 (7%)]\tAll Loss: 2.4300\tTriple Loss(1): 0.6691\tClassification Loss: 1.0918\n","Train Epoch: 1 [8320/110534 (8%)]\tAll Loss: 1.2499\tTriple Loss(0): 0.0000\tClassification Loss: 1.2499\n","Train Epoch: 1 [8480/110534 (8%)]\tAll Loss: 2.7319\tTriple Loss(1): 0.3658\tClassification Loss: 2.0004\n","Train Epoch: 1 [8640/110534 (8%)]\tAll Loss: 1.2924\tTriple Loss(1): 0.0819\tClassification Loss: 1.1286\n","Train Epoch: 1 [8800/110534 (8%)]\tAll Loss: 2.1268\tTriple Loss(1): 0.2611\tClassification Loss: 1.6046\n","Train Epoch: 1 [8960/110534 (8%)]\tAll Loss: 1.2018\tTriple Loss(1): 0.0210\tClassification Loss: 1.1598\n","Train Epoch: 1 [9120/110534 (8%)]\tAll Loss: 2.1419\tTriple Loss(1): 0.4273\tClassification Loss: 1.2874\n","Train Epoch: 1 [9280/110534 (8%)]\tAll Loss: 1.3895\tTriple Loss(1): 0.2315\tClassification Loss: 0.9265\n","Train Epoch: 1 [9440/110534 (9%)]\tAll Loss: 2.4712\tTriple Loss(1): 0.1975\tClassification Loss: 2.0761\n","\n","Test set: Average loss: 1.4743, Accuracy: 281/480 (59%)\n","\n","Train Epoch: 1 [9600/110534 (9%)]\tAll Loss: 2.6672\tTriple Loss(1): 0.3993\tClassification Loss: 1.8687\n","Train Epoch: 1 [9760/110534 (9%)]\tAll Loss: 2.5614\tTriple Loss(1): 0.4342\tClassification Loss: 1.6930\n","Train Epoch: 1 [9920/110534 (9%)]\tAll Loss: 1.7607\tTriple Loss(1): 0.3231\tClassification Loss: 1.1145\n","Train Epoch: 1 [10080/110534 (9%)]\tAll Loss: 2.4324\tTriple Loss(1): 0.2577\tClassification Loss: 1.9170\n","Train Epoch: 1 [10240/110534 (9%)]\tAll Loss: 1.5599\tTriple Loss(1): 0.0797\tClassification Loss: 1.4004\n","Train Epoch: 1 [10400/110534 (9%)]\tAll Loss: 1.3439\tTriple Loss(1): 0.0972\tClassification Loss: 1.1494\n","Train Epoch: 1 [10560/110534 (10%)]\tAll Loss: 1.5434\tTriple Loss(1): 0.0224\tClassification Loss: 1.4986\n","Train Epoch: 1 [10720/110534 (10%)]\tAll Loss: 1.2186\tTriple Loss(0): 0.0000\tClassification Loss: 1.2186\n","Train Epoch: 1 [10880/110534 (10%)]\tAll Loss: 1.8394\tTriple Loss(1): 0.1979\tClassification Loss: 1.4437\n","Train Epoch: 1 [11040/110534 (10%)]\tAll Loss: 1.9028\tTriple Loss(0): 0.0000\tClassification Loss: 1.9028\n","\n","Test set: Average loss: 1.4741, Accuracy: 282/480 (59%)\n","\n","Train Epoch: 1 [11200/110534 (10%)]\tAll Loss: 1.5566\tTriple Loss(1): 0.1851\tClassification Loss: 1.1864\n","Train Epoch: 1 [11360/110534 (10%)]\tAll Loss: 1.8884\tTriple Loss(1): 0.2180\tClassification Loss: 1.4524\n","Train Epoch: 1 [11520/110534 (10%)]\tAll Loss: 2.2547\tTriple Loss(1): 0.1081\tClassification Loss: 2.0386\n","Train Epoch: 1 [11680/110534 (11%)]\tAll Loss: 1.7746\tTriple Loss(1): 0.0000\tClassification Loss: 1.7746\n","Train Epoch: 1 [11840/110534 (11%)]\tAll Loss: 1.6530\tTriple Loss(1): 0.2216\tClassification Loss: 1.2098\n","Train Epoch: 1 [12000/110534 (11%)]\tAll Loss: 1.7803\tTriple Loss(1): 0.0000\tClassification Loss: 1.7803\n","Train Epoch: 1 [12160/110534 (11%)]\tAll Loss: 2.2128\tTriple Loss(1): 0.2627\tClassification Loss: 1.6874\n","Train Epoch: 1 [12320/110534 (11%)]\tAll Loss: 2.0811\tTriple Loss(1): 0.0724\tClassification Loss: 1.9363\n","Train Epoch: 1 [12480/110534 (11%)]\tAll Loss: 1.9745\tTriple Loss(1): 0.1601\tClassification Loss: 1.6544\n","Train Epoch: 1 [12640/110534 (11%)]\tAll Loss: 2.1191\tTriple Loss(1): 0.2349\tClassification Loss: 1.6493\n","\n","Test set: Average loss: 1.4817, Accuracy: 290/480 (60%)\n","\n","Train Epoch: 1 [12800/110534 (12%)]\tAll Loss: 1.2917\tTriple Loss(0): 0.0000\tClassification Loss: 1.2917\n","Train Epoch: 1 [12960/110534 (12%)]\tAll Loss: 1.6510\tTriple Loss(1): 0.0193\tClassification Loss: 1.6124\n","Train Epoch: 1 [13120/110534 (12%)]\tAll Loss: 2.8667\tTriple Loss(1): 0.7331\tClassification Loss: 1.4006\n","Train Epoch: 1 [13280/110534 (12%)]\tAll Loss: 1.1861\tTriple Loss(1): 0.0934\tClassification Loss: 0.9992\n","Train Epoch: 1 [13440/110534 (12%)]\tAll Loss: 1.5811\tTriple Loss(0): 0.0000\tClassification Loss: 1.5811\n","Train Epoch: 1 [13600/110534 (12%)]\tAll Loss: 2.0700\tTriple Loss(1): 0.1368\tClassification Loss: 1.7965\n","Train Epoch: 1 [13760/110534 (12%)]\tAll Loss: 1.3364\tTriple Loss(1): 0.0000\tClassification Loss: 1.3364\n","Train Epoch: 1 [13920/110534 (13%)]\tAll Loss: 1.8641\tTriple Loss(1): 0.1542\tClassification Loss: 1.5558\n","Train Epoch: 1 [14080/110534 (13%)]\tAll Loss: 2.4826\tTriple Loss(1): 0.0000\tClassification Loss: 2.4826\n","Train Epoch: 1 [14240/110534 (13%)]\tAll Loss: 2.1868\tTriple Loss(1): 0.1343\tClassification Loss: 1.9182\n","\n","Test set: Average loss: 1.4519, Accuracy: 289/480 (60%)\n","\n","Train Epoch: 1 [14400/110534 (13%)]\tAll Loss: 7.3120\tTriple Loss(0): 2.9192\tClassification Loss: 1.4735\n","Train Epoch: 1 [14560/110534 (13%)]\tAll Loss: 1.7987\tTriple Loss(0): 0.0000\tClassification Loss: 1.7987\n","Train Epoch: 1 [14720/110534 (13%)]\tAll Loss: 1.7948\tTriple Loss(0): 0.0000\tClassification Loss: 1.7948\n","Train Epoch: 1 [14880/110534 (13%)]\tAll Loss: 2.3041\tTriple Loss(1): 0.3911\tClassification Loss: 1.5219\n","Train Epoch: 1 [15040/110534 (14%)]\tAll Loss: 2.4737\tTriple Loss(1): 0.4331\tClassification Loss: 1.6074\n","Train Epoch: 1 [15200/110534 (14%)]\tAll Loss: 1.7377\tTriple Loss(1): 0.0913\tClassification Loss: 1.5552\n","Train Epoch: 1 [15360/110534 (14%)]\tAll Loss: 1.8896\tTriple Loss(1): 0.1160\tClassification Loss: 1.6577\n","Train Epoch: 1 [15520/110534 (14%)]\tAll Loss: 1.2980\tTriple Loss(1): 0.0470\tClassification Loss: 1.2040\n","Train Epoch: 1 [15680/110534 (14%)]\tAll Loss: 1.0194\tTriple Loss(0): 0.0000\tClassification Loss: 1.0194\n","Train Epoch: 1 [15840/110534 (14%)]\tAll Loss: 2.4260\tTriple Loss(1): 0.4589\tClassification Loss: 1.5081\n","\n","Test set: Average loss: 1.4787, Accuracy: 282/480 (59%)\n","\n","Train Epoch: 1 [16000/110534 (14%)]\tAll Loss: 1.5371\tTriple Loss(1): 0.3083\tClassification Loss: 0.9206\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_1000.pth.tar\n","Train Epoch: 1 [16160/110534 (15%)]\tAll Loss: 2.4289\tTriple Loss(1): 0.4117\tClassification Loss: 1.6055\n","Train Epoch: 1 [16320/110534 (15%)]\tAll Loss: 2.0229\tTriple Loss(1): 0.4199\tClassification Loss: 1.1831\n","Train Epoch: 1 [16480/110534 (15%)]\tAll Loss: 1.0554\tTriple Loss(0): 0.0000\tClassification Loss: 1.0554\n","Train Epoch: 1 [16640/110534 (15%)]\tAll Loss: 2.4721\tTriple Loss(1): 0.4556\tClassification Loss: 1.5609\n","Train Epoch: 1 [16800/110534 (15%)]\tAll Loss: 1.4633\tTriple Loss(0): 0.0000\tClassification Loss: 1.4633\n","Train Epoch: 1 [16960/110534 (15%)]\tAll Loss: 1.9433\tTriple Loss(1): 0.0068\tClassification Loss: 1.9297\n","Train Epoch: 1 [17120/110534 (15%)]\tAll Loss: 2.6055\tTriple Loss(1): 0.1601\tClassification Loss: 2.2853\n","Train Epoch: 1 [17280/110534 (16%)]\tAll Loss: 2.2885\tTriple Loss(1): 0.2389\tClassification Loss: 1.8107\n","Train Epoch: 1 [17440/110534 (16%)]\tAll Loss: 1.7113\tTriple Loss(1): 0.0583\tClassification Loss: 1.5948\n","\n","Test set: Average loss: 1.5157, Accuracy: 276/480 (58%)\n","\n","Train Epoch: 1 [17600/110534 (16%)]\tAll Loss: 1.7592\tTriple Loss(0): 0.0000\tClassification Loss: 1.7592\n","Train Epoch: 1 [17760/110534 (16%)]\tAll Loss: 2.7573\tTriple Loss(1): 0.7667\tClassification Loss: 1.2238\n","Train Epoch: 1 [17920/110534 (16%)]\tAll Loss: 1.9448\tTriple Loss(1): 0.2167\tClassification Loss: 1.5115\n","Train Epoch: 1 [18080/110534 (16%)]\tAll Loss: 1.3275\tTriple Loss(1): 0.0630\tClassification Loss: 1.2016\n","Train Epoch: 1 [18240/110534 (17%)]\tAll Loss: 2.7852\tTriple Loss(1): 0.3515\tClassification Loss: 2.0822\n","Train Epoch: 1 [18400/110534 (17%)]\tAll Loss: 2.7686\tTriple Loss(1): 0.4493\tClassification Loss: 1.8701\n","Train Epoch: 1 [18560/110534 (17%)]\tAll Loss: 1.7376\tTriple Loss(0): 0.0000\tClassification Loss: 1.7376\n","Train Epoch: 1 [18720/110534 (17%)]\tAll Loss: 2.2665\tTriple Loss(1): 0.1823\tClassification Loss: 1.9018\n","Train Epoch: 1 [18880/110534 (17%)]\tAll Loss: 2.0899\tTriple Loss(0): 0.0000\tClassification Loss: 2.0899\n","Train Epoch: 1 [19040/110534 (17%)]\tAll Loss: 1.2108\tTriple Loss(1): 0.0000\tClassification Loss: 1.2108\n","\n","Test set: Average loss: 1.4896, Accuracy: 285/480 (59%)\n","\n","Train Epoch: 1 [19200/110534 (17%)]\tAll Loss: 2.3287\tTriple Loss(1): 0.2214\tClassification Loss: 1.8860\n","Train Epoch: 1 [19360/110534 (18%)]\tAll Loss: 1.6626\tTriple Loss(1): 0.1319\tClassification Loss: 1.3988\n","Train Epoch: 1 [19520/110534 (18%)]\tAll Loss: 2.2105\tTriple Loss(1): 0.3638\tClassification Loss: 1.4829\n","Train Epoch: 1 [19680/110534 (18%)]\tAll Loss: 2.5274\tTriple Loss(1): 0.0845\tClassification Loss: 2.3584\n","Train Epoch: 1 [19840/110534 (18%)]\tAll Loss: 1.3955\tTriple Loss(1): 0.1797\tClassification Loss: 1.0361\n","Train Epoch: 1 [20000/110534 (18%)]\tAll Loss: 1.6571\tTriple Loss(1): 0.2495\tClassification Loss: 1.1581\n","Train Epoch: 1 [20160/110534 (18%)]\tAll Loss: 1.8518\tTriple Loss(1): 0.0000\tClassification Loss: 1.8518\n","Train Epoch: 1 [20320/110534 (18%)]\tAll Loss: 1.6773\tTriple Loss(1): 0.0514\tClassification Loss: 1.5745\n","Train Epoch: 1 [20480/110534 (19%)]\tAll Loss: 1.6440\tTriple Loss(1): 0.1238\tClassification Loss: 1.3964\n","Train Epoch: 1 [20640/110534 (19%)]\tAll Loss: 1.3566\tTriple Loss(0): 0.0000\tClassification Loss: 1.3566\n","\n","Test set: Average loss: 1.4783, Accuracy: 294/480 (61%)\n","\n","Train Epoch: 1 [20800/110534 (19%)]\tAll Loss: 1.1796\tTriple Loss(1): 0.0000\tClassification Loss: 1.1796\n","Train Epoch: 1 [20960/110534 (19%)]\tAll Loss: 1.3157\tTriple Loss(1): 0.1050\tClassification Loss: 1.1056\n","Train Epoch: 1 [21120/110534 (19%)]\tAll Loss: 2.6405\tTriple Loss(1): 0.5577\tClassification Loss: 1.5252\n","Train Epoch: 1 [21280/110534 (19%)]\tAll Loss: 1.7987\tTriple Loss(1): 0.2973\tClassification Loss: 1.2041\n","Train Epoch: 1 [21440/110534 (19%)]\tAll Loss: 2.0077\tTriple Loss(0): 0.0000\tClassification Loss: 2.0077\n","Train Epoch: 1 [21600/110534 (20%)]\tAll Loss: 2.0398\tTriple Loss(1): 0.0933\tClassification Loss: 1.8532\n","Train Epoch: 1 [21760/110534 (20%)]\tAll Loss: 2.2771\tTriple Loss(1): 0.3939\tClassification Loss: 1.4893\n","Train Epoch: 1 [21920/110534 (20%)]\tAll Loss: 1.3958\tTriple Loss(1): 0.1727\tClassification Loss: 1.0504\n","Train Epoch: 1 [22080/110534 (20%)]\tAll Loss: 1.6934\tTriple Loss(1): 0.0440\tClassification Loss: 1.6054\n","Train Epoch: 1 [22240/110534 (20%)]\tAll Loss: 1.7097\tTriple Loss(0): 0.0000\tClassification Loss: 1.7097\n","\n","Test set: Average loss: 1.5168, Accuracy: 281/480 (59%)\n","\n","Train Epoch: 1 [22400/110534 (20%)]\tAll Loss: 1.3569\tTriple Loss(1): 0.1120\tClassification Loss: 1.1328\n","Train Epoch: 1 [22560/110534 (20%)]\tAll Loss: 2.0133\tTriple Loss(1): 0.0000\tClassification Loss: 2.0133\n","Train Epoch: 1 [22720/110534 (21%)]\tAll Loss: 1.7410\tTriple Loss(1): 0.0000\tClassification Loss: 1.7410\n","Train Epoch: 1 [22880/110534 (21%)]\tAll Loss: 2.1969\tTriple Loss(1): 0.1687\tClassification Loss: 1.8594\n","Train Epoch: 1 [23040/110534 (21%)]\tAll Loss: 1.6867\tTriple Loss(1): 0.1232\tClassification Loss: 1.4404\n","Train Epoch: 1 [23200/110534 (21%)]\tAll Loss: 1.7056\tTriple Loss(1): 0.1947\tClassification Loss: 1.3162\n","Train Epoch: 1 [23360/110534 (21%)]\tAll Loss: 2.0242\tTriple Loss(1): 0.2051\tClassification Loss: 1.6141\n","Train Epoch: 1 [23520/110534 (21%)]\tAll Loss: 2.1472\tTriple Loss(1): 0.1665\tClassification Loss: 1.8141\n","Train Epoch: 1 [23680/110534 (21%)]\tAll Loss: 1.6691\tTriple Loss(1): 0.0881\tClassification Loss: 1.4929\n","Train Epoch: 1 [23840/110534 (22%)]\tAll Loss: 1.3429\tTriple Loss(1): 0.0662\tClassification Loss: 1.2105\n","\n","Test set: Average loss: 1.4670, Accuracy: 290/480 (60%)\n","\n","Train Epoch: 1 [24000/110534 (22%)]\tAll Loss: 2.0289\tTriple Loss(0): 0.0000\tClassification Loss: 2.0289\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_1500.pth.tar\n","Train Epoch: 1 [24160/110534 (22%)]\tAll Loss: 2.2555\tTriple Loss(1): 0.3803\tClassification Loss: 1.4949\n","Train Epoch: 1 [24320/110534 (22%)]\tAll Loss: 2.2030\tTriple Loss(1): 0.2343\tClassification Loss: 1.7344\n","Train Epoch: 1 [24480/110534 (22%)]\tAll Loss: 2.3656\tTriple Loss(1): 0.1791\tClassification Loss: 2.0074\n","Train Epoch: 1 [24640/110534 (22%)]\tAll Loss: 1.1767\tTriple Loss(1): 0.1044\tClassification Loss: 0.9678\n","Train Epoch: 1 [24800/110534 (22%)]\tAll Loss: 2.1673\tTriple Loss(1): 0.1407\tClassification Loss: 1.8858\n","Train Epoch: 1 [24960/110534 (23%)]\tAll Loss: 1.7119\tTriple Loss(1): 0.1732\tClassification Loss: 1.3656\n","Train Epoch: 1 [25120/110534 (23%)]\tAll Loss: 1.8443\tTriple Loss(0): 0.0000\tClassification Loss: 1.8443\n","Train Epoch: 1 [25280/110534 (23%)]\tAll Loss: 1.3995\tTriple Loss(1): 0.0518\tClassification Loss: 1.2960\n","Train Epoch: 1 [25440/110534 (23%)]\tAll Loss: 1.1575\tTriple Loss(0): 0.0000\tClassification Loss: 1.1575\n","\n","Test set: Average loss: 1.4959, Accuracy: 286/480 (60%)\n","\n","Train Epoch: 1 [25600/110534 (23%)]\tAll Loss: 1.7937\tTriple Loss(0): 0.0000\tClassification Loss: 1.7937\n","Train Epoch: 1 [25760/110534 (23%)]\tAll Loss: 2.4361\tTriple Loss(1): 0.3797\tClassification Loss: 1.6766\n","Train Epoch: 1 [25920/110534 (23%)]\tAll Loss: 1.7123\tTriple Loss(1): 0.0664\tClassification Loss: 1.5796\n","Train Epoch: 1 [26080/110534 (24%)]\tAll Loss: 1.4447\tTriple Loss(1): 0.0579\tClassification Loss: 1.3289\n","Train Epoch: 1 [26240/110534 (24%)]\tAll Loss: 1.8269\tTriple Loss(1): 0.0000\tClassification Loss: 1.8269\n","ERROR: Unexpected segmentation fault encountered in worker.\n","\u0000Traceback (most recent call last):\n","  File \"train.py\", line 87, in train\n","    data_tri_list = next(triplet_loader_iter)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 345, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 831, in _next_data\n","    raise StopIteration\n","StopIteration\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 761, in _try_get_data\n","    data = self._data_queue.get(timeout=timeout)\n","  File \"/usr/lib/python3.6/queue.py\", line 173, in get\n","    self.not_empty.wait(remaining)\n","  File \"/usr/lib/python3.6/threading.py\", line 299, in wait\n","    gotit = waiter.acquire(True, timeout)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\n","    _error_if_any_worker_fails()\n","RuntimeError: DataLoader worker (pid 5720) is killed by signal: Segmentation fault. \n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"train.py\", line 155, in <module>\n","    train(epoch)\n","  File \"train.py\", line 90, in train\n","    data_tri_list = next(triplet_loader_iter)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 345, in __next__\n","    data = self._next_data()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 841, in _next_data\n","    idx, data = self._get_data()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 798, in _get_data\n","    success, data = self._try_get_data()\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\", line 774, in _try_get_data\n","    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str))\n","RuntimeError: DataLoader worker (pid(s) 5720) exited unexpectedly\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g5b-KDF5GUd7","colab_type":"code","outputId":"40ecd8a2-32bd-46e0-bb17-ff2429062648","executionInfo":{"status":"ok","timestamp":1584699253819,"user_tz":-300,"elapsed":64469,"user":{"displayName":"Muhammad Ali","photoUrl":"","userId":"15673831022739340207"}},"colab":{"base_uri":"https://localhost:8080/"}},"source":["# From scratch. Freeze=True (2). LR=0.01\n","# model_5_500.pth.tar trained for 4.16 epochs. Stable at 58% accuracy and loss around 1.6 but loss was still going down slightly\n","! python train.py"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n","  \"please use transforms.Resize instead.\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:704: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n","  \"please use transforms.RandomResizedCrop instead.\")\n","Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to /root/.cache/torch/checkpoints/resnet50-19c8e357.pth\n","100% 97.8M/97.8M [00:00<00:00, 168MB/s]\n","/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n","train.py:132: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(data, volatile=True), Variable(target)\n","\n","Test set: Average loss: 3.4765, Accuracy: 45/960 (5%)\n","\n","Train Epoch: 1 [0/110534 (0%)]\tAll Loss: 4.3483\tTriple Loss(1): 0.5163\tClassification Loss: 3.3158\n","Train Epoch: 1 [320/110534 (0%)]\tAll Loss: 4.3431\tTriple Loss(1): 0.6884\tClassification Loss: 2.9662\n","Train Epoch: 1 [640/110534 (1%)]\tAll Loss: 5.8130\tTriple Loss(0): 1.5165\tClassification Loss: 2.7800\n","Train Epoch: 1 [960/110534 (1%)]\tAll Loss: 3.2336\tTriple Loss(1): 0.2471\tClassification Loss: 2.7393\n","Train Epoch: 1 [1280/110534 (1%)]\tAll Loss: 2.7347\tTriple Loss(0): 0.0000\tClassification Loss: 2.7347\n","Train Epoch: 1 [1600/110534 (1%)]\tAll Loss: 2.5127\tTriple Loss(0): 0.0000\tClassification Loss: 2.5127\n","Train Epoch: 1 [1920/110534 (2%)]\tAll Loss: 3.7189\tTriple Loss(1): 0.5633\tClassification Loss: 2.5924\n","Train Epoch: 1 [2240/110534 (2%)]\tAll Loss: 3.3676\tTriple Loss(1): 0.4493\tClassification Loss: 2.4691\n","Train Epoch: 1 [2560/110534 (2%)]\tAll Loss: 3.1038\tTriple Loss(1): 0.3192\tClassification Loss: 2.4655\n","Train Epoch: 1 [2880/110534 (3%)]\tAll Loss: 2.9974\tTriple Loss(1): 0.2652\tClassification Loss: 2.4670\n","\n","Test set: Average loss: 2.5878, Accuracy: 239/960 (25%)\n","\n","Train Epoch: 1 [3200/110534 (3%)]\tAll Loss: 3.7002\tTriple Loss(1): 0.5625\tClassification Loss: 2.5753\n","Train Epoch: 1 [3520/110534 (3%)]\tAll Loss: 2.1138\tTriple Loss(0): 0.0000\tClassification Loss: 2.1138\n","Train Epoch: 1 [3840/110534 (3%)]\tAll Loss: 3.3530\tTriple Loss(1): 0.4862\tClassification Loss: 2.3806\n","Train Epoch: 1 [4160/110534 (4%)]\tAll Loss: 3.3885\tTriple Loss(1): 0.4694\tClassification Loss: 2.4498\n","Train Epoch: 1 [4480/110534 (4%)]\tAll Loss: 3.1319\tTriple Loss(1): 0.3365\tClassification Loss: 2.4590\n","Train Epoch: 1 [4800/110534 (4%)]\tAll Loss: 3.2836\tTriple Loss(1): 0.4319\tClassification Loss: 2.4199\n","Train Epoch: 1 [5120/110534 (5%)]\tAll Loss: 3.4411\tTriple Loss(1): 0.5314\tClassification Loss: 2.3783\n","Train Epoch: 1 [5440/110534 (5%)]\tAll Loss: 2.4496\tTriple Loss(0): 0.0000\tClassification Loss: 2.4496\n","Train Epoch: 1 [5760/110534 (5%)]\tAll Loss: 3.2715\tTriple Loss(1): 0.5521\tClassification Loss: 2.1672\n","Train Epoch: 1 [6080/110534 (5%)]\tAll Loss: 3.2655\tTriple Loss(1): 0.4961\tClassification Loss: 2.2734\n","\n","Test set: Average loss: 2.4070, Accuracy: 293/960 (31%)\n","\n","Train Epoch: 1 [6400/110534 (6%)]\tAll Loss: 3.4184\tTriple Loss(1): 0.4781\tClassification Loss: 2.4621\n","Train Epoch: 1 [6720/110534 (6%)]\tAll Loss: 3.3485\tTriple Loss(1): 0.4487\tClassification Loss: 2.4512\n","Train Epoch: 1 [7040/110534 (6%)]\tAll Loss: 3.9483\tTriple Loss(1): 0.7355\tClassification Loss: 2.4773\n","Train Epoch: 1 [7360/110534 (7%)]\tAll Loss: 2.8470\tTriple Loss(1): 0.4186\tClassification Loss: 2.0097\n","Train Epoch: 1 [7680/110534 (7%)]\tAll Loss: 3.0134\tTriple Loss(1): 0.4657\tClassification Loss: 2.0820\n","Train Epoch: 1 [8000/110534 (7%)]\tAll Loss: 1.9897\tTriple Loss(0): 0.0000\tClassification Loss: 1.9897\n","Train Epoch: 1 [8320/110534 (8%)]\tAll Loss: 2.1861\tTriple Loss(0): 0.0000\tClassification Loss: 2.1861\n","Train Epoch: 1 [8640/110534 (8%)]\tAll Loss: 2.0087\tTriple Loss(0): 0.0000\tClassification Loss: 2.0087\n","Train Epoch: 1 [8960/110534 (8%)]\tAll Loss: 2.9460\tTriple Loss(1): 0.4324\tClassification Loss: 2.0813\n","Train Epoch: 1 [9280/110534 (8%)]\tAll Loss: 3.2819\tTriple Loss(1): 0.6515\tClassification Loss: 1.9788\n","\n","Test set: Average loss: 2.2947, Accuracy: 356/960 (37%)\n","\n","Train Epoch: 1 [9600/110534 (9%)]\tAll Loss: 2.3249\tTriple Loss(0): 0.0000\tClassification Loss: 2.3249\n","Train Epoch: 1 [9920/110534 (9%)]\tAll Loss: 2.9201\tTriple Loss(1): 0.3725\tClassification Loss: 2.1750\n","Train Epoch: 1 [10240/110534 (9%)]\tAll Loss: 2.2955\tTriple Loss(0): 0.0000\tClassification Loss: 2.2955\n","Train Epoch: 1 [10560/110534 (10%)]\tAll Loss: 4.7827\tTriple Loss(0): 1.2674\tClassification Loss: 2.2480\n","Train Epoch: 1 [10880/110534 (10%)]\tAll Loss: 2.4254\tTriple Loss(0): 0.0000\tClassification Loss: 2.4254\n","Train Epoch: 1 [11200/110534 (10%)]\tAll Loss: 2.4969\tTriple Loss(1): 0.3294\tClassification Loss: 1.8382\n","Train Epoch: 1 [11520/110534 (10%)]\tAll Loss: 2.8650\tTriple Loss(1): 0.3630\tClassification Loss: 2.1389\n","Train Epoch: 1 [11840/110534 (11%)]\tAll Loss: 2.5245\tTriple Loss(1): 0.2584\tClassification Loss: 2.0077\n","Train Epoch: 1 [12160/110534 (11%)]\tAll Loss: 2.3520\tTriple Loss(0): 0.0000\tClassification Loss: 2.3520\n","Train Epoch: 1 [12480/110534 (11%)]\tAll Loss: 2.9566\tTriple Loss(1): 0.4006\tClassification Loss: 2.1554\n","\n","Test set: Average loss: 2.2093, Accuracy: 353/960 (37%)\n","\n","Train Epoch: 1 [12800/110534 (12%)]\tAll Loss: 2.8315\tTriple Loss(1): 0.3449\tClassification Loss: 2.1417\n","Train Epoch: 1 [13120/110534 (12%)]\tAll Loss: 2.6978\tTriple Loss(1): 0.4072\tClassification Loss: 1.8835\n","Train Epoch: 1 [13440/110534 (12%)]\tAll Loss: 3.2743\tTriple Loss(1): 0.4901\tClassification Loss: 2.2942\n","Train Epoch: 1 [13760/110534 (12%)]\tAll Loss: 2.9040\tTriple Loss(1): 0.3723\tClassification Loss: 2.1593\n","Train Epoch: 1 [14080/110534 (13%)]\tAll Loss: 2.4449\tTriple Loss(1): 0.1961\tClassification Loss: 2.0528\n","Train Epoch: 1 [14400/110534 (13%)]\tAll Loss: 3.0787\tTriple Loss(1): 0.3534\tClassification Loss: 2.3719\n","Train Epoch: 1 [14720/110534 (13%)]\tAll Loss: 2.5468\tTriple Loss(1): 0.1878\tClassification Loss: 2.1712\n","Train Epoch: 1 [15040/110534 (14%)]\tAll Loss: 2.8062\tTriple Loss(1): 0.4043\tClassification Loss: 1.9977\n","Train Epoch: 1 [15360/110534 (14%)]\tAll Loss: 3.1750\tTriple Loss(1): 0.6566\tClassification Loss: 1.8619\n","Train Epoch: 1 [15680/110534 (14%)]\tAll Loss: 1.9020\tTriple Loss(0): 0.0000\tClassification Loss: 1.9020\n","\n","Test set: Average loss: 2.1455, Accuracy: 414/960 (43%)\n","\n","Train Epoch: 1 [16000/110534 (14%)]\tAll Loss: 3.3954\tTriple Loss(1): 0.6302\tClassification Loss: 2.1351\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_500.pth.tar\n","Train Epoch: 1 [16320/110534 (15%)]\tAll Loss: 1.8333\tTriple Loss(0): 0.0000\tClassification Loss: 1.8333\n","Train Epoch: 1 [16640/110534 (15%)]\tAll Loss: 3.3811\tTriple Loss(1): 0.6631\tClassification Loss: 2.0548\n","Train Epoch: 1 [16960/110534 (15%)]\tAll Loss: 2.9412\tTriple Loss(1): 0.3009\tClassification Loss: 2.3394\n","Train Epoch: 1 [17280/110534 (16%)]\tAll Loss: 2.6089\tTriple Loss(1): 0.3809\tClassification Loss: 1.8472\n","Train Epoch: 1 [17600/110534 (16%)]\tAll Loss: 2.7778\tTriple Loss(1): 0.3351\tClassification Loss: 2.1077\n","Train Epoch: 1 [17920/110534 (16%)]\tAll Loss: 3.1198\tTriple Loss(1): 0.5424\tClassification Loss: 2.0351\n","Train Epoch: 1 [18240/110534 (16%)]\tAll Loss: 3.1858\tTriple Loss(1): 0.5191\tClassification Loss: 2.1476\n","Train Epoch: 1 [18560/110534 (17%)]\tAll Loss: 2.7241\tTriple Loss(1): 0.3870\tClassification Loss: 1.9502\n","Train Epoch: 1 [18880/110534 (17%)]\tAll Loss: 2.1246\tTriple Loss(0): 0.0000\tClassification Loss: 2.1246\n","\n","Test set: Average loss: 2.0926, Accuracy: 451/960 (47%)\n","\n","Train Epoch: 1 [19200/110534 (17%)]\tAll Loss: 2.9398\tTriple Loss(1): 0.5479\tClassification Loss: 1.8440\n","Train Epoch: 1 [19520/110534 (18%)]\tAll Loss: 3.1365\tTriple Loss(1): 0.4846\tClassification Loss: 2.1672\n","Train Epoch: 1 [19840/110534 (18%)]\tAll Loss: 2.7402\tTriple Loss(1): 0.3493\tClassification Loss: 2.0415\n","Train Epoch: 1 [20160/110534 (18%)]\tAll Loss: 2.4842\tTriple Loss(1): 0.2579\tClassification Loss: 1.9684\n","Train Epoch: 1 [20480/110534 (19%)]\tAll Loss: 2.9076\tTriple Loss(1): 0.5844\tClassification Loss: 1.7389\n","Train Epoch: 1 [20800/110534 (19%)]\tAll Loss: 2.3740\tTriple Loss(0): 0.0000\tClassification Loss: 2.3740\n","Train Epoch: 1 [21120/110534 (19%)]\tAll Loss: 2.5532\tTriple Loss(1): 0.3270\tClassification Loss: 1.8992\n","Train Epoch: 1 [21440/110534 (19%)]\tAll Loss: 2.9301\tTriple Loss(1): 0.4902\tClassification Loss: 1.9498\n","Train Epoch: 1 [21760/110534 (20%)]\tAll Loss: 2.9281\tTriple Loss(1): 0.4011\tClassification Loss: 2.1259\n","Train Epoch: 1 [22080/110534 (20%)]\tAll Loss: 2.1255\tTriple Loss(0): 0.0000\tClassification Loss: 2.1255\n","\n","Test set: Average loss: 2.0552, Accuracy: 442/960 (46%)\n","\n","Train Epoch: 1 [22400/110534 (20%)]\tAll Loss: 2.4062\tTriple Loss(1): 0.3059\tClassification Loss: 1.7944\n","Train Epoch: 1 [22720/110534 (21%)]\tAll Loss: 2.0716\tTriple Loss(0): 0.0000\tClassification Loss: 2.0716\n","Train Epoch: 1 [23040/110534 (21%)]\tAll Loss: 2.6025\tTriple Loss(1): 0.3060\tClassification Loss: 1.9905\n","Train Epoch: 1 [23360/110534 (21%)]\tAll Loss: 2.9490\tTriple Loss(1): 0.4043\tClassification Loss: 2.1405\n","Train Epoch: 1 [23680/110534 (21%)]\tAll Loss: 3.5304\tTriple Loss(1): 0.6128\tClassification Loss: 2.3049\n","Train Epoch: 1 [24000/110534 (22%)]\tAll Loss: 2.7842\tTriple Loss(1): 0.4443\tClassification Loss: 1.8957\n","Train Epoch: 1 [24320/110534 (22%)]\tAll Loss: 2.9956\tTriple Loss(1): 0.4409\tClassification Loss: 2.1139\n","Train Epoch: 1 [24640/110534 (22%)]\tAll Loss: 3.2313\tTriple Loss(1): 0.5145\tClassification Loss: 2.2024\n","Train Epoch: 1 [24960/110534 (23%)]\tAll Loss: 2.6267\tTriple Loss(1): 0.3616\tClassification Loss: 1.9035\n","Train Epoch: 1 [25280/110534 (23%)]\tAll Loss: 2.5326\tTriple Loss(1): 0.2467\tClassification Loss: 2.0391\n","\n","Test set: Average loss: 2.0189, Accuracy: 445/960 (46%)\n","\n","Train Epoch: 1 [25600/110534 (23%)]\tAll Loss: 2.8240\tTriple Loss(1): 0.3765\tClassification Loss: 2.0709\n","Train Epoch: 1 [25920/110534 (23%)]\tAll Loss: 2.7654\tTriple Loss(1): 0.2802\tClassification Loss: 2.2051\n","Train Epoch: 1 [26240/110534 (24%)]\tAll Loss: 2.6757\tTriple Loss(1): 0.4651\tClassification Loss: 1.7454\n","Train Epoch: 1 [26560/110534 (24%)]\tAll Loss: 2.7239\tTriple Loss(1): 0.2846\tClassification Loss: 2.1547\n","Train Epoch: 1 [26880/110534 (24%)]\tAll Loss: 2.8952\tTriple Loss(1): 0.3654\tClassification Loss: 2.1643\n","Train Epoch: 1 [27200/110534 (25%)]\tAll Loss: 2.5882\tTriple Loss(1): 0.2133\tClassification Loss: 2.1617\n","Train Epoch: 1 [27520/110534 (25%)]\tAll Loss: 2.9765\tTriple Loss(1): 0.4522\tClassification Loss: 2.0720\n","Train Epoch: 1 [27840/110534 (25%)]\tAll Loss: 2.6444\tTriple Loss(1): 0.4858\tClassification Loss: 1.6727\n","Train Epoch: 1 [28160/110534 (25%)]\tAll Loss: 2.1910\tTriple Loss(0): 0.0000\tClassification Loss: 2.1910\n","Train Epoch: 1 [28480/110534 (26%)]\tAll Loss: 2.6191\tTriple Loss(1): 0.2397\tClassification Loss: 2.1397\n","\n","Test set: Average loss: 1.9852, Accuracy: 464/960 (48%)\n","\n","Train Epoch: 1 [28800/110534 (26%)]\tAll Loss: 2.8718\tTriple Loss(1): 0.5096\tClassification Loss: 1.8527\n","Train Epoch: 1 [29120/110534 (26%)]\tAll Loss: 1.9934\tTriple Loss(0): 0.0000\tClassification Loss: 1.9934\n","Train Epoch: 1 [29440/110534 (27%)]\tAll Loss: 1.8415\tTriple Loss(0): 0.0000\tClassification Loss: 1.8415\n","Train Epoch: 1 [29760/110534 (27%)]\tAll Loss: 2.8349\tTriple Loss(1): 0.5418\tClassification Loss: 1.7513\n","Train Epoch: 1 [30080/110534 (27%)]\tAll Loss: 2.8767\tTriple Loss(1): 0.5367\tClassification Loss: 1.8032\n","Train Epoch: 1 [30400/110534 (27%)]\tAll Loss: 1.8885\tTriple Loss(0): 0.0000\tClassification Loss: 1.8885\n","Train Epoch: 1 [30720/110534 (28%)]\tAll Loss: 2.3419\tTriple Loss(1): 0.3519\tClassification Loss: 1.6381\n","Train Epoch: 1 [31040/110534 (28%)]\tAll Loss: 2.0692\tTriple Loss(0): 0.0000\tClassification Loss: 2.0692\n","Train Epoch: 1 [31360/110534 (28%)]\tAll Loss: 2.4971\tTriple Loss(1): 0.1483\tClassification Loss: 2.2005\n","Train Epoch: 1 [31680/110534 (29%)]\tAll Loss: 2.8840\tTriple Loss(1): 0.5302\tClassification Loss: 1.8236\n","\n","Test set: Average loss: 1.9634, Accuracy: 495/960 (52%)\n","\n","Train Epoch: 1 [32000/110534 (29%)]\tAll Loss: 2.7454\tTriple Loss(1): 0.3377\tClassification Loss: 2.0701\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_1000.pth.tar\n","Train Epoch: 1 [32320/110534 (29%)]\tAll Loss: 2.7000\tTriple Loss(1): 0.4641\tClassification Loss: 1.7718\n","Train Epoch: 1 [32640/110534 (30%)]\tAll Loss: 2.9430\tTriple Loss(1): 0.4569\tClassification Loss: 2.0292\n","Train Epoch: 1 [32960/110534 (30%)]\tAll Loss: 2.1577\tTriple Loss(0): 0.0000\tClassification Loss: 2.1577\n","Train Epoch: 1 [33280/110534 (30%)]\tAll Loss: 2.9199\tTriple Loss(1): 0.4382\tClassification Loss: 2.0434\n","Train Epoch: 1 [33600/110534 (30%)]\tAll Loss: 2.3569\tTriple Loss(1): 0.2624\tClassification Loss: 1.8322\n","Train Epoch: 1 [33920/110534 (31%)]\tAll Loss: 2.4958\tTriple Loss(1): 0.3111\tClassification Loss: 1.8736\n","Train Epoch: 1 [34240/110534 (31%)]\tAll Loss: 2.3389\tTriple Loss(1): 0.1821\tClassification Loss: 1.9747\n","Train Epoch: 1 [34560/110534 (31%)]\tAll Loss: 2.2353\tTriple Loss(1): 0.2653\tClassification Loss: 1.7047\n","Train Epoch: 1 [34880/110534 (32%)]\tAll Loss: 2.4089\tTriple Loss(1): 0.2764\tClassification Loss: 1.8561\n","\n","Test set: Average loss: 1.9388, Accuracy: 509/960 (53%)\n","\n","Train Epoch: 1 [35200/110534 (32%)]\tAll Loss: 3.0277\tTriple Loss(1): 0.4425\tClassification Loss: 2.1426\n","Train Epoch: 1 [35520/110534 (32%)]\tAll Loss: 2.5437\tTriple Loss(1): 0.3404\tClassification Loss: 1.8630\n","Train Epoch: 1 [35840/110534 (32%)]\tAll Loss: 2.4272\tTriple Loss(1): 0.3055\tClassification Loss: 1.8162\n","Train Epoch: 1 [36160/110534 (33%)]\tAll Loss: 1.7582\tTriple Loss(0): 0.0000\tClassification Loss: 1.7582\n","Train Epoch: 1 [36480/110534 (33%)]\tAll Loss: 2.0010\tTriple Loss(0): 0.0000\tClassification Loss: 2.0010\n","Train Epoch: 1 [36800/110534 (33%)]\tAll Loss: 2.5182\tTriple Loss(1): 0.3421\tClassification Loss: 1.8340\n","Train Epoch: 1 [37120/110534 (34%)]\tAll Loss: 2.0998\tTriple Loss(0): 0.0000\tClassification Loss: 2.0998\n","Train Epoch: 1 [37440/110534 (34%)]\tAll Loss: 2.1519\tTriple Loss(0): 0.0000\tClassification Loss: 2.1519\n","Train Epoch: 1 [37760/110534 (34%)]\tAll Loss: 2.2487\tTriple Loss(1): 0.2287\tClassification Loss: 1.7913\n","Train Epoch: 1 [38080/110534 (34%)]\tAll Loss: 2.7475\tTriple Loss(1): 0.3595\tClassification Loss: 2.0285\n","\n","Test set: Average loss: 1.9134, Accuracy: 506/960 (53%)\n","\n","Train Epoch: 1 [38400/110534 (35%)]\tAll Loss: 2.1651\tTriple Loss(1): 0.2237\tClassification Loss: 1.7178\n","Train Epoch: 1 [38720/110534 (35%)]\tAll Loss: 2.3181\tTriple Loss(1): 0.2214\tClassification Loss: 1.8752\n","Train Epoch: 1 [39040/110534 (35%)]\tAll Loss: 1.7000\tTriple Loss(0): 0.0000\tClassification Loss: 1.7000\n","Train Epoch: 1 [39360/110534 (36%)]\tAll Loss: 2.3534\tTriple Loss(1): 0.3298\tClassification Loss: 1.6938\n","Train Epoch: 1 [39680/110534 (36%)]\tAll Loss: 2.1633\tTriple Loss(0): 0.0000\tClassification Loss: 2.1633\n","Train Epoch: 1 [40000/110534 (36%)]\tAll Loss: 2.1082\tTriple Loss(1): 0.3570\tClassification Loss: 1.3942\n","Train Epoch: 1 [40320/110534 (36%)]\tAll Loss: 1.9464\tTriple Loss(0): 0.0000\tClassification Loss: 1.9464\n","Train Epoch: 1 [40640/110534 (37%)]\tAll Loss: 2.4581\tTriple Loss(1): 0.2881\tClassification Loss: 1.8819\n","Train Epoch: 1 [40960/110534 (37%)]\tAll Loss: 2.4596\tTriple Loss(1): 0.3763\tClassification Loss: 1.7070\n","Train Epoch: 1 [41280/110534 (37%)]\tAll Loss: 2.8360\tTriple Loss(1): 0.3933\tClassification Loss: 2.0495\n","\n","Test set: Average loss: 1.8913, Accuracy: 501/960 (52%)\n","\n","Train Epoch: 1 [41600/110534 (38%)]\tAll Loss: 2.6485\tTriple Loss(1): 0.2794\tClassification Loss: 2.0896\n","Train Epoch: 1 [41920/110534 (38%)]\tAll Loss: 1.8013\tTriple Loss(0): 0.0000\tClassification Loss: 1.8013\n","Train Epoch: 1 [42240/110534 (38%)]\tAll Loss: 2.3413\tTriple Loss(1): 0.2126\tClassification Loss: 1.9160\n","Train Epoch: 1 [42560/110534 (38%)]\tAll Loss: 2.4925\tTriple Loss(1): 0.3506\tClassification Loss: 1.7913\n","Train Epoch: 1 [42880/110534 (39%)]\tAll Loss: 2.5602\tTriple Loss(1): 0.3528\tClassification Loss: 1.8546\n","Train Epoch: 1 [43200/110534 (39%)]\tAll Loss: 2.9310\tTriple Loss(1): 0.4195\tClassification Loss: 2.0920\n","Train Epoch: 1 [43520/110534 (39%)]\tAll Loss: 2.2854\tTriple Loss(1): 0.2855\tClassification Loss: 1.7144\n","Train Epoch: 1 [43840/110534 (40%)]\tAll Loss: 2.2022\tTriple Loss(1): 0.2600\tClassification Loss: 1.6823\n","Train Epoch: 1 [44160/110534 (40%)]\tAll Loss: 2.6150\tTriple Loss(1): 0.2260\tClassification Loss: 2.1630\n","Train Epoch: 1 [44480/110534 (40%)]\tAll Loss: 2.5634\tTriple Loss(1): 0.2866\tClassification Loss: 1.9902\n","\n","Test set: Average loss: 1.8762, Accuracy: 507/960 (53%)\n","\n","Train Epoch: 1 [44800/110534 (41%)]\tAll Loss: 2.0189\tTriple Loss(1): 0.2534\tClassification Loss: 1.5121\n","Train Epoch: 1 [45120/110534 (41%)]\tAll Loss: 2.7420\tTriple Loss(1): 0.4076\tClassification Loss: 1.9269\n","Train Epoch: 1 [45440/110534 (41%)]\tAll Loss: 2.1491\tTriple Loss(1): 0.1709\tClassification Loss: 1.8072\n","Train Epoch: 1 [45760/110534 (41%)]\tAll Loss: 2.4320\tTriple Loss(1): 0.4516\tClassification Loss: 1.5288\n","Train Epoch: 1 [46080/110534 (42%)]\tAll Loss: 1.5956\tTriple Loss(0): 0.0000\tClassification Loss: 1.5956\n","Train Epoch: 1 [46400/110534 (42%)]\tAll Loss: 2.4420\tTriple Loss(1): 0.1715\tClassification Loss: 2.0991\n","Train Epoch: 1 [46720/110534 (42%)]\tAll Loss: 2.8519\tTriple Loss(1): 0.4245\tClassification Loss: 2.0029\n","Train Epoch: 1 [47040/110534 (43%)]\tAll Loss: 2.5904\tTriple Loss(1): 0.3942\tClassification Loss: 1.8020\n","Train Epoch: 1 [47360/110534 (43%)]\tAll Loss: 2.6433\tTriple Loss(1): 0.3151\tClassification Loss: 2.0130\n","Train Epoch: 1 [47680/110534 (43%)]\tAll Loss: 1.6539\tTriple Loss(1): 0.0781\tClassification Loss: 1.4976\n","\n","Test set: Average loss: 1.8631, Accuracy: 505/960 (53%)\n","\n","Train Epoch: 1 [48000/110534 (43%)]\tAll Loss: 3.2785\tTriple Loss(1): 0.5653\tClassification Loss: 2.1478\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_1500.pth.tar\n","Train Epoch: 1 [48320/110534 (44%)]\tAll Loss: 1.8732\tTriple Loss(0): 0.0000\tClassification Loss: 1.8732\n","Train Epoch: 1 [48640/110534 (44%)]\tAll Loss: 2.5874\tTriple Loss(1): 0.2613\tClassification Loss: 2.0649\n","Train Epoch: 1 [48960/110534 (44%)]\tAll Loss: 2.1897\tTriple Loss(1): 0.2048\tClassification Loss: 1.7800\n","Train Epoch: 1 [49280/110534 (45%)]\tAll Loss: 2.7718\tTriple Loss(1): 0.4770\tClassification Loss: 1.8178\n","Train Epoch: 1 [49600/110534 (45%)]\tAll Loss: 2.2213\tTriple Loss(1): 0.1181\tClassification Loss: 1.9852\n","Train Epoch: 1 [49920/110534 (45%)]\tAll Loss: 2.5682\tTriple Loss(1): 0.4894\tClassification Loss: 1.5893\n","Train Epoch: 1 [50240/110534 (45%)]\tAll Loss: 2.1994\tTriple Loss(1): 0.3703\tClassification Loss: 1.4588\n","Train Epoch: 1 [50560/110534 (46%)]\tAll Loss: 2.9597\tTriple Loss(1): 0.4937\tClassification Loss: 1.9723\n","Train Epoch: 1 [50880/110534 (46%)]\tAll Loss: 2.4249\tTriple Loss(1): 0.3435\tClassification Loss: 1.7379\n","\n","Test set: Average loss: 1.8459, Accuracy: 514/960 (54%)\n","\n","Train Epoch: 1 [51200/110534 (46%)]\tAll Loss: 2.6998\tTriple Loss(1): 0.4711\tClassification Loss: 1.7577\n","Train Epoch: 1 [51520/110534 (47%)]\tAll Loss: 2.5335\tTriple Loss(1): 0.4784\tClassification Loss: 1.5767\n","Train Epoch: 1 [51840/110534 (47%)]\tAll Loss: 2.4529\tTriple Loss(1): 0.3118\tClassification Loss: 1.8293\n","Train Epoch: 1 [52160/110534 (47%)]\tAll Loss: 1.8344\tTriple Loss(0): 0.0000\tClassification Loss: 1.8344\n","Train Epoch: 1 [52480/110534 (47%)]\tAll Loss: 2.6516\tTriple Loss(1): 0.1587\tClassification Loss: 2.3342\n","Train Epoch: 1 [52800/110534 (48%)]\tAll Loss: 1.7014\tTriple Loss(0): 0.0000\tClassification Loss: 1.7014\n","Train Epoch: 1 [53120/110534 (48%)]\tAll Loss: 1.7610\tTriple Loss(0): 0.0000\tClassification Loss: 1.7610\n","Train Epoch: 1 [53440/110534 (48%)]\tAll Loss: 2.1598\tTriple Loss(1): 0.1105\tClassification Loss: 1.9388\n","Train Epoch: 1 [53760/110534 (49%)]\tAll Loss: 2.5967\tTriple Loss(1): 0.3039\tClassification Loss: 1.9889\n","Train Epoch: 1 [54080/110534 (49%)]\tAll Loss: 2.7736\tTriple Loss(1): 0.4240\tClassification Loss: 1.9255\n","\n","Test set: Average loss: 1.8410, Accuracy: 508/960 (53%)\n","\n","Train Epoch: 1 [54400/110534 (49%)]\tAll Loss: 2.8189\tTriple Loss(1): 0.4549\tClassification Loss: 1.9091\n","Train Epoch: 1 [54720/110534 (49%)]\tAll Loss: 2.6048\tTriple Loss(1): 0.3069\tClassification Loss: 1.9909\n","Train Epoch: 1 [55040/110534 (50%)]\tAll Loss: 2.5006\tTriple Loss(1): 0.1884\tClassification Loss: 2.1238\n","Train Epoch: 1 [55360/110534 (50%)]\tAll Loss: 1.9566\tTriple Loss(1): 0.1422\tClassification Loss: 1.6722\n","Train Epoch: 1 [55680/110534 (50%)]\tAll Loss: 2.2964\tTriple Loss(1): 0.1742\tClassification Loss: 1.9481\n","Train Epoch: 1 [56000/110534 (51%)]\tAll Loss: 3.4284\tTriple Loss(1): 0.5743\tClassification Loss: 2.2799\n","Train Epoch: 1 [56320/110534 (51%)]\tAll Loss: 2.3117\tTriple Loss(1): 0.1571\tClassification Loss: 1.9976\n","Train Epoch: 1 [56640/110534 (51%)]\tAll Loss: 2.4158\tTriple Loss(1): 0.5048\tClassification Loss: 1.4063\n","Train Epoch: 1 [56960/110534 (52%)]\tAll Loss: 2.8628\tTriple Loss(1): 0.5304\tClassification Loss: 1.8021\n","Train Epoch: 1 [57280/110534 (52%)]\tAll Loss: 3.0746\tTriple Loss(1): 0.5206\tClassification Loss: 2.0335\n","\n","Test set: Average loss: 1.8273, Accuracy: 521/960 (54%)\n","\n","Train Epoch: 1 [57600/110534 (52%)]\tAll Loss: 2.3462\tTriple Loss(1): 0.1881\tClassification Loss: 1.9701\n","Train Epoch: 1 [57920/110534 (52%)]\tAll Loss: 2.9457\tTriple Loss(1): 0.4668\tClassification Loss: 2.0122\n","Train Epoch: 1 [58240/110534 (53%)]\tAll Loss: 2.2595\tTriple Loss(1): 0.2970\tClassification Loss: 1.6654\n","Train Epoch: 1 [58560/110534 (53%)]\tAll Loss: 1.8096\tTriple Loss(0): 0.0000\tClassification Loss: 1.8096\n","Train Epoch: 1 [58880/110534 (53%)]\tAll Loss: 1.9453\tTriple Loss(0): 0.0000\tClassification Loss: 1.9453\n","Train Epoch: 1 [59200/110534 (54%)]\tAll Loss: 2.0179\tTriple Loss(0): 0.0000\tClassification Loss: 2.0179\n","Train Epoch: 1 [59520/110534 (54%)]\tAll Loss: 1.9886\tTriple Loss(1): 0.1341\tClassification Loss: 1.7204\n","Train Epoch: 1 [59840/110534 (54%)]\tAll Loss: 1.6498\tTriple Loss(0): 0.0000\tClassification Loss: 1.6498\n","Train Epoch: 1 [60160/110534 (54%)]\tAll Loss: 2.3359\tTriple Loss(1): 0.3142\tClassification Loss: 1.7074\n","Train Epoch: 1 [60480/110534 (55%)]\tAll Loss: 1.7006\tTriple Loss(0): 0.0000\tClassification Loss: 1.7006\n","\n","Test set: Average loss: 1.8186, Accuracy: 516/960 (54%)\n","\n","Train Epoch: 1 [60800/110534 (55%)]\tAll Loss: 1.9742\tTriple Loss(0): 0.0000\tClassification Loss: 1.9742\n","Train Epoch: 1 [61120/110534 (55%)]\tAll Loss: 2.8295\tTriple Loss(1): 0.4873\tClassification Loss: 1.8549\n","Train Epoch: 1 [61440/110534 (56%)]\tAll Loss: 2.5856\tTriple Loss(1): 0.6225\tClassification Loss: 1.3406\n","Train Epoch: 1 [61760/110534 (56%)]\tAll Loss: 2.5430\tTriple Loss(1): 0.2986\tClassification Loss: 1.9459\n","Train Epoch: 1 [62080/110534 (56%)]\tAll Loss: 2.4627\tTriple Loss(1): 0.3963\tClassification Loss: 1.6701\n","Train Epoch: 1 [62400/110534 (56%)]\tAll Loss: 2.0829\tTriple Loss(1): 0.1892\tClassification Loss: 1.7045\n","Train Epoch: 1 [62720/110534 (57%)]\tAll Loss: 1.4925\tTriple Loss(0): 0.0000\tClassification Loss: 1.4925\n","Train Epoch: 1 [63040/110534 (57%)]\tAll Loss: 2.7423\tTriple Loss(1): 0.1598\tClassification Loss: 2.4227\n","Train Epoch: 1 [63360/110534 (57%)]\tAll Loss: 2.3618\tTriple Loss(1): 0.2468\tClassification Loss: 1.8681\n","Train Epoch: 1 [63680/110534 (58%)]\tAll Loss: 2.3278\tTriple Loss(1): 0.4459\tClassification Loss: 1.4361\n","\n","Test set: Average loss: 1.8103, Accuracy: 506/960 (53%)\n","\n","Train Epoch: 1 [64000/110534 (58%)]\tAll Loss: 2.8265\tTriple Loss(1): 0.3108\tClassification Loss: 2.2049\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_2000.pth.tar\n","Train Epoch: 1 [64320/110534 (58%)]\tAll Loss: 1.6694\tTriple Loss(0): 0.0000\tClassification Loss: 1.6694\n","Train Epoch: 1 [64640/110534 (58%)]\tAll Loss: 2.8607\tTriple Loss(1): 0.1525\tClassification Loss: 2.5558\n","Train Epoch: 1 [64960/110534 (59%)]\tAll Loss: 2.3631\tTriple Loss(1): 0.3434\tClassification Loss: 1.6764\n","Train Epoch: 1 [65280/110534 (59%)]\tAll Loss: 2.6446\tTriple Loss(1): 0.3635\tClassification Loss: 1.9175\n","Train Epoch: 1 [65600/110534 (59%)]\tAll Loss: 4.7827\tTriple Loss(1): 1.4682\tClassification Loss: 1.8464\n","Train Epoch: 1 [65920/110534 (60%)]\tAll Loss: 2.0072\tTriple Loss(1): 0.2383\tClassification Loss: 1.5306\n","Train Epoch: 1 [66240/110534 (60%)]\tAll Loss: 2.4007\tTriple Loss(1): 0.3395\tClassification Loss: 1.7217\n","Train Epoch: 1 [66560/110534 (60%)]\tAll Loss: 2.8311\tTriple Loss(1): 0.5403\tClassification Loss: 1.7505\n","Train Epoch: 1 [66880/110534 (60%)]\tAll Loss: 2.5571\tTriple Loss(1): 0.2954\tClassification Loss: 1.9662\n","\n","Test set: Average loss: 1.8084, Accuracy: 507/960 (53%)\n","\n","Train Epoch: 1 [67200/110534 (61%)]\tAll Loss: 2.4412\tTriple Loss(1): 0.2294\tClassification Loss: 1.9824\n","Train Epoch: 1 [67520/110534 (61%)]\tAll Loss: 2.6413\tTriple Loss(1): 0.3574\tClassification Loss: 1.9264\n","Train Epoch: 1 [67840/110534 (61%)]\tAll Loss: 2.9034\tTriple Loss(1): 0.4123\tClassification Loss: 2.0788\n","Train Epoch: 1 [68160/110534 (62%)]\tAll Loss: 1.7608\tTriple Loss(1): 0.1701\tClassification Loss: 1.4205\n","Train Epoch: 1 [68480/110534 (62%)]\tAll Loss: 2.0638\tTriple Loss(1): 0.2492\tClassification Loss: 1.5654\n","Train Epoch: 1 [68800/110534 (62%)]\tAll Loss: 2.3183\tTriple Loss(1): 0.3316\tClassification Loss: 1.6551\n","Train Epoch: 1 [69120/110534 (63%)]\tAll Loss: 2.3030\tTriple Loss(1): 0.2948\tClassification Loss: 1.7135\n","Train Epoch: 1 [69440/110534 (63%)]\tAll Loss: 2.4529\tTriple Loss(1): 0.2853\tClassification Loss: 1.8823\n","Train Epoch: 1 [69760/110534 (63%)]\tAll Loss: 2.0452\tTriple Loss(1): 0.1375\tClassification Loss: 1.7701\n","Train Epoch: 1 [70080/110534 (63%)]\tAll Loss: 1.6478\tTriple Loss(1): 0.0251\tClassification Loss: 1.5975\n","\n","Test set: Average loss: 1.7928, Accuracy: 519/960 (54%)\n","\n","Train Epoch: 1 [70400/110534 (64%)]\tAll Loss: 2.3296\tTriple Loss(1): 0.3641\tClassification Loss: 1.6013\n","Train Epoch: 1 [70720/110534 (64%)]\tAll Loss: 1.8982\tTriple Loss(1): 0.2154\tClassification Loss: 1.4675\n","Train Epoch: 1 [71040/110534 (64%)]\tAll Loss: 2.2778\tTriple Loss(1): 0.2835\tClassification Loss: 1.7109\n","Train Epoch: 1 [71360/110534 (65%)]\tAll Loss: 2.1244\tTriple Loss(1): 0.1839\tClassification Loss: 1.7567\n","Train Epoch: 1 [71680/110534 (65%)]\tAll Loss: 2.4158\tTriple Loss(1): 0.2634\tClassification Loss: 1.8889\n","Train Epoch: 1 [72000/110534 (65%)]\tAll Loss: 2.6531\tTriple Loss(1): 0.4082\tClassification Loss: 1.8368\n","Train Epoch: 1 [72320/110534 (65%)]\tAll Loss: 2.3675\tTriple Loss(1): 0.3691\tClassification Loss: 1.6293\n","Train Epoch: 1 [72640/110534 (66%)]\tAll Loss: 2.2782\tTriple Loss(1): 0.4456\tClassification Loss: 1.3870\n","Train Epoch: 1 [72960/110534 (66%)]\tAll Loss: 2.0959\tTriple Loss(1): 0.2819\tClassification Loss: 1.5322\n","Train Epoch: 1 [73280/110534 (66%)]\tAll Loss: 2.0327\tTriple Loss(1): 0.3087\tClassification Loss: 1.4154\n","\n","Test set: Average loss: 1.7856, Accuracy: 525/960 (55%)\n","\n","Train Epoch: 1 [73600/110534 (67%)]\tAll Loss: 2.8245\tTriple Loss(1): 0.3028\tClassification Loss: 2.2190\n","Train Epoch: 1 [73920/110534 (67%)]\tAll Loss: 2.5612\tTriple Loss(1): 0.3287\tClassification Loss: 1.9038\n","Train Epoch: 1 [74240/110534 (67%)]\tAll Loss: 2.0773\tTriple Loss(1): 0.2912\tClassification Loss: 1.4949\n","Train Epoch: 1 [74560/110534 (67%)]\tAll Loss: 2.9874\tTriple Loss(1): 0.3831\tClassification Loss: 2.2212\n","Train Epoch: 1 [74880/110534 (68%)]\tAll Loss: 2.4087\tTriple Loss(1): 0.2402\tClassification Loss: 1.9284\n","Train Epoch: 1 [75200/110534 (68%)]\tAll Loss: 2.0415\tTriple Loss(1): 0.1795\tClassification Loss: 1.6826\n","Train Epoch: 1 [75520/110534 (68%)]\tAll Loss: 2.2684\tTriple Loss(1): 0.3777\tClassification Loss: 1.5129\n","Train Epoch: 1 [75840/110534 (69%)]\tAll Loss: 2.3190\tTriple Loss(1): 0.2496\tClassification Loss: 1.8198\n","Train Epoch: 1 [76160/110534 (69%)]\tAll Loss: 2.1836\tTriple Loss(1): 0.1938\tClassification Loss: 1.7961\n","Train Epoch: 1 [76480/110534 (69%)]\tAll Loss: 2.0495\tTriple Loss(1): 0.2256\tClassification Loss: 1.5982\n","\n","Test set: Average loss: 1.7764, Accuracy: 532/960 (55%)\n","\n","Train Epoch: 1 [76800/110534 (69%)]\tAll Loss: 1.7576\tTriple Loss(0): 0.0000\tClassification Loss: 1.7576\n","Train Epoch: 1 [77120/110534 (70%)]\tAll Loss: 1.7004\tTriple Loss(0): 0.0000\tClassification Loss: 1.7004\n","Train Epoch: 1 [77440/110534 (70%)]\tAll Loss: 1.7737\tTriple Loss(0): 0.0000\tClassification Loss: 1.7737\n","Train Epoch: 1 [77760/110534 (70%)]\tAll Loss: 1.5699\tTriple Loss(0): 0.0000\tClassification Loss: 1.5699\n","Train Epoch: 1 [78080/110534 (71%)]\tAll Loss: 2.1513\tTriple Loss(0): 0.0000\tClassification Loss: 2.1513\n","Train Epoch: 1 [78400/110534 (71%)]\tAll Loss: 1.8238\tTriple Loss(0): 0.0000\tClassification Loss: 1.8238\n","Train Epoch: 1 [78720/110534 (71%)]\tAll Loss: 1.9410\tTriple Loss(1): 0.2228\tClassification Loss: 1.4953\n","Train Epoch: 1 [79040/110534 (71%)]\tAll Loss: 2.2730\tTriple Loss(1): 0.2462\tClassification Loss: 1.7806\n","Train Epoch: 1 [79360/110534 (72%)]\tAll Loss: 2.7604\tTriple Loss(1): 0.3362\tClassification Loss: 2.0881\n","Train Epoch: 1 [79680/110534 (72%)]\tAll Loss: 2.2181\tTriple Loss(1): 0.2943\tClassification Loss: 1.6295\n","\n","Test set: Average loss: 1.7714, Accuracy: 522/960 (54%)\n","\n","Train Epoch: 1 [80000/110534 (72%)]\tAll Loss: 2.3018\tTriple Loss(1): 0.2581\tClassification Loss: 1.7856\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_2500.pth.tar\n","Train Epoch: 1 [80320/110534 (73%)]\tAll Loss: 1.8587\tTriple Loss(1): 0.2124\tClassification Loss: 1.4340\n","Train Epoch: 1 [80640/110534 (73%)]\tAll Loss: 1.8738\tTriple Loss(1): 0.1223\tClassification Loss: 1.6291\n","Train Epoch: 1 [80960/110534 (73%)]\tAll Loss: 1.6995\tTriple Loss(0): 0.0000\tClassification Loss: 1.6995\n","Train Epoch: 1 [81280/110534 (74%)]\tAll Loss: 2.2450\tTriple Loss(1): 0.3222\tClassification Loss: 1.6006\n","Train Epoch: 1 [81600/110534 (74%)]\tAll Loss: 1.4992\tTriple Loss(0): 0.0000\tClassification Loss: 1.4992\n","Train Epoch: 1 [81920/110534 (74%)]\tAll Loss: 2.1801\tTriple Loss(1): 0.2521\tClassification Loss: 1.6758\n","Train Epoch: 1 [82240/110534 (74%)]\tAll Loss: 2.8596\tTriple Loss(1): 0.5311\tClassification Loss: 1.7975\n","Train Epoch: 1 [82560/110534 (75%)]\tAll Loss: 1.7594\tTriple Loss(0): 0.0000\tClassification Loss: 1.7594\n","Train Epoch: 1 [82880/110534 (75%)]\tAll Loss: 1.9521\tTriple Loss(0): 0.0000\tClassification Loss: 1.9521\n","\n","Test set: Average loss: 1.7686, Accuracy: 524/960 (55%)\n","\n","Train Epoch: 1 [83200/110534 (75%)]\tAll Loss: 2.5890\tTriple Loss(1): 0.4552\tClassification Loss: 1.6786\n","Train Epoch: 1 [83520/110534 (76%)]\tAll Loss: 2.0330\tTriple Loss(1): 0.1459\tClassification Loss: 1.7413\n","Train Epoch: 1 [83840/110534 (76%)]\tAll Loss: 2.1185\tTriple Loss(1): 0.2446\tClassification Loss: 1.6293\n","Train Epoch: 1 [84160/110534 (76%)]\tAll Loss: 2.1832\tTriple Loss(1): 0.3422\tClassification Loss: 1.4988\n","Train Epoch: 1 [84480/110534 (76%)]\tAll Loss: 1.7955\tTriple Loss(1): 0.1631\tClassification Loss: 1.4693\n","Train Epoch: 1 [84800/110534 (77%)]\tAll Loss: 2.3203\tTriple Loss(1): 0.1800\tClassification Loss: 1.9603\n","Train Epoch: 1 [85120/110534 (77%)]\tAll Loss: 2.3963\tTriple Loss(1): 0.4111\tClassification Loss: 1.5741\n","Train Epoch: 1 [85440/110534 (77%)]\tAll Loss: 1.9818\tTriple Loss(1): 0.2462\tClassification Loss: 1.4894\n","Train Epoch: 1 [85760/110534 (78%)]\tAll Loss: 2.2152\tTriple Loss(1): 0.2293\tClassification Loss: 1.7566\n","Train Epoch: 1 [86080/110534 (78%)]\tAll Loss: 2.4546\tTriple Loss(1): 0.3882\tClassification Loss: 1.6782\n","\n","Test set: Average loss: 1.7602, Accuracy: 529/960 (55%)\n","\n","Train Epoch: 1 [86400/110534 (78%)]\tAll Loss: 2.5731\tTriple Loss(1): 0.1879\tClassification Loss: 2.1973\n","Train Epoch: 1 [86720/110534 (78%)]\tAll Loss: 1.9862\tTriple Loss(1): 0.2532\tClassification Loss: 1.4797\n","Train Epoch: 1 [87040/110534 (79%)]\tAll Loss: 2.3193\tTriple Loss(1): 0.3745\tClassification Loss: 1.5703\n","Train Epoch: 1 [87360/110534 (79%)]\tAll Loss: 2.4982\tTriple Loss(1): 0.3791\tClassification Loss: 1.7399\n","Train Epoch: 1 [87680/110534 (79%)]\tAll Loss: 2.1686\tTriple Loss(1): 0.2151\tClassification Loss: 1.7384\n","Train Epoch: 1 [88000/110534 (80%)]\tAll Loss: 3.0999\tTriple Loss(1): 0.3523\tClassification Loss: 2.3954\n","Train Epoch: 1 [88320/110534 (80%)]\tAll Loss: 2.3341\tTriple Loss(1): 0.3855\tClassification Loss: 1.5630\n","Train Epoch: 1 [88640/110534 (80%)]\tAll Loss: 3.2444\tTriple Loss(1): 0.5510\tClassification Loss: 2.1423\n","Train Epoch: 1 [88960/110534 (80%)]\tAll Loss: 2.5136\tTriple Loss(1): 0.3174\tClassification Loss: 1.8788\n","Train Epoch: 1 [89280/110534 (81%)]\tAll Loss: 6.7501\tTriple Loss(0): 2.5056\tClassification Loss: 1.7390\n","\n","Test set: Average loss: 1.7580, Accuracy: 533/960 (56%)\n","\n","Train Epoch: 1 [89600/110534 (81%)]\tAll Loss: 2.7892\tTriple Loss(1): 0.3187\tClassification Loss: 2.1518\n","Train Epoch: 1 [89920/110534 (81%)]\tAll Loss: 2.8351\tTriple Loss(1): 0.3775\tClassification Loss: 2.0800\n","Train Epoch: 1 [90240/110534 (82%)]\tAll Loss: 2.0197\tTriple Loss(1): 0.2858\tClassification Loss: 1.4481\n","Train Epoch: 1 [90560/110534 (82%)]\tAll Loss: 1.7117\tTriple Loss(1): 0.1398\tClassification Loss: 1.4321\n","Train Epoch: 1 [90880/110534 (82%)]\tAll Loss: 1.7422\tTriple Loss(0): 0.0000\tClassification Loss: 1.7422\n","Train Epoch: 1 [91200/110534 (82%)]\tAll Loss: 2.0855\tTriple Loss(1): 0.1686\tClassification Loss: 1.7483\n","Train Epoch: 1 [91520/110534 (83%)]\tAll Loss: 2.3808\tTriple Loss(1): 0.3919\tClassification Loss: 1.5970\n","Train Epoch: 1 [91840/110534 (83%)]\tAll Loss: 1.7672\tTriple Loss(0): 0.0000\tClassification Loss: 1.7672\n","Train Epoch: 1 [92160/110534 (83%)]\tAll Loss: 2.5439\tTriple Loss(1): 0.2743\tClassification Loss: 1.9953\n","Train Epoch: 1 [92480/110534 (84%)]\tAll Loss: 2.3065\tTriple Loss(1): 0.3451\tClassification Loss: 1.6163\n","\n","Test set: Average loss: 1.7517, Accuracy: 532/960 (55%)\n","\n","Train Epoch: 1 [92800/110534 (84%)]\tAll Loss: 1.6471\tTriple Loss(0): 0.0000\tClassification Loss: 1.6471\n","Train Epoch: 1 [93120/110534 (84%)]\tAll Loss: 1.9948\tTriple Loss(0): 0.0000\tClassification Loss: 1.9948\n","Train Epoch: 1 [93440/110534 (85%)]\tAll Loss: 2.1370\tTriple Loss(1): 0.1822\tClassification Loss: 1.7726\n","Train Epoch: 1 [93760/110534 (85%)]\tAll Loss: 2.3059\tTriple Loss(1): 0.4311\tClassification Loss: 1.4438\n","Train Epoch: 1 [94080/110534 (85%)]\tAll Loss: 2.5431\tTriple Loss(1): 0.4537\tClassification Loss: 1.6357\n","Train Epoch: 1 [94400/110534 (85%)]\tAll Loss: 1.8640\tTriple Loss(1): 0.0899\tClassification Loss: 1.6843\n","Train Epoch: 1 [94720/110534 (86%)]\tAll Loss: 2.0161\tTriple Loss(1): 0.1419\tClassification Loss: 1.7324\n","Train Epoch: 1 [95040/110534 (86%)]\tAll Loss: 2.9017\tTriple Loss(1): 0.4830\tClassification Loss: 1.9358\n","Train Epoch: 1 [95360/110534 (86%)]\tAll Loss: 2.5064\tTriple Loss(1): 0.3799\tClassification Loss: 1.7467\n","Train Epoch: 1 [95680/110534 (87%)]\tAll Loss: 6.8117\tTriple Loss(0): 2.5527\tClassification Loss: 1.7063\n","\n","Test set: Average loss: 1.7453, Accuracy: 544/960 (57%)\n","\n","Train Epoch: 1 [96000/110534 (87%)]\tAll Loss: 1.4101\tTriple Loss(0): 0.0000\tClassification Loss: 1.4101\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_3000.pth.tar\n","Train Epoch: 1 [96320/110534 (87%)]\tAll Loss: 1.8353\tTriple Loss(1): 0.1840\tClassification Loss: 1.4673\n","Train Epoch: 1 [96640/110534 (87%)]\tAll Loss: 2.2191\tTriple Loss(1): 0.1817\tClassification Loss: 1.8557\n","Train Epoch: 1 [96960/110534 (88%)]\tAll Loss: 1.3168\tTriple Loss(0): 0.0000\tClassification Loss: 1.3168\n","Train Epoch: 1 [97280/110534 (88%)]\tAll Loss: 1.9920\tTriple Loss(1): 0.2517\tClassification Loss: 1.4887\n","Train Epoch: 1 [97600/110534 (88%)]\tAll Loss: 2.3372\tTriple Loss(1): 0.1470\tClassification Loss: 2.0431\n","Train Epoch: 1 [97920/110534 (89%)]\tAll Loss: 1.6884\tTriple Loss(1): 0.2043\tClassification Loss: 1.2799\n","Train Epoch: 1 [98240/110534 (89%)]\tAll Loss: 2.0529\tTriple Loss(1): 0.3436\tClassification Loss: 1.3658\n","Train Epoch: 1 [98560/110534 (89%)]\tAll Loss: 2.7592\tTriple Loss(1): 0.5253\tClassification Loss: 1.7086\n","Train Epoch: 1 [98880/110534 (89%)]\tAll Loss: 2.2714\tTriple Loss(1): 0.3334\tClassification Loss: 1.6045\n","\n","Test set: Average loss: 1.7466, Accuracy: 531/960 (55%)\n","\n","Train Epoch: 1 [99200/110534 (90%)]\tAll Loss: 1.4943\tTriple Loss(0): 0.0000\tClassification Loss: 1.4943\n","Train Epoch: 1 [99520/110534 (90%)]\tAll Loss: 2.2640\tTriple Loss(1): 0.2768\tClassification Loss: 1.7104\n","Train Epoch: 1 [99840/110534 (90%)]\tAll Loss: 2.4267\tTriple Loss(1): 0.3344\tClassification Loss: 1.7579\n","Train Epoch: 1 [100160/110534 (91%)]\tAll Loss: 1.9971\tTriple Loss(0): 0.0000\tClassification Loss: 1.9971\n","Train Epoch: 1 [100480/110534 (91%)]\tAll Loss: 2.1782\tTriple Loss(1): 0.2529\tClassification Loss: 1.6725\n","Train Epoch: 1 [100800/110534 (91%)]\tAll Loss: 2.7019\tTriple Loss(1): 0.1748\tClassification Loss: 2.3522\n","Train Epoch: 1 [101120/110534 (91%)]\tAll Loss: 1.4711\tTriple Loss(0): 0.0000\tClassification Loss: 1.4711\n","Train Epoch: 1 [101440/110534 (92%)]\tAll Loss: 2.0412\tTriple Loss(1): 0.3027\tClassification Loss: 1.4359\n","Train Epoch: 1 [101760/110534 (92%)]\tAll Loss: 1.8082\tTriple Loss(1): 0.0902\tClassification Loss: 1.6278\n","Train Epoch: 1 [102080/110534 (92%)]\tAll Loss: 2.6796\tTriple Loss(1): 0.2749\tClassification Loss: 2.1299\n","\n","Test set: Average loss: 1.7372, Accuracy: 530/960 (55%)\n","\n","Train Epoch: 1 [102400/110534 (93%)]\tAll Loss: 2.4854\tTriple Loss(1): 0.3328\tClassification Loss: 1.8197\n","Train Epoch: 1 [102720/110534 (93%)]\tAll Loss: 2.5434\tTriple Loss(1): 0.3558\tClassification Loss: 1.8319\n","Train Epoch: 1 [103040/110534 (93%)]\tAll Loss: 2.0624\tTriple Loss(1): 0.2261\tClassification Loss: 1.6101\n","Train Epoch: 1 [103360/110534 (93%)]\tAll Loss: 2.2817\tTriple Loss(1): 0.2097\tClassification Loss: 1.8623\n","Train Epoch: 1 [103680/110534 (94%)]\tAll Loss: 2.1291\tTriple Loss(1): 0.1359\tClassification Loss: 1.8572\n","Train Epoch: 1 [104000/110534 (94%)]\tAll Loss: 1.6326\tTriple Loss(0): 0.0000\tClassification Loss: 1.6326\n","Train Epoch: 1 [104320/110534 (94%)]\tAll Loss: 2.5769\tTriple Loss(1): 0.2595\tClassification Loss: 2.0580\n","Train Epoch: 1 [104640/110534 (95%)]\tAll Loss: 1.9418\tTriple Loss(0): 0.0000\tClassification Loss: 1.9418\n","Train Epoch: 1 [104960/110534 (95%)]\tAll Loss: 2.2493\tTriple Loss(1): 0.3140\tClassification Loss: 1.6213\n","Train Epoch: 1 [105280/110534 (95%)]\tAll Loss: 2.4247\tTriple Loss(1): 0.3199\tClassification Loss: 1.7849\n","\n","Test set: Average loss: 1.7575, Accuracy: 517/960 (54%)\n","\n","Train Epoch: 1 [105600/110534 (96%)]\tAll Loss: 2.3755\tTriple Loss(1): 0.3085\tClassification Loss: 1.7585\n","Train Epoch: 1 [105920/110534 (96%)]\tAll Loss: 2.2612\tTriple Loss(1): 0.2635\tClassification Loss: 1.7341\n","Train Epoch: 1 [106240/110534 (96%)]\tAll Loss: 2.2239\tTriple Loss(1): 0.3401\tClassification Loss: 1.5436\n","Train Epoch: 1 [106560/110534 (96%)]\tAll Loss: 2.0847\tTriple Loss(1): 0.2904\tClassification Loss: 1.5039\n","Train Epoch: 1 [106880/110534 (97%)]\tAll Loss: 2.3152\tTriple Loss(1): 0.1991\tClassification Loss: 1.9169\n","Train Epoch: 1 [107200/110534 (97%)]\tAll Loss: 2.6478\tTriple Loss(1): 0.4249\tClassification Loss: 1.7981\n","Train Epoch: 1 [107520/110534 (97%)]\tAll Loss: 2.9075\tTriple Loss(1): 0.4104\tClassification Loss: 2.0868\n","Train Epoch: 1 [107840/110534 (98%)]\tAll Loss: 2.3895\tTriple Loss(1): 0.4341\tClassification Loss: 1.5214\n","Train Epoch: 1 [108160/110534 (98%)]\tAll Loss: 2.0814\tTriple Loss(1): 0.2965\tClassification Loss: 1.4883\n","Train Epoch: 1 [108480/110534 (98%)]\tAll Loss: 2.5762\tTriple Loss(1): 0.4362\tClassification Loss: 1.7038\n","\n","Test set: Average loss: 1.7241, Accuracy: 535/960 (56%)\n","\n","Train Epoch: 1 [108800/110534 (98%)]\tAll Loss: 2.6288\tTriple Loss(1): 0.4828\tClassification Loss: 1.6632\n","Train Epoch: 1 [109120/110534 (99%)]\tAll Loss: 2.3780\tTriple Loss(1): 0.3193\tClassification Loss: 1.7395\n","Train Epoch: 1 [109440/110534 (99%)]\tAll Loss: 1.6101\tTriple Loss(0): 0.0000\tClassification Loss: 1.6101\n","Train Epoch: 1 [109760/110534 (99%)]\tAll Loss: 2.0934\tTriple Loss(1): 0.2363\tClassification Loss: 1.6209\n","Train Epoch: 1 [110080/110534 (100%)]\tAll Loss: 2.1771\tTriple Loss(1): 0.2957\tClassification Loss: 1.5857\n","Train Epoch: 1 [110400/110534 (100%)]\tAll Loss: 2.1458\tTriple Loss(1): 0.2415\tClassification Loss: 1.6629\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_final.pth.tar\n","\n","Test set: Average loss: 1.7276, Accuracy: 535/960 (56%)\n","\n","Train Epoch: 2 [0/110534 (0%)]\tAll Loss: 2.4888\tTriple Loss(1): 0.4119\tClassification Loss: 1.6651\n","Train Epoch: 2 [320/110534 (0%)]\tAll Loss: 1.9379\tTriple Loss(1): 0.1140\tClassification Loss: 1.7098\n","Train Epoch: 2 [640/110534 (1%)]\tAll Loss: 3.1061\tTriple Loss(0): 1.0354\tClassification Loss: 1.0353\n","Train Epoch: 2 [960/110534 (1%)]\tAll Loss: 2.6403\tTriple Loss(1): 0.3697\tClassification Loss: 1.9010\n","Train Epoch: 2 [1280/110534 (1%)]\tAll Loss: 2.5230\tTriple Loss(1): 0.4533\tClassification Loss: 1.6164\n","Train Epoch: 2 [1600/110534 (1%)]\tAll Loss: 2.4727\tTriple Loss(1): 0.3389\tClassification Loss: 1.7949\n","Train Epoch: 2 [1920/110534 (2%)]\tAll Loss: 1.8828\tTriple Loss(1): 0.0665\tClassification Loss: 1.7497\n","Train Epoch: 2 [2240/110534 (2%)]\tAll Loss: 2.1663\tTriple Loss(1): 0.2365\tClassification Loss: 1.6934\n","Train Epoch: 2 [2560/110534 (2%)]\tAll Loss: 1.6943\tTriple Loss(0): 0.0000\tClassification Loss: 1.6943\n","Train Epoch: 2 [2880/110534 (3%)]\tAll Loss: 2.3294\tTriple Loss(1): 0.1955\tClassification Loss: 1.9384\n","\n","Test set: Average loss: 1.7290, Accuracy: 543/960 (57%)\n","\n","Train Epoch: 2 [3200/110534 (3%)]\tAll Loss: 2.8168\tTriple Loss(1): 0.2830\tClassification Loss: 2.2508\n","Train Epoch: 2 [3520/110534 (3%)]\tAll Loss: 1.6901\tTriple Loss(1): 0.2318\tClassification Loss: 1.2266\n","Train Epoch: 2 [3840/110534 (3%)]\tAll Loss: 2.3281\tTriple Loss(1): 0.3439\tClassification Loss: 1.6402\n","Train Epoch: 2 [4160/110534 (4%)]\tAll Loss: 2.7172\tTriple Loss(1): 0.5102\tClassification Loss: 1.6969\n","Train Epoch: 2 [4480/110534 (4%)]\tAll Loss: 2.1177\tTriple Loss(1): 0.3157\tClassification Loss: 1.4864\n","Train Epoch: 2 [4800/110534 (4%)]\tAll Loss: 1.9950\tTriple Loss(1): 0.2141\tClassification Loss: 1.5669\n","Train Epoch: 2 [5120/110534 (5%)]\tAll Loss: 2.8651\tTriple Loss(1): 0.4585\tClassification Loss: 1.9481\n","Train Epoch: 2 [5440/110534 (5%)]\tAll Loss: 2.1426\tTriple Loss(1): 0.1962\tClassification Loss: 1.7502\n","Train Epoch: 2 [5760/110534 (5%)]\tAll Loss: 1.9910\tTriple Loss(1): 0.2595\tClassification Loss: 1.4720\n","Train Epoch: 2 [6080/110534 (5%)]\tAll Loss: 1.6211\tTriple Loss(0): 0.0000\tClassification Loss: 1.6211\n","\n","Test set: Average loss: 1.7271, Accuracy: 538/960 (56%)\n","\n","Train Epoch: 2 [6400/110534 (6%)]\tAll Loss: 2.5488\tTriple Loss(1): 0.2871\tClassification Loss: 1.9746\n","Train Epoch: 2 [6720/110534 (6%)]\tAll Loss: 2.0933\tTriple Loss(1): 0.1458\tClassification Loss: 1.8016\n","Train Epoch: 2 [7040/110534 (6%)]\tAll Loss: 2.2270\tTriple Loss(1): 0.2193\tClassification Loss: 1.7884\n","Train Epoch: 2 [7360/110534 (7%)]\tAll Loss: 1.7071\tTriple Loss(0): 0.0000\tClassification Loss: 1.7071\n","Train Epoch: 2 [7680/110534 (7%)]\tAll Loss: 2.0665\tTriple Loss(1): 0.2850\tClassification Loss: 1.4966\n","Train Epoch: 2 [8000/110534 (7%)]\tAll Loss: 1.9871\tTriple Loss(1): 0.3577\tClassification Loss: 1.2716\n","Train Epoch: 2 [8320/110534 (8%)]\tAll Loss: 1.6625\tTriple Loss(0): 0.0000\tClassification Loss: 1.6625\n","Train Epoch: 2 [8640/110534 (8%)]\tAll Loss: 1.9762\tTriple Loss(1): 0.2428\tClassification Loss: 1.4906\n","Train Epoch: 2 [8960/110534 (8%)]\tAll Loss: 1.8285\tTriple Loss(1): 0.1404\tClassification Loss: 1.5476\n","Train Epoch: 2 [9280/110534 (8%)]\tAll Loss: 5.3859\tTriple Loss(0): 1.9683\tClassification Loss: 1.4494\n","\n","Test set: Average loss: 1.7165, Accuracy: 544/960 (57%)\n","\n","Train Epoch: 2 [9600/110534 (9%)]\tAll Loss: 2.5125\tTriple Loss(1): 0.2882\tClassification Loss: 1.9361\n","Train Epoch: 2 [9920/110534 (9%)]\tAll Loss: 2.1329\tTriple Loss(1): 0.1695\tClassification Loss: 1.7939\n","Train Epoch: 2 [10240/110534 (9%)]\tAll Loss: 1.8770\tTriple Loss(0): 0.0000\tClassification Loss: 1.8770\n","Train Epoch: 2 [10560/110534 (10%)]\tAll Loss: 2.6242\tTriple Loss(1): 0.3769\tClassification Loss: 1.8704\n","Train Epoch: 2 [10880/110534 (10%)]\tAll Loss: 2.4676\tTriple Loss(1): 0.2982\tClassification Loss: 1.8711\n","Train Epoch: 2 [11200/110534 (10%)]\tAll Loss: 1.9896\tTriple Loss(1): 0.2266\tClassification Loss: 1.5365\n","Train Epoch: 2 [11520/110534 (10%)]\tAll Loss: 2.1402\tTriple Loss(1): 0.2774\tClassification Loss: 1.5854\n","Train Epoch: 2 [11840/110534 (11%)]\tAll Loss: 1.9730\tTriple Loss(1): 0.1723\tClassification Loss: 1.6283\n","Train Epoch: 2 [12160/110534 (11%)]\tAll Loss: 2.3370\tTriple Loss(1): 0.3032\tClassification Loss: 1.7306\n","Train Epoch: 2 [12480/110534 (11%)]\tAll Loss: 1.7091\tTriple Loss(0): 0.0000\tClassification Loss: 1.7091\n","\n","Test set: Average loss: 1.7131, Accuracy: 543/960 (57%)\n","\n","Train Epoch: 2 [12800/110534 (12%)]\tAll Loss: 1.8863\tTriple Loss(1): 0.1178\tClassification Loss: 1.6507\n","Train Epoch: 2 [13120/110534 (12%)]\tAll Loss: 1.7322\tTriple Loss(1): 0.2247\tClassification Loss: 1.2828\n","Train Epoch: 2 [13440/110534 (12%)]\tAll Loss: 1.8523\tTriple Loss(0): 0.0000\tClassification Loss: 1.8523\n","Train Epoch: 2 [13760/110534 (12%)]\tAll Loss: 2.2200\tTriple Loss(1): 0.2416\tClassification Loss: 1.7368\n","Train Epoch: 2 [14080/110534 (13%)]\tAll Loss: 1.5162\tTriple Loss(0): 0.0000\tClassification Loss: 1.5162\n","Train Epoch: 2 [14400/110534 (13%)]\tAll Loss: 2.7163\tTriple Loss(1): 0.2974\tClassification Loss: 2.1216\n","Train Epoch: 2 [14720/110534 (13%)]\tAll Loss: 1.7607\tTriple Loss(0): 0.0000\tClassification Loss: 1.7607\n","Train Epoch: 2 [15040/110534 (14%)]\tAll Loss: 2.2110\tTriple Loss(0): 0.2735\tClassification Loss: 1.6640\n","Train Epoch: 2 [15360/110534 (14%)]\tAll Loss: 2.1503\tTriple Loss(1): 0.2862\tClassification Loss: 1.5779\n","Train Epoch: 2 [15680/110534 (14%)]\tAll Loss: 2.1043\tTriple Loss(1): 0.2909\tClassification Loss: 1.5226\n","\n","Test set: Average loss: 1.7034, Accuracy: 541/960 (56%)\n","\n","Train Epoch: 2 [16000/110534 (14%)]\tAll Loss: 2.3310\tTriple Loss(1): 0.2466\tClassification Loss: 1.8378\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_500.pth.tar\n","Train Epoch: 2 [16320/110534 (15%)]\tAll Loss: 2.0527\tTriple Loss(1): 0.3712\tClassification Loss: 1.3103\n","Train Epoch: 2 [16640/110534 (15%)]\tAll Loss: 2.6672\tTriple Loss(1): 0.3718\tClassification Loss: 1.9235\n","Train Epoch: 2 [16960/110534 (15%)]\tAll Loss: 2.6399\tTriple Loss(1): 0.3046\tClassification Loss: 2.0308\n","Train Epoch: 2 [17280/110534 (16%)]\tAll Loss: 1.6875\tTriple Loss(1): 0.0874\tClassification Loss: 1.5126\n","Train Epoch: 2 [17600/110534 (16%)]\tAll Loss: 2.4644\tTriple Loss(1): 0.4327\tClassification Loss: 1.5990\n","Train Epoch: 2 [17920/110534 (16%)]\tAll Loss: 1.8505\tTriple Loss(0): 0.0000\tClassification Loss: 1.8505\n","Train Epoch: 2 [18240/110534 (16%)]\tAll Loss: 2.0013\tTriple Loss(0): 0.0000\tClassification Loss: 2.0013\n","Train Epoch: 2 [18560/110534 (17%)]\tAll Loss: 1.5661\tTriple Loss(0): 0.0000\tClassification Loss: 1.5661\n","Train Epoch: 2 [18880/110534 (17%)]\tAll Loss: 2.3244\tTriple Loss(1): 0.3121\tClassification Loss: 1.7002\n","\n","Test set: Average loss: 1.7087, Accuracy: 538/960 (56%)\n","\n","Train Epoch: 2 [19200/110534 (17%)]\tAll Loss: 1.9395\tTriple Loss(1): 0.2556\tClassification Loss: 1.4283\n","Train Epoch: 2 [19520/110534 (18%)]\tAll Loss: 2.4110\tTriple Loss(1): 0.2813\tClassification Loss: 1.8483\n","Train Epoch: 2 [19840/110534 (18%)]\tAll Loss: 2.1709\tTriple Loss(1): 0.2929\tClassification Loss: 1.5851\n","Train Epoch: 2 [20160/110534 (18%)]\tAll Loss: 2.0533\tTriple Loss(1): 0.2123\tClassification Loss: 1.6286\n","Train Epoch: 2 [20480/110534 (19%)]\tAll Loss: 2.1315\tTriple Loss(1): 0.2673\tClassification Loss: 1.5970\n","Train Epoch: 2 [20800/110534 (19%)]\tAll Loss: 2.6052\tTriple Loss(1): 0.2684\tClassification Loss: 2.0684\n","Train Epoch: 2 [21120/110534 (19%)]\tAll Loss: 1.6049\tTriple Loss(0): 0.0000\tClassification Loss: 1.6049\n","Train Epoch: 2 [21440/110534 (19%)]\tAll Loss: 2.0198\tTriple Loss(1): 0.2723\tClassification Loss: 1.4751\n","Train Epoch: 2 [21760/110534 (20%)]\tAll Loss: 2.1268\tTriple Loss(0): 0.0000\tClassification Loss: 2.1268\n","Train Epoch: 2 [22080/110534 (20%)]\tAll Loss: 2.3836\tTriple Loss(1): 0.3423\tClassification Loss: 1.6990\n","\n","Test set: Average loss: 1.7116, Accuracy: 519/960 (54%)\n","\n","Train Epoch: 2 [22400/110534 (20%)]\tAll Loss: 2.2553\tTriple Loss(1): 0.3756\tClassification Loss: 1.5042\n","Train Epoch: 2 [22720/110534 (21%)]\tAll Loss: 1.7874\tTriple Loss(0): 0.0000\tClassification Loss: 1.7874\n","Train Epoch: 2 [23040/110534 (21%)]\tAll Loss: 2.2437\tTriple Loss(1): 0.2144\tClassification Loss: 1.8149\n","Train Epoch: 2 [23360/110534 (21%)]\tAll Loss: 2.0689\tTriple Loss(1): 0.1629\tClassification Loss: 1.7432\n","Train Epoch: 2 [23680/110534 (21%)]\tAll Loss: 2.5378\tTriple Loss(1): 0.3068\tClassification Loss: 1.9241\n","Train Epoch: 2 [24000/110534 (22%)]\tAll Loss: 2.6956\tTriple Loss(1): 0.5635\tClassification Loss: 1.5685\n","Train Epoch: 2 [24320/110534 (22%)]\tAll Loss: 2.0538\tTriple Loss(1): 0.1398\tClassification Loss: 1.7742\n","Train Epoch: 2 [24640/110534 (22%)]\tAll Loss: 2.5665\tTriple Loss(1): 0.1888\tClassification Loss: 2.1890\n","Train Epoch: 2 [24960/110534 (23%)]\tAll Loss: 1.8926\tTriple Loss(1): 0.1344\tClassification Loss: 1.6238\n","Train Epoch: 2 [25280/110534 (23%)]\tAll Loss: 2.3642\tTriple Loss(1): 0.3091\tClassification Loss: 1.7461\n","\n","Test set: Average loss: 1.6995, Accuracy: 537/960 (56%)\n","\n","Train Epoch: 2 [25600/110534 (23%)]\tAll Loss: 2.0844\tTriple Loss(1): 0.1718\tClassification Loss: 1.7408\n","Train Epoch: 2 [25920/110534 (23%)]\tAll Loss: 1.8263\tTriple Loss(0): 0.0000\tClassification Loss: 1.8263\n","Train Epoch: 2 [26240/110534 (24%)]\tAll Loss: 1.8329\tTriple Loss(1): 0.0921\tClassification Loss: 1.6486\n","Train Epoch: 2 [26560/110534 (24%)]\tAll Loss: 2.8687\tTriple Loss(1): 0.4491\tClassification Loss: 1.9704\n","Train Epoch: 2 [26880/110534 (24%)]\tAll Loss: 2.3955\tTriple Loss(1): 0.2196\tClassification Loss: 1.9563\n","Train Epoch: 2 [27200/110534 (25%)]\tAll Loss: 3.0978\tTriple Loss(1): 0.5665\tClassification Loss: 1.9648\n","Train Epoch: 2 [27520/110534 (25%)]\tAll Loss: 2.1992\tTriple Loss(1): 0.2747\tClassification Loss: 1.6498\n","Train Epoch: 2 [27840/110534 (25%)]\tAll Loss: 2.0458\tTriple Loss(1): 0.2615\tClassification Loss: 1.5229\n","Train Epoch: 2 [28160/110534 (25%)]\tAll Loss: 2.6621\tTriple Loss(1): 0.3085\tClassification Loss: 2.0452\n","Train Epoch: 2 [28480/110534 (26%)]\tAll Loss: 2.4246\tTriple Loss(0): 0.2080\tClassification Loss: 2.0085\n","\n","Test set: Average loss: 1.6955, Accuracy: 543/960 (57%)\n","\n","Train Epoch: 2 [28800/110534 (26%)]\tAll Loss: 1.4340\tTriple Loss(0): 0.0000\tClassification Loss: 1.4340\n","Train Epoch: 2 [29120/110534 (26%)]\tAll Loss: 1.9399\tTriple Loss(1): 0.0777\tClassification Loss: 1.7846\n","Train Epoch: 2 [29440/110534 (27%)]\tAll Loss: 2.5564\tTriple Loss(1): 0.4713\tClassification Loss: 1.6137\n","Train Epoch: 2 [29760/110534 (27%)]\tAll Loss: 1.7505\tTriple Loss(1): 0.1493\tClassification Loss: 1.4520\n","Train Epoch: 2 [30080/110534 (27%)]\tAll Loss: 1.9909\tTriple Loss(1): 0.2490\tClassification Loss: 1.4928\n","Train Epoch: 2 [30400/110534 (27%)]\tAll Loss: 1.5536\tTriple Loss(0): 0.0000\tClassification Loss: 1.5536\n","Train Epoch: 2 [30720/110534 (28%)]\tAll Loss: 2.2994\tTriple Loss(1): 0.4106\tClassification Loss: 1.4783\n","Train Epoch: 2 [31040/110534 (28%)]\tAll Loss: 2.3579\tTriple Loss(1): 0.2380\tClassification Loss: 1.8820\n","Train Epoch: 2 [31360/110534 (28%)]\tAll Loss: 2.3309\tTriple Loss(1): 0.2575\tClassification Loss: 1.8158\n","Train Epoch: 2 [31680/110534 (29%)]\tAll Loss: 2.0479\tTriple Loss(1): 0.1818\tClassification Loss: 1.6843\n","\n","Test set: Average loss: 1.6989, Accuracy: 533/960 (56%)\n","\n","Train Epoch: 2 [32000/110534 (29%)]\tAll Loss: 2.2790\tTriple Loss(1): 0.2495\tClassification Loss: 1.7801\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_1000.pth.tar\n","Train Epoch: 2 [32320/110534 (29%)]\tAll Loss: 2.2293\tTriple Loss(1): 0.3772\tClassification Loss: 1.4748\n","Train Epoch: 2 [32640/110534 (30%)]\tAll Loss: 2.1376\tTriple Loss(1): 0.1651\tClassification Loss: 1.8075\n","Train Epoch: 2 [32960/110534 (30%)]\tAll Loss: 2.6559\tTriple Loss(1): 0.4026\tClassification Loss: 1.8507\n","Train Epoch: 2 [33280/110534 (30%)]\tAll Loss: 2.2985\tTriple Loss(1): 0.2356\tClassification Loss: 1.8273\n","Train Epoch: 2 [33600/110534 (30%)]\tAll Loss: 1.9524\tTriple Loss(1): 0.1834\tClassification Loss: 1.5856\n","Train Epoch: 2 [33920/110534 (31%)]\tAll Loss: 7.8739\tTriple Loss(0): 3.1530\tClassification Loss: 1.5679\n","Train Epoch: 2 [34240/110534 (31%)]\tAll Loss: 2.7509\tTriple Loss(1): 0.5702\tClassification Loss: 1.6105\n","Train Epoch: 2 [34560/110534 (31%)]\tAll Loss: 2.4724\tTriple Loss(1): 0.4983\tClassification Loss: 1.4758\n","Train Epoch: 2 [34880/110534 (32%)]\tAll Loss: 1.4401\tTriple Loss(0): 0.0000\tClassification Loss: 1.4401\n","\n","Test set: Average loss: 1.6903, Accuracy: 539/960 (56%)\n","\n","Train Epoch: 2 [35200/110534 (32%)]\tAll Loss: 2.7976\tTriple Loss(1): 0.3994\tClassification Loss: 1.9987\n","Train Epoch: 2 [35520/110534 (32%)]\tAll Loss: 2.1194\tTriple Loss(1): 0.1015\tClassification Loss: 1.9164\n","Train Epoch: 2 [35840/110534 (32%)]\tAll Loss: 1.7293\tTriple Loss(1): 0.1504\tClassification Loss: 1.4285\n","Train Epoch: 2 [36160/110534 (33%)]\tAll Loss: 1.8832\tTriple Loss(1): 0.1570\tClassification Loss: 1.5693\n","Train Epoch: 2 [36480/110534 (33%)]\tAll Loss: 2.5406\tTriple Loss(1): 0.3438\tClassification Loss: 1.8531\n","Train Epoch: 2 [36800/110534 (33%)]\tAll Loss: 2.0727\tTriple Loss(1): 0.2235\tClassification Loss: 1.6256\n","Train Epoch: 2 [37120/110534 (34%)]\tAll Loss: 2.2950\tTriple Loss(1): 0.2119\tClassification Loss: 1.8713\n","Train Epoch: 2 [37440/110534 (34%)]\tAll Loss: 2.0083\tTriple Loss(0): 0.0000\tClassification Loss: 2.0083\n","Train Epoch: 2 [37760/110534 (34%)]\tAll Loss: 2.5735\tTriple Loss(1): 0.4206\tClassification Loss: 1.7324\n","Train Epoch: 2 [38080/110534 (34%)]\tAll Loss: 2.3746\tTriple Loss(1): 0.3393\tClassification Loss: 1.6959\n","\n","Test set: Average loss: 1.6885, Accuracy: 535/960 (56%)\n","\n","Train Epoch: 2 [38400/110534 (35%)]\tAll Loss: 1.7433\tTriple Loss(1): 0.1770\tClassification Loss: 1.3892\n","Train Epoch: 2 [38720/110534 (35%)]\tAll Loss: 1.8362\tTriple Loss(0): 0.0000\tClassification Loss: 1.8362\n","Train Epoch: 2 [39040/110534 (35%)]\tAll Loss: 1.8537\tTriple Loss(1): 0.1941\tClassification Loss: 1.4656\n","Train Epoch: 2 [39360/110534 (36%)]\tAll Loss: 2.1808\tTriple Loss(1): 0.2845\tClassification Loss: 1.6119\n","Train Epoch: 2 [39680/110534 (36%)]\tAll Loss: 2.3698\tTriple Loss(1): 0.1858\tClassification Loss: 1.9981\n","Train Epoch: 2 [40000/110534 (36%)]\tAll Loss: 1.9526\tTriple Loss(1): 0.3570\tClassification Loss: 1.2386\n","Train Epoch: 2 [40320/110534 (36%)]\tAll Loss: 2.5555\tTriple Loss(1): 0.3686\tClassification Loss: 1.8184\n","Train Epoch: 2 [40640/110534 (37%)]\tAll Loss: 2.5359\tTriple Loss(1): 0.3273\tClassification Loss: 1.8813\n","Train Epoch: 2 [40960/110534 (37%)]\tAll Loss: 2.0179\tTriple Loss(1): 0.2364\tClassification Loss: 1.5450\n","Train Epoch: 2 [41280/110534 (37%)]\tAll Loss: 2.3799\tTriple Loss(1): 0.2943\tClassification Loss: 1.7914\n","\n","Test set: Average loss: 1.6858, Accuracy: 535/960 (56%)\n","\n","Train Epoch: 2 [41600/110534 (38%)]\tAll Loss: 1.8126\tTriple Loss(0): 0.0000\tClassification Loss: 1.8126\n","Train Epoch: 2 [41920/110534 (38%)]\tAll Loss: 1.8717\tTriple Loss(1): 0.1132\tClassification Loss: 1.6454\n","Train Epoch: 2 [42240/110534 (38%)]\tAll Loss: 2.3829\tTriple Loss(1): 0.3631\tClassification Loss: 1.6566\n","Train Epoch: 2 [42560/110534 (38%)]\tAll Loss: 1.8253\tTriple Loss(0): 0.0000\tClassification Loss: 1.8253\n","Train Epoch: 2 [42880/110534 (39%)]\tAll Loss: 1.8487\tTriple Loss(0): 0.0000\tClassification Loss: 1.8487\n","Train Epoch: 2 [43200/110534 (39%)]\tAll Loss: 2.2524\tTriple Loss(1): 0.2966\tClassification Loss: 1.6592\n","Train Epoch: 2 [43520/110534 (39%)]\tAll Loss: 1.9347\tTriple Loss(1): 0.1892\tClassification Loss: 1.5563\n","Train Epoch: 2 [43840/110534 (40%)]\tAll Loss: 2.0369\tTriple Loss(1): 0.2353\tClassification Loss: 1.5662\n","Train Epoch: 2 [44160/110534 (40%)]\tAll Loss: 2.7770\tTriple Loss(1): 0.3737\tClassification Loss: 2.0296\n","Train Epoch: 2 [44480/110534 (40%)]\tAll Loss: 1.6665\tTriple Loss(0): 0.0000\tClassification Loss: 1.6665\n","\n","Test set: Average loss: 1.6857, Accuracy: 543/960 (57%)\n","\n","Train Epoch: 2 [44800/110534 (41%)]\tAll Loss: 1.2702\tTriple Loss(0): 0.0000\tClassification Loss: 1.2702\n","Train Epoch: 2 [45120/110534 (41%)]\tAll Loss: 2.2318\tTriple Loss(1): 0.3212\tClassification Loss: 1.5894\n","Train Epoch: 2 [45440/110534 (41%)]\tAll Loss: 2.1643\tTriple Loss(1): 0.2339\tClassification Loss: 1.6964\n","Train Epoch: 2 [45760/110534 (41%)]\tAll Loss: 1.4610\tTriple Loss(0): 0.0000\tClassification Loss: 1.4610\n","Train Epoch: 2 [46080/110534 (42%)]\tAll Loss: 1.2680\tTriple Loss(0): 0.0000\tClassification Loss: 1.2680\n","Train Epoch: 2 [46400/110534 (42%)]\tAll Loss: 2.4933\tTriple Loss(1): 0.3189\tClassification Loss: 1.8555\n","Train Epoch: 2 [46720/110534 (42%)]\tAll Loss: 1.7976\tTriple Loss(0): 0.0000\tClassification Loss: 1.7976\n","Train Epoch: 2 [47040/110534 (43%)]\tAll Loss: 2.4857\tTriple Loss(1): 0.3821\tClassification Loss: 1.7216\n","Train Epoch: 2 [47360/110534 (43%)]\tAll Loss: 2.1261\tTriple Loss(1): 0.2403\tClassification Loss: 1.6455\n","Train Epoch: 2 [47680/110534 (43%)]\tAll Loss: 1.6445\tTriple Loss(1): 0.1569\tClassification Loss: 1.3307\n","\n","Test set: Average loss: 1.6887, Accuracy: 539/960 (56%)\n","\n","Train Epoch: 2 [48000/110534 (43%)]\tAll Loss: 4.4430\tTriple Loss(0): 1.2317\tClassification Loss: 1.9797\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_1500.pth.tar\n","Train Epoch: 2 [48320/110534 (44%)]\tAll Loss: 2.3963\tTriple Loss(1): 0.3189\tClassification Loss: 1.7585\n","Train Epoch: 2 [48640/110534 (44%)]\tAll Loss: 1.8761\tTriple Loss(0): 0.0000\tClassification Loss: 1.8761\n","Train Epoch: 2 [48960/110534 (44%)]\tAll Loss: 2.5714\tTriple Loss(1): 0.3741\tClassification Loss: 1.8231\n","Train Epoch: 2 [49280/110534 (45%)]\tAll Loss: 2.1606\tTriple Loss(1): 0.3252\tClassification Loss: 1.5103\n","Train Epoch: 2 [49600/110534 (45%)]\tAll Loss: 2.5213\tTriple Loss(1): 0.3579\tClassification Loss: 1.8055\n","Train Epoch: 2 [49920/110534 (45%)]\tAll Loss: 1.9394\tTriple Loss(1): 0.2814\tClassification Loss: 1.3767\n","Train Epoch: 2 [50240/110534 (45%)]\tAll Loss: 1.8231\tTriple Loss(1): 0.1122\tClassification Loss: 1.5988\n","Train Epoch: 2 [50560/110534 (46%)]\tAll Loss: 2.0492\tTriple Loss(1): 0.1633\tClassification Loss: 1.7226\n","Train Epoch: 2 [50880/110534 (46%)]\tAll Loss: 2.4563\tTriple Loss(1): 0.3262\tClassification Loss: 1.8039\n","\n","Test set: Average loss: 1.6805, Accuracy: 544/960 (57%)\n","\n","Train Epoch: 2 [51200/110534 (46%)]\tAll Loss: 5.4395\tTriple Loss(0): 1.9318\tClassification Loss: 1.5760\n","Train Epoch: 2 [51520/110534 (47%)]\tAll Loss: 2.5007\tTriple Loss(1): 0.4199\tClassification Loss: 1.6609\n","Train Epoch: 2 [51840/110534 (47%)]\tAll Loss: 2.3323\tTriple Loss(1): 0.3514\tClassification Loss: 1.6295\n","Train Epoch: 2 [52160/110534 (47%)]\tAll Loss: 2.8511\tTriple Loss(1): 0.4236\tClassification Loss: 2.0039\n","Train Epoch: 2 [52480/110534 (47%)]\tAll Loss: 2.3624\tTriple Loss(1): 0.0829\tClassification Loss: 2.1966\n","Train Epoch: 2 [52800/110534 (48%)]\tAll Loss: 1.9530\tTriple Loss(1): 0.2018\tClassification Loss: 1.5494\n","Train Epoch: 2 [53120/110534 (48%)]\tAll Loss: 1.4486\tTriple Loss(0): 0.0000\tClassification Loss: 1.4486\n","Train Epoch: 2 [53440/110534 (48%)]\tAll Loss: 8.3666\tTriple Loss(0): 3.2471\tClassification Loss: 1.8724\n","Train Epoch: 2 [53760/110534 (49%)]\tAll Loss: 2.3406\tTriple Loss(1): 0.2747\tClassification Loss: 1.7912\n","Train Epoch: 2 [54080/110534 (49%)]\tAll Loss: 1.7550\tTriple Loss(0): 0.0000\tClassification Loss: 1.7550\n","\n","Test set: Average loss: 1.6849, Accuracy: 546/960 (57%)\n","\n","Train Epoch: 2 [54400/110534 (49%)]\tAll Loss: 2.6238\tTriple Loss(1): 0.4617\tClassification Loss: 1.7004\n","Train Epoch: 2 [54720/110534 (49%)]\tAll Loss: 2.3234\tTriple Loss(1): 0.3148\tClassification Loss: 1.6937\n","Train Epoch: 2 [55040/110534 (50%)]\tAll Loss: 2.7421\tTriple Loss(1): 0.3915\tClassification Loss: 1.9591\n","Train Epoch: 2 [55360/110534 (50%)]\tAll Loss: 1.4227\tTriple Loss(0): 0.0000\tClassification Loss: 1.4227\n","Train Epoch: 2 [55680/110534 (50%)]\tAll Loss: 2.3809\tTriple Loss(1): 0.2776\tClassification Loss: 1.8257\n","Train Epoch: 2 [56000/110534 (51%)]\tAll Loss: 2.5533\tTriple Loss(1): 0.2423\tClassification Loss: 2.0687\n","Train Epoch: 2 [56320/110534 (51%)]\tAll Loss: 1.8539\tTriple Loss(0): 0.0000\tClassification Loss: 1.8539\n","Train Epoch: 2 [56640/110534 (51%)]\tAll Loss: 1.8054\tTriple Loss(1): 0.2720\tClassification Loss: 1.2613\n","Train Epoch: 2 [56960/110534 (52%)]\tAll Loss: 2.1948\tTriple Loss(1): 0.2064\tClassification Loss: 1.7820\n","Train Epoch: 2 [57280/110534 (52%)]\tAll Loss: 2.4135\tTriple Loss(1): 0.2962\tClassification Loss: 1.8211\n","\n","Test set: Average loss: 1.6769, Accuracy: 552/960 (58%)\n","\n","Train Epoch: 2 [57600/110534 (52%)]\tAll Loss: 2.3116\tTriple Loss(1): 0.2254\tClassification Loss: 1.8609\n","Train Epoch: 2 [57920/110534 (52%)]\tAll Loss: 2.5277\tTriple Loss(1): 0.3809\tClassification Loss: 1.7659\n","Train Epoch: 2 [58240/110534 (53%)]\tAll Loss: 1.8676\tTriple Loss(1): 0.2463\tClassification Loss: 1.3750\n","Train Epoch: 2 [58560/110534 (53%)]\tAll Loss: 1.5860\tTriple Loss(0): 0.0000\tClassification Loss: 1.5860\n","Train Epoch: 2 [58880/110534 (53%)]\tAll Loss: 2.2577\tTriple Loss(1): 0.2192\tClassification Loss: 1.8193\n","Train Epoch: 2 [59200/110534 (54%)]\tAll Loss: 2.2929\tTriple Loss(1): 0.2316\tClassification Loss: 1.8298\n","Train Epoch: 2 [59520/110534 (54%)]\tAll Loss: 1.3778\tTriple Loss(0): 0.0000\tClassification Loss: 1.3778\n","Train Epoch: 2 [59840/110534 (54%)]\tAll Loss: 2.3819\tTriple Loss(1): 0.4661\tClassification Loss: 1.4497\n","Train Epoch: 2 [60160/110534 (54%)]\tAll Loss: 2.0080\tTriple Loss(1): 0.2097\tClassification Loss: 1.5886\n","Train Epoch: 2 [60480/110534 (55%)]\tAll Loss: 1.7734\tTriple Loss(1): 0.0735\tClassification Loss: 1.6264\n","\n","Test set: Average loss: 1.6746, Accuracy: 540/960 (56%)\n","\n","Train Epoch: 2 [60800/110534 (55%)]\tAll Loss: 1.9851\tTriple Loss(1): 0.0935\tClassification Loss: 1.7982\n","Train Epoch: 2 [61120/110534 (55%)]\tAll Loss: 2.4575\tTriple Loss(1): 0.3659\tClassification Loss: 1.7257\n","Train Epoch: 2 [61440/110534 (56%)]\tAll Loss: 1.8211\tTriple Loss(1): 0.3792\tClassification Loss: 1.0626\n","Train Epoch: 2 [61760/110534 (56%)]\tAll Loss: 2.1943\tTriple Loss(1): 0.2128\tClassification Loss: 1.7688\n","Train Epoch: 2 [62080/110534 (56%)]\tAll Loss: 2.3574\tTriple Loss(1): 0.3177\tClassification Loss: 1.7219\n","Train Epoch: 2 [62400/110534 (56%)]\tAll Loss: 2.4445\tTriple Loss(0): 0.4804\tClassification Loss: 1.4837\n","Train Epoch: 2 [62720/110534 (57%)]\tAll Loss: 1.4021\tTriple Loss(0): 0.0000\tClassification Loss: 1.4021\n","Train Epoch: 2 [63040/110534 (57%)]\tAll Loss: 3.1305\tTriple Loss(1): 0.5339\tClassification Loss: 2.0627\n","Train Epoch: 2 [63360/110534 (57%)]\tAll Loss: 2.1954\tTriple Loss(1): 0.2520\tClassification Loss: 1.6914\n","Train Epoch: 2 [63680/110534 (58%)]\tAll Loss: 1.4770\tTriple Loss(0): 0.0000\tClassification Loss: 1.4770\n","\n","Test set: Average loss: 1.6816, Accuracy: 541/960 (56%)\n","\n","Train Epoch: 2 [64000/110534 (58%)]\tAll Loss: 2.4947\tTriple Loss(1): 0.1955\tClassification Loss: 2.1038\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_2000.pth.tar\n","Train Epoch: 2 [64320/110534 (58%)]\tAll Loss: 2.1477\tTriple Loss(1): 0.3229\tClassification Loss: 1.5019\n","Train Epoch: 2 [64640/110534 (58%)]\tAll Loss: 4.8599\tTriple Loss(0): 1.3121\tClassification Loss: 2.2358\n","Train Epoch: 2 [64960/110534 (59%)]\tAll Loss: 2.3308\tTriple Loss(1): 0.3097\tClassification Loss: 1.7113\n","Train Epoch: 2 [65280/110534 (59%)]\tAll Loss: 1.8602\tTriple Loss(1): 0.0377\tClassification Loss: 1.7848\n","Train Epoch: 2 [65600/110534 (59%)]\tAll Loss: 1.9804\tTriple Loss(1): 0.1440\tClassification Loss: 1.6924\n","Train Epoch: 2 [65920/110534 (60%)]\tAll Loss: 1.9103\tTriple Loss(1): 0.1011\tClassification Loss: 1.7081\n","Train Epoch: 2 [66240/110534 (60%)]\tAll Loss: 1.7736\tTriple Loss(0): 0.0000\tClassification Loss: 1.7736\n","Train Epoch: 2 [66560/110534 (60%)]\tAll Loss: 1.9558\tTriple Loss(1): 0.2270\tClassification Loss: 1.5019\n","Train Epoch: 2 [66880/110534 (60%)]\tAll Loss: 1.7467\tTriple Loss(0): 0.0000\tClassification Loss: 1.7467\n","\n","Test set: Average loss: 1.6729, Accuracy: 533/960 (56%)\n","\n","Train Epoch: 2 [67200/110534 (61%)]\tAll Loss: 2.2452\tTriple Loss(1): 0.2022\tClassification Loss: 1.8409\n","Train Epoch: 2 [67520/110534 (61%)]\tAll Loss: 2.4317\tTriple Loss(1): 0.2716\tClassification Loss: 1.8886\n","Train Epoch: 2 [67840/110534 (61%)]\tAll Loss: 2.3753\tTriple Loss(1): 0.2353\tClassification Loss: 1.9047\n","Train Epoch: 2 [68160/110534 (62%)]\tAll Loss: 2.1274\tTriple Loss(1): 0.3165\tClassification Loss: 1.4944\n","Train Epoch: 2 [68480/110534 (62%)]\tAll Loss: 1.4172\tTriple Loss(0): 0.0000\tClassification Loss: 1.4172\n","Train Epoch: 2 [68800/110534 (62%)]\tAll Loss: 1.9005\tTriple Loss(1): 0.1482\tClassification Loss: 1.6042\n","Train Epoch: 2 [69120/110534 (63%)]\tAll Loss: 2.1510\tTriple Loss(0): 0.2272\tClassification Loss: 1.6966\n","Train Epoch: 2 [69440/110534 (63%)]\tAll Loss: 2.2385\tTriple Loss(1): 0.2939\tClassification Loss: 1.6507\n","Train Epoch: 2 [69760/110534 (63%)]\tAll Loss: 2.4104\tTriple Loss(1): 0.3484\tClassification Loss: 1.7135\n","Train Epoch: 2 [70080/110534 (63%)]\tAll Loss: 2.0236\tTriple Loss(1): 0.3436\tClassification Loss: 1.3364\n","\n","Test set: Average loss: 1.6682, Accuracy: 548/960 (57%)\n","\n","Train Epoch: 2 [70400/110534 (64%)]\tAll Loss: 2.3131\tTriple Loss(1): 0.3779\tClassification Loss: 1.5573\n","Train Epoch: 2 [70720/110534 (64%)]\tAll Loss: 1.7775\tTriple Loss(0): 0.2021\tClassification Loss: 1.3732\n","Train Epoch: 2 [71040/110534 (64%)]\tAll Loss: 2.4137\tTriple Loss(1): 0.4493\tClassification Loss: 1.5151\n","Train Epoch: 2 [71360/110534 (65%)]\tAll Loss: 2.4237\tTriple Loss(1): 0.2843\tClassification Loss: 1.8552\n","Train Epoch: 2 [71680/110534 (65%)]\tAll Loss: 2.5274\tTriple Loss(1): 0.3993\tClassification Loss: 1.7289\n","Train Epoch: 2 [72000/110534 (65%)]\tAll Loss: 2.0057\tTriple Loss(1): 0.2138\tClassification Loss: 1.5780\n","Train Epoch: 2 [72320/110534 (65%)]\tAll Loss: 1.9913\tTriple Loss(1): 0.2056\tClassification Loss: 1.5800\n","Train Epoch: 2 [72640/110534 (66%)]\tAll Loss: 1.9437\tTriple Loss(1): 0.2373\tClassification Loss: 1.4691\n","Train Epoch: 2 [72960/110534 (66%)]\tAll Loss: 1.6841\tTriple Loss(1): 0.1537\tClassification Loss: 1.3767\n","Train Epoch: 2 [73280/110534 (66%)]\tAll Loss: 1.2847\tTriple Loss(0): 0.0000\tClassification Loss: 1.2847\n","\n","Test set: Average loss: 1.6652, Accuracy: 545/960 (57%)\n","\n","Train Epoch: 2 [73600/110534 (67%)]\tAll Loss: 2.8814\tTriple Loss(1): 0.3406\tClassification Loss: 2.2002\n","Train Epoch: 2 [73920/110534 (67%)]\tAll Loss: 2.2439\tTriple Loss(1): 0.1486\tClassification Loss: 1.9466\n","Train Epoch: 2 [74240/110534 (67%)]\tAll Loss: 1.8759\tTriple Loss(1): 0.2104\tClassification Loss: 1.4550\n","Train Epoch: 2 [74560/110534 (67%)]\tAll Loss: 2.3478\tTriple Loss(1): 0.1687\tClassification Loss: 2.0105\n","Train Epoch: 2 [74880/110534 (68%)]\tAll Loss: 2.6002\tTriple Loss(1): 0.4244\tClassification Loss: 1.7513\n","Train Epoch: 2 [75200/110534 (68%)]\tAll Loss: 1.5798\tTriple Loss(0): 0.0000\tClassification Loss: 1.5798\n","Train Epoch: 2 [75520/110534 (68%)]\tAll Loss: 3.7064\tTriple Loss(0): 1.1333\tClassification Loss: 1.4397\n","Train Epoch: 2 [75840/110534 (69%)]\tAll Loss: 2.1227\tTriple Loss(1): 0.2768\tClassification Loss: 1.5691\n","Train Epoch: 2 [76160/110534 (69%)]\tAll Loss: 2.2644\tTriple Loss(1): 0.2883\tClassification Loss: 1.6877\n","Train Epoch: 2 [76480/110534 (69%)]\tAll Loss: 2.0898\tTriple Loss(1): 0.1945\tClassification Loss: 1.7009\n","\n","Test set: Average loss: 1.6621, Accuracy: 542/960 (56%)\n","\n","Train Epoch: 2 [76800/110534 (69%)]\tAll Loss: 1.9161\tTriple Loss(1): 0.2286\tClassification Loss: 1.4590\n","Train Epoch: 2 [77120/110534 (70%)]\tAll Loss: 2.3094\tTriple Loss(1): 0.3386\tClassification Loss: 1.6322\n","Train Epoch: 2 [77440/110534 (70%)]\tAll Loss: 2.4436\tTriple Loss(1): 0.4432\tClassification Loss: 1.5572\n","Train Epoch: 2 [77760/110534 (70%)]\tAll Loss: 1.6590\tTriple Loss(0): 0.0000\tClassification Loss: 1.6590\n","Train Epoch: 2 [78080/110534 (71%)]\tAll Loss: 2.3398\tTriple Loss(1): 0.1994\tClassification Loss: 1.9410\n","Train Epoch: 2 [78400/110534 (71%)]\tAll Loss: 2.2804\tTriple Loss(1): 0.3473\tClassification Loss: 1.5858\n","Train Epoch: 2 [78720/110534 (71%)]\tAll Loss: 1.9721\tTriple Loss(1): 0.2674\tClassification Loss: 1.4373\n","Train Epoch: 2 [79040/110534 (71%)]\tAll Loss: 1.8302\tTriple Loss(0): 0.0000\tClassification Loss: 1.8302\n","Train Epoch: 2 [79360/110534 (72%)]\tAll Loss: 3.2934\tTriple Loss(1): 0.6495\tClassification Loss: 1.9943\n","Train Epoch: 2 [79680/110534 (72%)]\tAll Loss: 1.7346\tTriple Loss(0): 0.1125\tClassification Loss: 1.5096\n","\n","Test set: Average loss: 1.6705, Accuracy: 543/960 (57%)\n","\n","Train Epoch: 2 [80000/110534 (72%)]\tAll Loss: 1.7868\tTriple Loss(1): 0.1214\tClassification Loss: 1.5439\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_2500.pth.tar\n","Train Epoch: 2 [80320/110534 (73%)]\tAll Loss: 1.7362\tTriple Loss(1): 0.2156\tClassification Loss: 1.3050\n","Train Epoch: 2 [80640/110534 (73%)]\tAll Loss: 2.0687\tTriple Loss(1): 0.2768\tClassification Loss: 1.5151\n","Train Epoch: 2 [80960/110534 (73%)]\tAll Loss: 2.0660\tTriple Loss(1): 0.1528\tClassification Loss: 1.7603\n","Train Epoch: 2 [81280/110534 (74%)]\tAll Loss: 2.2158\tTriple Loss(1): 0.3962\tClassification Loss: 1.4235\n","Train Epoch: 2 [81600/110534 (74%)]\tAll Loss: 2.0526\tTriple Loss(1): 0.3006\tClassification Loss: 1.4513\n","Train Epoch: 2 [81920/110534 (74%)]\tAll Loss: 1.7403\tTriple Loss(0): 0.0000\tClassification Loss: 1.7403\n","Train Epoch: 2 [82240/110534 (74%)]\tAll Loss: 2.0119\tTriple Loss(0): 0.0000\tClassification Loss: 2.0119\n","Train Epoch: 2 [82560/110534 (75%)]\tAll Loss: 2.1282\tTriple Loss(1): 0.2630\tClassification Loss: 1.6023\n","Train Epoch: 2 [82880/110534 (75%)]\tAll Loss: 2.9401\tTriple Loss(1): 0.4661\tClassification Loss: 2.0079\n","\n","Test set: Average loss: 1.6642, Accuracy: 547/960 (57%)\n","\n","Train Epoch: 2 [83200/110534 (75%)]\tAll Loss: 2.6491\tTriple Loss(1): 0.4791\tClassification Loss: 1.6909\n","Train Epoch: 2 [83520/110534 (76%)]\tAll Loss: 2.1388\tTriple Loss(1): 0.2522\tClassification Loss: 1.6344\n","Train Epoch: 2 [83840/110534 (76%)]\tAll Loss: 1.5501\tTriple Loss(1): 0.1077\tClassification Loss: 1.3348\n","Train Epoch: 2 [84160/110534 (76%)]\tAll Loss: 1.5154\tTriple Loss(1): 0.0899\tClassification Loss: 1.3357\n","Train Epoch: 2 [84480/110534 (76%)]\tAll Loss: 2.1333\tTriple Loss(1): 0.4090\tClassification Loss: 1.3153\n","Train Epoch: 2 [84800/110534 (77%)]\tAll Loss: 2.4755\tTriple Loss(1): 0.3304\tClassification Loss: 1.8147\n","Train Epoch: 2 [85120/110534 (77%)]\tAll Loss: 1.6974\tTriple Loss(1): 0.1493\tClassification Loss: 1.3989\n","Train Epoch: 2 [85440/110534 (77%)]\tAll Loss: 1.3685\tTriple Loss(0): 0.0000\tClassification Loss: 1.3685\n","Train Epoch: 2 [85760/110534 (78%)]\tAll Loss: 2.0540\tTriple Loss(1): 0.1693\tClassification Loss: 1.7154\n","Train Epoch: 2 [86080/110534 (78%)]\tAll Loss: 2.1369\tTriple Loss(1): 0.2520\tClassification Loss: 1.6329\n","\n","Test set: Average loss: 1.6589, Accuracy: 542/960 (56%)\n","\n","Train Epoch: 2 [86400/110534 (78%)]\tAll Loss: 2.7954\tTriple Loss(1): 0.3633\tClassification Loss: 2.0689\n","Train Epoch: 2 [86720/110534 (78%)]\tAll Loss: 2.0131\tTriple Loss(1): 0.2909\tClassification Loss: 1.4313\n","Train Epoch: 2 [87040/110534 (79%)]\tAll Loss: 1.9479\tTriple Loss(1): 0.2629\tClassification Loss: 1.4221\n","Train Epoch: 2 [87360/110534 (79%)]\tAll Loss: 2.2029\tTriple Loss(1): 0.3202\tClassification Loss: 1.5624\n","Train Epoch: 2 [87680/110534 (79%)]\tAll Loss: 1.9986\tTriple Loss(1): 0.1565\tClassification Loss: 1.6856\n","Train Epoch: 2 [88000/110534 (80%)]\tAll Loss: 2.7150\tTriple Loss(1): 0.2276\tClassification Loss: 2.2598\n","Train Epoch: 2 [88320/110534 (80%)]\tAll Loss: 2.0369\tTriple Loss(1): 0.1559\tClassification Loss: 1.7251\n","Train Epoch: 2 [88640/110534 (80%)]\tAll Loss: 2.4387\tTriple Loss(1): 0.2317\tClassification Loss: 1.9752\n","Train Epoch: 2 [88960/110534 (80%)]\tAll Loss: 1.7794\tTriple Loss(0): 0.0000\tClassification Loss: 1.7794\n","Train Epoch: 2 [89280/110534 (81%)]\tAll Loss: 2.6597\tTriple Loss(1): 0.4533\tClassification Loss: 1.7531\n","\n","Test set: Average loss: 1.6613, Accuracy: 543/960 (57%)\n","\n","Train Epoch: 2 [89600/110534 (81%)]\tAll Loss: 2.5266\tTriple Loss(1): 0.1955\tClassification Loss: 2.1357\n","Train Epoch: 2 [89920/110534 (81%)]\tAll Loss: 2.5861\tTriple Loss(1): 0.4182\tClassification Loss: 1.7497\n","Train Epoch: 2 [90240/110534 (82%)]\tAll Loss: 2.0395\tTriple Loss(1): 0.1985\tClassification Loss: 1.6424\n","Train Epoch: 2 [90560/110534 (82%)]\tAll Loss: 1.6235\tTriple Loss(1): 0.1489\tClassification Loss: 1.3257\n","Train Epoch: 2 [90880/110534 (82%)]\tAll Loss: 2.1334\tTriple Loss(1): 0.2267\tClassification Loss: 1.6801\n","Train Epoch: 2 [91200/110534 (82%)]\tAll Loss: 1.8708\tTriple Loss(1): 0.2210\tClassification Loss: 1.4289\n","Train Epoch: 2 [91520/110534 (83%)]\tAll Loss: 1.6401\tTriple Loss(0): 0.0000\tClassification Loss: 1.6401\n","Train Epoch: 2 [91840/110534 (83%)]\tAll Loss: 2.6120\tTriple Loss(1): 0.5362\tClassification Loss: 1.5396\n","Train Epoch: 2 [92160/110534 (83%)]\tAll Loss: 2.7132\tTriple Loss(1): 0.3545\tClassification Loss: 2.0042\n","Train Epoch: 2 [92480/110534 (84%)]\tAll Loss: 1.8850\tTriple Loss(1): 0.1350\tClassification Loss: 1.6151\n","\n","Test set: Average loss: 1.6691, Accuracy: 546/960 (57%)\n","\n","Train Epoch: 2 [92800/110534 (84%)]\tAll Loss: 1.9706\tTriple Loss(1): 0.1624\tClassification Loss: 1.6457\n","Train Epoch: 2 [93120/110534 (84%)]\tAll Loss: 1.9647\tTriple Loss(1): 0.0951\tClassification Loss: 1.7745\n","Train Epoch: 2 [93440/110534 (85%)]\tAll Loss: 3.0173\tTriple Loss(0): 0.6316\tClassification Loss: 1.7541\n","Train Epoch: 2 [93760/110534 (85%)]\tAll Loss: 2.2309\tTriple Loss(1): 0.3585\tClassification Loss: 1.5138\n","Train Epoch: 2 [94080/110534 (85%)]\tAll Loss: 1.7631\tTriple Loss(1): 0.1097\tClassification Loss: 1.5437\n","Train Epoch: 2 [94400/110534 (85%)]\tAll Loss: 2.2216\tTriple Loss(1): 0.2614\tClassification Loss: 1.6988\n","Train Epoch: 2 [94720/110534 (86%)]\tAll Loss: 1.7035\tTriple Loss(0): 0.0000\tClassification Loss: 1.7035\n","Train Epoch: 2 [95040/110534 (86%)]\tAll Loss: 1.9317\tTriple Loss(1): 0.1473\tClassification Loss: 1.6370\n","Train Epoch: 2 [95360/110534 (86%)]\tAll Loss: 2.1842\tTriple Loss(1): 0.2473\tClassification Loss: 1.6895\n","Train Epoch: 2 [95680/110534 (87%)]\tAll Loss: 1.8858\tTriple Loss(1): 0.1593\tClassification Loss: 1.5672\n","\n","Test set: Average loss: 1.6595, Accuracy: 542/960 (56%)\n","\n","Train Epoch: 2 [96000/110534 (87%)]\tAll Loss: 1.8131\tTriple Loss(1): 0.1966\tClassification Loss: 1.4199\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_3000.pth.tar\n","Train Epoch: 2 [96320/110534 (87%)]\tAll Loss: 1.8410\tTriple Loss(1): 0.1606\tClassification Loss: 1.5199\n","Train Epoch: 2 [96640/110534 (87%)]\tAll Loss: 1.7218\tTriple Loss(0): 0.0000\tClassification Loss: 1.7218\n","Train Epoch: 2 [96960/110534 (88%)]\tAll Loss: 1.3942\tTriple Loss(1): 0.0839\tClassification Loss: 1.2265\n","Train Epoch: 2 [97280/110534 (88%)]\tAll Loss: 2.0034\tTriple Loss(1): 0.3788\tClassification Loss: 1.2457\n","Train Epoch: 2 [97600/110534 (88%)]\tAll Loss: 1.9115\tTriple Loss(0): 0.0000\tClassification Loss: 1.9115\n","Train Epoch: 2 [97920/110534 (89%)]\tAll Loss: 1.1860\tTriple Loss(0): 0.0000\tClassification Loss: 1.1860\n","Train Epoch: 2 [98240/110534 (89%)]\tAll Loss: 2.2573\tTriple Loss(0): 0.4676\tClassification Loss: 1.3220\n","Train Epoch: 2 [98560/110534 (89%)]\tAll Loss: 1.9454\tTriple Loss(1): 0.2523\tClassification Loss: 1.4408\n","Train Epoch: 2 [98880/110534 (89%)]\tAll Loss: 2.0493\tTriple Loss(1): 0.2397\tClassification Loss: 1.5700\n","\n","Test set: Average loss: 1.6667, Accuracy: 537/960 (56%)\n","\n","Train Epoch: 2 [99200/110534 (90%)]\tAll Loss: 1.9327\tTriple Loss(1): 0.2137\tClassification Loss: 1.5053\n","Train Epoch: 2 [99520/110534 (90%)]\tAll Loss: 2.1136\tTriple Loss(1): 0.2694\tClassification Loss: 1.5748\n","Train Epoch: 2 [99840/110534 (90%)]\tAll Loss: 2.6177\tTriple Loss(1): 0.4630\tClassification Loss: 1.6916\n","Train Epoch: 2 [100160/110534 (91%)]\tAll Loss: 2.4375\tTriple Loss(1): 0.2947\tClassification Loss: 1.8480\n","Train Epoch: 2 [100480/110534 (91%)]\tAll Loss: 2.3060\tTriple Loss(1): 0.3379\tClassification Loss: 1.6301\n","Train Epoch: 2 [100800/110534 (91%)]\tAll Loss: 2.9281\tTriple Loss(1): 0.3593\tClassification Loss: 2.2094\n","Train Epoch: 2 [101120/110534 (91%)]\tAll Loss: 2.1426\tTriple Loss(1): 0.3471\tClassification Loss: 1.4484\n","Train Epoch: 2 [101440/110534 (92%)]\tAll Loss: 1.8111\tTriple Loss(1): 0.1888\tClassification Loss: 1.4334\n","Train Epoch: 2 [101760/110534 (92%)]\tAll Loss: 2.3895\tTriple Loss(1): 0.3468\tClassification Loss: 1.6958\n","Train Epoch: 2 [102080/110534 (92%)]\tAll Loss: 2.2525\tTriple Loss(1): 0.1657\tClassification Loss: 1.9212\n","\n","Test set: Average loss: 1.6635, Accuracy: 541/960 (56%)\n","\n","Train Epoch: 2 [102400/110534 (93%)]\tAll Loss: 2.3588\tTriple Loss(1): 0.2714\tClassification Loss: 1.8161\n","Train Epoch: 2 [102720/110534 (93%)]\tAll Loss: 2.5350\tTriple Loss(1): 0.3770\tClassification Loss: 1.7811\n","Train Epoch: 2 [103040/110534 (93%)]\tAll Loss: 2.1051\tTriple Loss(1): 0.3401\tClassification Loss: 1.4248\n","Train Epoch: 2 [103360/110534 (93%)]\tAll Loss: 1.9985\tTriple Loss(1): 0.0618\tClassification Loss: 1.8750\n","Train Epoch: 2 [103680/110534 (94%)]\tAll Loss: 2.6616\tTriple Loss(1): 0.5040\tClassification Loss: 1.6536\n","Train Epoch: 2 [104000/110534 (94%)]\tAll Loss: 1.5379\tTriple Loss(0): 0.0000\tClassification Loss: 1.5379\n","Train Epoch: 2 [104320/110534 (94%)]\tAll Loss: 2.2485\tTriple Loss(1): 0.1496\tClassification Loss: 1.9494\n","Train Epoch: 2 [104640/110534 (95%)]\tAll Loss: 2.3869\tTriple Loss(1): 0.1854\tClassification Loss: 2.0161\n","Train Epoch: 2 [104960/110534 (95%)]\tAll Loss: 1.9741\tTriple Loss(1): 0.2553\tClassification Loss: 1.4635\n","Train Epoch: 2 [105280/110534 (95%)]\tAll Loss: 2.1159\tTriple Loss(1): 0.2185\tClassification Loss: 1.6789\n","\n","Test set: Average loss: 1.6629, Accuracy: 546/960 (57%)\n","\n","Train Epoch: 2 [105600/110534 (96%)]\tAll Loss: 2.0852\tTriple Loss(1): 0.1616\tClassification Loss: 1.7619\n","Train Epoch: 2 [105920/110534 (96%)]\tAll Loss: 2.0030\tTriple Loss(1): 0.1505\tClassification Loss: 1.7021\n","Train Epoch: 2 [106240/110534 (96%)]\tAll Loss: 1.4003\tTriple Loss(0): 0.0000\tClassification Loss: 1.4003\n","Train Epoch: 2 [106560/110534 (96%)]\tAll Loss: 1.6857\tTriple Loss(0): 0.0000\tClassification Loss: 1.6857\n","Train Epoch: 2 [106880/110534 (97%)]\tAll Loss: 2.1564\tTriple Loss(0): 0.0000\tClassification Loss: 2.1564\n","Train Epoch: 2 [107200/110534 (97%)]\tAll Loss: 3.1570\tTriple Loss(0): 0.6883\tClassification Loss: 1.7805\n","Train Epoch: 2 [107520/110534 (97%)]\tAll Loss: 2.1602\tTriple Loss(1): 0.1613\tClassification Loss: 1.8377\n","Train Epoch: 2 [107840/110534 (98%)]\tAll Loss: 2.2330\tTriple Loss(1): 0.3723\tClassification Loss: 1.4883\n","Train Epoch: 2 [108160/110534 (98%)]\tAll Loss: 1.6095\tTriple Loss(1): 0.1682\tClassification Loss: 1.2731\n","Train Epoch: 2 [108480/110534 (98%)]\tAll Loss: 2.2882\tTriple Loss(1): 0.3563\tClassification Loss: 1.5756\n","\n","Test set: Average loss: 1.6581, Accuracy: 548/960 (57%)\n","\n","Train Epoch: 2 [108800/110534 (98%)]\tAll Loss: 3.6565\tTriple Loss(0): 1.0185\tClassification Loss: 1.6195\n","Train Epoch: 2 [109120/110534 (99%)]\tAll Loss: 2.5592\tTriple Loss(1): 0.2760\tClassification Loss: 2.0072\n","Train Epoch: 2 [109440/110534 (99%)]\tAll Loss: 1.6302\tTriple Loss(0): 0.1109\tClassification Loss: 1.4083\n","Train Epoch: 2 [109760/110534 (99%)]\tAll Loss: 1.8644\tTriple Loss(1): 0.1104\tClassification Loss: 1.6436\n","Train Epoch: 2 [110080/110534 (100%)]\tAll Loss: 2.2714\tTriple Loss(1): 0.2779\tClassification Loss: 1.7155\n","Train Epoch: 2 [110400/110534 (100%)]\tAll Loss: 2.0645\tTriple Loss(1): 0.2581\tClassification Loss: 1.5484\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_2_final.pth.tar\n","\n","Test set: Average loss: 1.6647, Accuracy: 532/960 (55%)\n","\n","Train Epoch: 3 [0/110534 (0%)]\tAll Loss: 1.9444\tTriple Loss(1): 0.1538\tClassification Loss: 1.6367\n","Train Epoch: 3 [320/110534 (0%)]\tAll Loss: 1.9898\tTriple Loss(1): 0.2035\tClassification Loss: 1.5829\n","Train Epoch: 3 [640/110534 (1%)]\tAll Loss: 1.8764\tTriple Loss(1): 0.4137\tClassification Loss: 1.0490\n","Train Epoch: 3 [960/110534 (1%)]\tAll Loss: 2.5638\tTriple Loss(1): 0.2467\tClassification Loss: 2.0703\n","Train Epoch: 3 [1280/110534 (1%)]\tAll Loss: 2.4402\tTriple Loss(1): 0.4695\tClassification Loss: 1.5012\n","Train Epoch: 3 [1600/110534 (1%)]\tAll Loss: 2.3215\tTriple Loss(1): 0.2782\tClassification Loss: 1.7652\n","Train Epoch: 3 [1920/110534 (2%)]\tAll Loss: 2.1047\tTriple Loss(1): 0.1371\tClassification Loss: 1.8304\n","Train Epoch: 3 [2240/110534 (2%)]\tAll Loss: 1.7270\tTriple Loss(0): 0.0000\tClassification Loss: 1.7270\n","Train Epoch: 3 [2560/110534 (2%)]\tAll Loss: 2.3596\tTriple Loss(1): 0.3298\tClassification Loss: 1.6999\n","Train Epoch: 3 [2880/110534 (3%)]\tAll Loss: 2.0704\tTriple Loss(1): 0.1513\tClassification Loss: 1.7679\n","\n","Test set: Average loss: 1.6640, Accuracy: 541/960 (56%)\n","\n","Train Epoch: 3 [3200/110534 (3%)]\tAll Loss: 1.9580\tTriple Loss(0): 0.0000\tClassification Loss: 1.9580\n","Train Epoch: 3 [3520/110534 (3%)]\tAll Loss: 1.6650\tTriple Loss(1): 0.1892\tClassification Loss: 1.2867\n","Train Epoch: 3 [3840/110534 (3%)]\tAll Loss: 1.9112\tTriple Loss(1): 0.1232\tClassification Loss: 1.6649\n","Train Epoch: 3 [4160/110534 (4%)]\tAll Loss: 2.2322\tTriple Loss(1): 0.3181\tClassification Loss: 1.5960\n","Train Epoch: 3 [4480/110534 (4%)]\tAll Loss: 2.0191\tTriple Loss(1): 0.2481\tClassification Loss: 1.5228\n","Train Epoch: 3 [4800/110534 (4%)]\tAll Loss: 1.5659\tTriple Loss(0): 0.0000\tClassification Loss: 1.5659\n","Train Epoch: 3 [5120/110534 (5%)]\tAll Loss: 1.9423\tTriple Loss(1): 0.1386\tClassification Loss: 1.6650\n","Train Epoch: 3 [5440/110534 (5%)]\tAll Loss: 2.2206\tTriple Loss(1): 0.2513\tClassification Loss: 1.7181\n","Train Epoch: 3 [5760/110534 (5%)]\tAll Loss: 1.9764\tTriple Loss(1): 0.2503\tClassification Loss: 1.4758\n","Train Epoch: 3 [6080/110534 (5%)]\tAll Loss: 2.1138\tTriple Loss(1): 0.2574\tClassification Loss: 1.5990\n","\n","Test set: Average loss: 1.6543, Accuracy: 543/960 (57%)\n","\n","Train Epoch: 3 [6400/110534 (6%)]\tAll Loss: 2.2415\tTriple Loss(1): 0.2315\tClassification Loss: 1.7785\n","Train Epoch: 3 [6720/110534 (6%)]\tAll Loss: 1.9133\tTriple Loss(1): 0.1847\tClassification Loss: 1.5440\n","Train Epoch: 3 [7040/110534 (6%)]\tAll Loss: 1.8251\tTriple Loss(0): 0.0000\tClassification Loss: 1.8251\n","Train Epoch: 3 [7360/110534 (7%)]\tAll Loss: 2.0961\tTriple Loss(1): 0.2942\tClassification Loss: 1.5077\n","Train Epoch: 3 [7680/110534 (7%)]\tAll Loss: 1.2267\tTriple Loss(0): 0.0000\tClassification Loss: 1.2267\n","Train Epoch: 3 [8000/110534 (7%)]\tAll Loss: 2.3039\tTriple Loss(1): 0.4791\tClassification Loss: 1.3456\n","Train Epoch: 3 [8320/110534 (8%)]\tAll Loss: 1.7519\tTriple Loss(1): 0.1040\tClassification Loss: 1.5439\n","Train Epoch: 3 [8640/110534 (8%)]\tAll Loss: 1.9910\tTriple Loss(1): 0.3813\tClassification Loss: 1.2284\n","Train Epoch: 3 [8960/110534 (8%)]\tAll Loss: 1.9546\tTriple Loss(1): 0.2521\tClassification Loss: 1.4503\n","Train Epoch: 3 [9280/110534 (8%)]\tAll Loss: 1.7522\tTriple Loss(1): 0.2007\tClassification Loss: 1.3508\n","\n","Test set: Average loss: 1.6493, Accuracy: 549/960 (57%)\n","\n","Train Epoch: 3 [9600/110534 (9%)]\tAll Loss: 2.3714\tTriple Loss(1): 0.3017\tClassification Loss: 1.7680\n","Train Epoch: 3 [9920/110534 (9%)]\tAll Loss: 2.1995\tTriple Loss(1): 0.2789\tClassification Loss: 1.6416\n","Train Epoch: 3 [10240/110534 (9%)]\tAll Loss: 2.3498\tTriple Loss(1): 0.2461\tClassification Loss: 1.8576\n","Train Epoch: 3 [10560/110534 (10%)]\tAll Loss: 2.1456\tTriple Loss(1): 0.1719\tClassification Loss: 1.8018\n","Train Epoch: 3 [10880/110534 (10%)]\tAll Loss: 2.6118\tTriple Loss(1): 0.3696\tClassification Loss: 1.8726\n","Train Epoch: 3 [11200/110534 (10%)]\tAll Loss: 1.7860\tTriple Loss(1): 0.2594\tClassification Loss: 1.2672\n","Train Epoch: 3 [11520/110534 (10%)]\tAll Loss: 1.5803\tTriple Loss(0): 0.0000\tClassification Loss: 1.5803\n","Train Epoch: 3 [11840/110534 (11%)]\tAll Loss: 2.0584\tTriple Loss(1): 0.1707\tClassification Loss: 1.7170\n","Train Epoch: 3 [12160/110534 (11%)]\tAll Loss: 1.8099\tTriple Loss(0): 0.0000\tClassification Loss: 1.8099\n","Train Epoch: 3 [12480/110534 (11%)]\tAll Loss: 2.2176\tTriple Loss(1): 0.2333\tClassification Loss: 1.7510\n","\n","Test set: Average loss: 1.6532, Accuracy: 551/960 (57%)\n","\n","Train Epoch: 3 [12800/110534 (12%)]\tAll Loss: 2.1384\tTriple Loss(1): 0.3221\tClassification Loss: 1.4943\n","Train Epoch: 3 [13120/110534 (12%)]\tAll Loss: 1.3973\tTriple Loss(0): 0.0000\tClassification Loss: 1.3973\n","Train Epoch: 3 [13440/110534 (12%)]\tAll Loss: 1.9962\tTriple Loss(0): 0.0000\tClassification Loss: 1.9962\n","Train Epoch: 3 [13760/110534 (12%)]\tAll Loss: 2.0375\tTriple Loss(1): 0.1760\tClassification Loss: 1.6855\n","Train Epoch: 3 [14080/110534 (13%)]\tAll Loss: 2.3512\tTriple Loss(1): 0.3113\tClassification Loss: 1.7287\n","Train Epoch: 3 [14400/110534 (13%)]\tAll Loss: 2.6253\tTriple Loss(1): 0.2240\tClassification Loss: 2.1773\n","Train Epoch: 3 [14720/110534 (13%)]\tAll Loss: 2.2869\tTriple Loss(1): 0.2412\tClassification Loss: 1.8046\n","Train Epoch: 3 [15040/110534 (14%)]\tAll Loss: 2.0509\tTriple Loss(1): 0.3294\tClassification Loss: 1.3922\n","Train Epoch: 3 [15360/110534 (14%)]\tAll Loss: 1.8597\tTriple Loss(1): 0.1985\tClassification Loss: 1.4626\n","Train Epoch: 3 [15680/110534 (14%)]\tAll Loss: 1.4384\tTriple Loss(0): 0.0000\tClassification Loss: 1.4384\n","\n","Test set: Average loss: 1.6466, Accuracy: 544/960 (57%)\n","\n","Train Epoch: 3 [16000/110534 (14%)]\tAll Loss: 5.7329\tTriple Loss(0): 1.9399\tClassification Loss: 1.8530\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_500.pth.tar\n","Train Epoch: 3 [16320/110534 (15%)]\tAll Loss: 1.8269\tTriple Loss(1): 0.2669\tClassification Loss: 1.2931\n","Train Epoch: 3 [16640/110534 (15%)]\tAll Loss: 1.9150\tTriple Loss(1): 0.1821\tClassification Loss: 1.5508\n","Train Epoch: 3 [16960/110534 (15%)]\tAll Loss: 1.9914\tTriple Loss(0): 0.0000\tClassification Loss: 1.9914\n","Train Epoch: 3 [17280/110534 (16%)]\tAll Loss: 1.6781\tTriple Loss(1): 0.1296\tClassification Loss: 1.4189\n","Train Epoch: 3 [17600/110534 (16%)]\tAll Loss: 2.4527\tTriple Loss(1): 0.3734\tClassification Loss: 1.7059\n","Train Epoch: 3 [17920/110534 (16%)]\tAll Loss: 2.0821\tTriple Loss(1): 0.1518\tClassification Loss: 1.7784\n","Train Epoch: 3 [18240/110534 (16%)]\tAll Loss: 2.0339\tTriple Loss(1): 0.0604\tClassification Loss: 1.9131\n","Train Epoch: 3 [18560/110534 (17%)]\tAll Loss: 1.9909\tTriple Loss(1): 0.2292\tClassification Loss: 1.5326\n","Train Epoch: 3 [18880/110534 (17%)]\tAll Loss: 2.1103\tTriple Loss(1): 0.2532\tClassification Loss: 1.6040\n","\n","Test set: Average loss: 1.6454, Accuracy: 552/960 (58%)\n","\n","Train Epoch: 3 [19200/110534 (17%)]\tAll Loss: 1.4262\tTriple Loss(0): 0.0000\tClassification Loss: 1.4262\n","Train Epoch: 3 [19520/110534 (18%)]\tAll Loss: 2.1427\tTriple Loss(1): 0.2295\tClassification Loss: 1.6836\n","Train Epoch: 3 [19840/110534 (18%)]\tAll Loss: 2.3921\tTriple Loss(1): 0.3755\tClassification Loss: 1.6411\n","Train Epoch: 3 [20160/110534 (18%)]\tAll Loss: 1.9291\tTriple Loss(1): 0.1303\tClassification Loss: 1.6686\n","Train Epoch: 3 [20480/110534 (19%)]\tAll Loss: 1.9943\tTriple Loss(1): 0.2876\tClassification Loss: 1.4191\n","Train Epoch: 3 [20800/110534 (19%)]\tAll Loss: 3.3002\tTriple Loss(1): 0.5478\tClassification Loss: 2.2047\n","Train Epoch: 3 [21120/110534 (19%)]\tAll Loss: 1.9359\tTriple Loss(1): 0.2321\tClassification Loss: 1.4717\n","Train Epoch: 3 [21440/110534 (19%)]\tAll Loss: 2.3040\tTriple Loss(1): 0.4554\tClassification Loss: 1.3932\n","Train Epoch: 3 [21760/110534 (20%)]\tAll Loss: 2.4705\tTriple Loss(1): 0.2163\tClassification Loss: 2.0378\n","Train Epoch: 3 [22080/110534 (20%)]\tAll Loss: 2.0128\tTriple Loss(1): 0.1486\tClassification Loss: 1.7155\n","\n","Test set: Average loss: 1.6504, Accuracy: 529/960 (55%)\n","\n","Train Epoch: 3 [22400/110534 (20%)]\tAll Loss: 2.2688\tTriple Loss(1): 0.3205\tClassification Loss: 1.6279\n","Train Epoch: 3 [22720/110534 (21%)]\tAll Loss: 1.6511\tTriple Loss(0): 0.0000\tClassification Loss: 1.6511\n","Train Epoch: 3 [23040/110534 (21%)]\tAll Loss: 1.6268\tTriple Loss(0): 0.0000\tClassification Loss: 1.6268\n","Train Epoch: 3 [23360/110534 (21%)]\tAll Loss: 1.6899\tTriple Loss(1): 0.0578\tClassification Loss: 1.5742\n","Train Epoch: 3 [23680/110534 (21%)]\tAll Loss: 2.2949\tTriple Loss(1): 0.2079\tClassification Loss: 1.8791\n","Train Epoch: 3 [24000/110534 (22%)]\tAll Loss: 2.3342\tTriple Loss(1): 0.4290\tClassification Loss: 1.4762\n","Train Epoch: 3 [24320/110534 (22%)]\tAll Loss: 2.2253\tTriple Loss(1): 0.2651\tClassification Loss: 1.6951\n","Train Epoch: 3 [24640/110534 (22%)]\tAll Loss: 2.0580\tTriple Loss(0): 0.0000\tClassification Loss: 2.0580\n","Train Epoch: 3 [24960/110534 (23%)]\tAll Loss: 1.9726\tTriple Loss(1): 0.2246\tClassification Loss: 1.5233\n","Train Epoch: 3 [25280/110534 (23%)]\tAll Loss: 2.2613\tTriple Loss(1): 0.3067\tClassification Loss: 1.6479\n","\n","Test set: Average loss: 1.6415, Accuracy: 546/960 (57%)\n","\n","Train Epoch: 3 [25600/110534 (23%)]\tAll Loss: 1.5874\tTriple Loss(0): 0.0000\tClassification Loss: 1.5874\n","Train Epoch: 3 [25920/110534 (23%)]\tAll Loss: 2.3986\tTriple Loss(1): 0.3505\tClassification Loss: 1.6977\n","Train Epoch: 3 [26240/110534 (24%)]\tAll Loss: 2.6749\tTriple Loss(1): 0.5807\tClassification Loss: 1.5135\n","Train Epoch: 3 [26560/110534 (24%)]\tAll Loss: 2.4483\tTriple Loss(1): 0.3346\tClassification Loss: 1.7791\n","Train Epoch: 3 [26880/110534 (24%)]\tAll Loss: 1.8399\tTriple Loss(0): 0.0000\tClassification Loss: 1.8399\n","Train Epoch: 3 [27200/110534 (25%)]\tAll Loss: 2.4215\tTriple Loss(1): 0.2254\tClassification Loss: 1.9707\n","Train Epoch: 3 [27520/110534 (25%)]\tAll Loss: 1.8356\tTriple Loss(1): 0.1488\tClassification Loss: 1.5380\n","Train Epoch: 3 [27840/110534 (25%)]\tAll Loss: 1.8149\tTriple Loss(1): 0.1800\tClassification Loss: 1.4548\n","Train Epoch: 3 [28160/110534 (25%)]\tAll Loss: 2.3969\tTriple Loss(1): 0.2692\tClassification Loss: 1.8585\n","Train Epoch: 3 [28480/110534 (26%)]\tAll Loss: 2.3744\tTriple Loss(1): 0.2819\tClassification Loss: 1.8106\n","\n","Test set: Average loss: 1.6448, Accuracy: 557/960 (58%)\n","\n","Train Epoch: 3 [28800/110534 (26%)]\tAll Loss: 1.6487\tTriple Loss(1): 0.1860\tClassification Loss: 1.2767\n","Train Epoch: 3 [29120/110534 (26%)]\tAll Loss: 2.2537\tTriple Loss(1): 0.2628\tClassification Loss: 1.7282\n","Train Epoch: 3 [29440/110534 (27%)]\tAll Loss: 1.4078\tTriple Loss(0): 0.0000\tClassification Loss: 1.4078\n","Train Epoch: 3 [29760/110534 (27%)]\tAll Loss: 1.6080\tTriple Loss(1): 0.1991\tClassification Loss: 1.2098\n","Train Epoch: 3 [30080/110534 (27%)]\tAll Loss: 1.5150\tTriple Loss(1): 0.0758\tClassification Loss: 1.3634\n","Train Epoch: 3 [30400/110534 (27%)]\tAll Loss: 1.7987\tTriple Loss(1): 0.2249\tClassification Loss: 1.3489\n","Train Epoch: 3 [30720/110534 (28%)]\tAll Loss: 2.1277\tTriple Loss(1): 0.4177\tClassification Loss: 1.2922\n","Train Epoch: 3 [31040/110534 (28%)]\tAll Loss: 2.1116\tTriple Loss(1): 0.1590\tClassification Loss: 1.7937\n","Train Epoch: 3 [31360/110534 (28%)]\tAll Loss: 2.2025\tTriple Loss(1): 0.2723\tClassification Loss: 1.6578\n","Train Epoch: 3 [31680/110534 (29%)]\tAll Loss: 1.8642\tTriple Loss(1): 0.1463\tClassification Loss: 1.5716\n","\n","Test set: Average loss: 1.6380, Accuracy: 550/960 (57%)\n","\n","Train Epoch: 3 [32000/110534 (29%)]\tAll Loss: 1.9004\tTriple Loss(1): 0.0000\tClassification Loss: 1.9004\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_1000.pth.tar\n","Train Epoch: 3 [32320/110534 (29%)]\tAll Loss: 1.9250\tTriple Loss(1): 0.1913\tClassification Loss: 1.5425\n","Train Epoch: 3 [32640/110534 (30%)]\tAll Loss: 1.9585\tTriple Loss(1): 0.2242\tClassification Loss: 1.5100\n","Train Epoch: 3 [32960/110534 (30%)]\tAll Loss: 1.9648\tTriple Loss(1): 0.1368\tClassification Loss: 1.6911\n","Train Epoch: 3 [33280/110534 (30%)]\tAll Loss: 2.2617\tTriple Loss(1): 0.2602\tClassification Loss: 1.7412\n","Train Epoch: 3 [33600/110534 (30%)]\tAll Loss: 1.5912\tTriple Loss(0): 0.0000\tClassification Loss: 1.5912\n","Train Epoch: 3 [33920/110534 (31%)]\tAll Loss: 2.0048\tTriple Loss(1): 0.2263\tClassification Loss: 1.5522\n","Train Epoch: 3 [34240/110534 (31%)]\tAll Loss: 2.5789\tTriple Loss(1): 0.3772\tClassification Loss: 1.8244\n","Train Epoch: 3 [34560/110534 (31%)]\tAll Loss: 2.0613\tTriple Loss(1): 0.3154\tClassification Loss: 1.4305\n","Train Epoch: 3 [34880/110534 (32%)]\tAll Loss: 1.7031\tTriple Loss(1): 0.0676\tClassification Loss: 1.5680\n","\n","Test set: Average loss: 1.6343, Accuracy: 549/960 (57%)\n","\n","Train Epoch: 3 [35200/110534 (32%)]\tAll Loss: 2.4243\tTriple Loss(1): 0.1646\tClassification Loss: 2.0950\n","Train Epoch: 3 [35520/110534 (32%)]\tAll Loss: 1.6794\tTriple Loss(0): 0.0000\tClassification Loss: 1.6794\n","Train Epoch: 3 [35840/110534 (32%)]\tAll Loss: 1.6273\tTriple Loss(1): 0.1516\tClassification Loss: 1.3240\n","Train Epoch: 3 [36160/110534 (33%)]\tAll Loss: 2.2452\tTriple Loss(1): 0.3345\tClassification Loss: 1.5762\n","Train Epoch: 3 [36480/110534 (33%)]\tAll Loss: 2.0733\tTriple Loss(1): 0.1947\tClassification Loss: 1.6839\n","Train Epoch: 3 [36800/110534 (33%)]\tAll Loss: 2.4671\tTriple Loss(1): 0.3269\tClassification Loss: 1.8133\n","Train Epoch: 3 [37120/110534 (34%)]\tAll Loss: 2.2441\tTriple Loss(1): 0.1563\tClassification Loss: 1.9315\n","Train Epoch: 3 [37440/110534 (34%)]\tAll Loss: 1.9050\tTriple Loss(1): 0.1263\tClassification Loss: 1.6523\n","Train Epoch: 3 [37760/110534 (34%)]\tAll Loss: 1.6936\tTriple Loss(1): 0.0529\tClassification Loss: 1.5879\n","Train Epoch: 3 [38080/110534 (34%)]\tAll Loss: 1.9887\tTriple Loss(1): 0.0907\tClassification Loss: 1.8073\n","\n","Test set: Average loss: 1.6415, Accuracy: 548/960 (57%)\n","\n","Train Epoch: 3 [38400/110534 (35%)]\tAll Loss: 1.3891\tTriple Loss(0): 0.0000\tClassification Loss: 1.3891\n","Train Epoch: 3 [38720/110534 (35%)]\tAll Loss: 2.5951\tTriple Loss(1): 0.3112\tClassification Loss: 1.9727\n","Train Epoch: 3 [39040/110534 (35%)]\tAll Loss: 1.4274\tTriple Loss(0): 0.0000\tClassification Loss: 1.4274\n","Train Epoch: 3 [39360/110534 (36%)]\tAll Loss: 1.3725\tTriple Loss(0): 0.0000\tClassification Loss: 1.3725\n","Train Epoch: 3 [39680/110534 (36%)]\tAll Loss: 2.1414\tTriple Loss(1): 0.1849\tClassification Loss: 1.7715\n","Train Epoch: 3 [40000/110534 (36%)]\tAll Loss: 1.7204\tTriple Loss(1): 0.2208\tClassification Loss: 1.2789\n","Train Epoch: 3 [40320/110534 (36%)]\tAll Loss: 2.0371\tTriple Loss(1): 0.1190\tClassification Loss: 1.7991\n","Train Epoch: 3 [40640/110534 (37%)]\tAll Loss: 2.2603\tTriple Loss(1): 0.2372\tClassification Loss: 1.7858\n","Train Epoch: 3 [40960/110534 (37%)]\tAll Loss: 1.5412\tTriple Loss(0): 0.0000\tClassification Loss: 1.5412\n","Train Epoch: 3 [41280/110534 (37%)]\tAll Loss: 2.0678\tTriple Loss(1): 0.2143\tClassification Loss: 1.6391\n","\n","Test set: Average loss: 1.6333, Accuracy: 547/960 (57%)\n","\n","Train Epoch: 3 [41600/110534 (38%)]\tAll Loss: 1.6474\tTriple Loss(1): 0.0368\tClassification Loss: 1.5737\n","Train Epoch: 3 [41920/110534 (38%)]\tAll Loss: 1.9990\tTriple Loss(1): 0.1882\tClassification Loss: 1.6227\n","Train Epoch: 3 [42240/110534 (38%)]\tAll Loss: 1.7048\tTriple Loss(0): 0.0000\tClassification Loss: 1.7048\n","Train Epoch: 3 [42560/110534 (38%)]\tAll Loss: 1.7313\tTriple Loss(1): 0.1665\tClassification Loss: 1.3983\n","Train Epoch: 3 [42880/110534 (39%)]\tAll Loss: 1.6392\tTriple Loss(0): 0.0000\tClassification Loss: 1.6392\n","Train Epoch: 3 [43200/110534 (39%)]\tAll Loss: 1.7118\tTriple Loss(0): 0.0000\tClassification Loss: 1.7118\n","Train Epoch: 3 [43520/110534 (39%)]\tAll Loss: 1.9168\tTriple Loss(1): 0.2374\tClassification Loss: 1.4420\n","Train Epoch: 3 [43840/110534 (40%)]\tAll Loss: 2.2104\tTriple Loss(1): 0.2632\tClassification Loss: 1.6840\n","Train Epoch: 3 [44160/110534 (40%)]\tAll Loss: 2.5213\tTriple Loss(1): 0.2113\tClassification Loss: 2.0987\n","Train Epoch: 3 [44480/110534 (40%)]\tAll Loss: 1.6990\tTriple Loss(0): 0.0000\tClassification Loss: 1.6990\n","\n","Test set: Average loss: 1.6327, Accuracy: 562/960 (59%)\n","\n","Train Epoch: 3 [44800/110534 (41%)]\tAll Loss: 1.8694\tTriple Loss(1): 0.2851\tClassification Loss: 1.2992\n","Train Epoch: 3 [45120/110534 (41%)]\tAll Loss: 1.8155\tTriple Loss(1): 0.0417\tClassification Loss: 1.7322\n","Train Epoch: 3 [45440/110534 (41%)]\tAll Loss: 1.6589\tTriple Loss(1): 0.1390\tClassification Loss: 1.3808\n","Train Epoch: 3 [45760/110534 (41%)]\tAll Loss: 1.6444\tTriple Loss(1): 0.1431\tClassification Loss: 1.3582\n","Train Epoch: 3 [46080/110534 (42%)]\tAll Loss: 1.8285\tTriple Loss(1): 0.2568\tClassification Loss: 1.3149\n","Train Epoch: 3 [46400/110534 (42%)]\tAll Loss: 2.0277\tTriple Loss(0): 0.0000\tClassification Loss: 2.0277\n","Train Epoch: 3 [46720/110534 (42%)]\tAll Loss: 2.4568\tTriple Loss(1): 0.2478\tClassification Loss: 1.9612\n","Train Epoch: 3 [47040/110534 (43%)]\tAll Loss: 2.2700\tTriple Loss(1): 0.3820\tClassification Loss: 1.5059\n","Train Epoch: 3 [47360/110534 (43%)]\tAll Loss: 2.3403\tTriple Loss(1): 0.3601\tClassification Loss: 1.6201\n","Train Epoch: 3 [47680/110534 (43%)]\tAll Loss: 1.6766\tTriple Loss(1): 0.2061\tClassification Loss: 1.2645\n","\n","Test set: Average loss: 1.6363, Accuracy: 541/960 (56%)\n","\n","Train Epoch: 3 [48000/110534 (43%)]\tAll Loss: 2.3369\tTriple Loss(1): 0.1846\tClassification Loss: 1.9678\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_1500.pth.tar\n","Train Epoch: 3 [48320/110534 (44%)]\tAll Loss: 2.0344\tTriple Loss(1): 0.2491\tClassification Loss: 1.5362\n","Train Epoch: 3 [48640/110534 (44%)]\tAll Loss: 2.5994\tTriple Loss(1): 0.3193\tClassification Loss: 1.9608\n","Train Epoch: 3 [48960/110534 (44%)]\tAll Loss: 2.0912\tTriple Loss(1): 0.3516\tClassification Loss: 1.3881\n","Train Epoch: 3 [49280/110534 (45%)]\tAll Loss: 1.9475\tTriple Loss(1): 0.2019\tClassification Loss: 1.5438\n","Train Epoch: 3 [49600/110534 (45%)]\tAll Loss: 3.9717\tTriple Loss(0): 1.1959\tClassification Loss: 1.5799\n","Train Epoch: 3 [49920/110534 (45%)]\tAll Loss: 1.9204\tTriple Loss(1): 0.3578\tClassification Loss: 1.2049\n","Train Epoch: 3 [50240/110534 (45%)]\tAll Loss: 1.9564\tTriple Loss(1): 0.3390\tClassification Loss: 1.2785\n","Train Epoch: 3 [50560/110534 (46%)]\tAll Loss: 2.3836\tTriple Loss(1): 0.3028\tClassification Loss: 1.7781\n","Train Epoch: 3 [50880/110534 (46%)]\tAll Loss: 2.3294\tTriple Loss(1): 0.3235\tClassification Loss: 1.6824\n","\n","Test set: Average loss: 1.6281, Accuracy: 560/960 (58%)\n","\n","Train Epoch: 3 [51200/110534 (46%)]\tAll Loss: 1.4515\tTriple Loss(0): 0.0000\tClassification Loss: 1.4515\n","Train Epoch: 3 [51520/110534 (47%)]\tAll Loss: 2.0303\tTriple Loss(1): 0.3244\tClassification Loss: 1.3816\n","Train Epoch: 3 [51840/110534 (47%)]\tAll Loss: 1.9317\tTriple Loss(1): 0.1772\tClassification Loss: 1.5773\n","Train Epoch: 3 [52160/110534 (47%)]\tAll Loss: 2.1229\tTriple Loss(1): 0.2072\tClassification Loss: 1.7084\n","Train Epoch: 3 [52480/110534 (47%)]\tAll Loss: 2.8164\tTriple Loss(1): 0.2314\tClassification Loss: 2.3536\n","Train Epoch: 3 [52800/110534 (48%)]\tAll Loss: 1.3932\tTriple Loss(0): 0.0000\tClassification Loss: 1.3932\n","Train Epoch: 3 [53120/110534 (48%)]\tAll Loss: 2.0384\tTriple Loss(1): 0.2116\tClassification Loss: 1.6152\n","Train Epoch: 3 [53440/110534 (48%)]\tAll Loss: 2.2906\tTriple Loss(1): 0.2513\tClassification Loss: 1.7880\n","Train Epoch: 3 [53760/110534 (49%)]\tAll Loss: 1.9553\tTriple Loss(1): 0.1013\tClassification Loss: 1.7526\n","Train Epoch: 3 [54080/110534 (49%)]\tAll Loss: 2.4975\tTriple Loss(1): 0.2097\tClassification Loss: 2.0781\n","\n","Test set: Average loss: 1.6343, Accuracy: 549/960 (57%)\n","\n","Train Epoch: 3 [54400/110534 (49%)]\tAll Loss: 2.2396\tTriple Loss(1): 0.1844\tClassification Loss: 1.8708\n","Train Epoch: 3 [54720/110534 (49%)]\tAll Loss: 2.0620\tTriple Loss(1): 0.1597\tClassification Loss: 1.7426\n","Train Epoch: 3 [55040/110534 (50%)]\tAll Loss: 2.2182\tTriple Loss(1): 0.1242\tClassification Loss: 1.9698\n","Train Epoch: 3 [55360/110534 (50%)]\tAll Loss: 2.2291\tTriple Loss(1): 0.3018\tClassification Loss: 1.6255\n","Train Epoch: 3 [55680/110534 (50%)]\tAll Loss: 2.3113\tTriple Loss(1): 0.2009\tClassification Loss: 1.9096\n","Train Epoch: 3 [56000/110534 (51%)]\tAll Loss: 2.5859\tTriple Loss(1): 0.2457\tClassification Loss: 2.0945\n","Train Epoch: 3 [56320/110534 (51%)]\tAll Loss: 2.3593\tTriple Loss(1): 0.3034\tClassification Loss: 1.7525\n","Train Epoch: 3 [56640/110534 (51%)]\tAll Loss: 1.6820\tTriple Loss(1): 0.2259\tClassification Loss: 1.2303\n","Train Epoch: 3 [56960/110534 (52%)]\tAll Loss: 1.5436\tTriple Loss(0): 0.0000\tClassification Loss: 1.5436\n","Train Epoch: 3 [57280/110534 (52%)]\tAll Loss: 2.0458\tTriple Loss(1): 0.0676\tClassification Loss: 1.9107\n","\n","Test set: Average loss: 1.6301, Accuracy: 555/960 (58%)\n","\n","Train Epoch: 3 [57600/110534 (52%)]\tAll Loss: 2.2502\tTriple Loss(1): 0.2137\tClassification Loss: 1.8229\n","Train Epoch: 3 [57920/110534 (52%)]\tAll Loss: 1.7763\tTriple Loss(1): 0.0711\tClassification Loss: 1.6341\n","Train Epoch: 3 [58240/110534 (53%)]\tAll Loss: 2.2550\tTriple Loss(1): 0.4140\tClassification Loss: 1.4270\n","Train Epoch: 3 [58560/110534 (53%)]\tAll Loss: 2.1723\tTriple Loss(1): 0.3453\tClassification Loss: 1.4817\n","Train Epoch: 3 [58880/110534 (53%)]\tAll Loss: 1.6535\tTriple Loss(0): 0.0000\tClassification Loss: 1.6535\n","Train Epoch: 3 [59200/110534 (54%)]\tAll Loss: 2.2643\tTriple Loss(1): 0.1647\tClassification Loss: 1.9349\n","Train Epoch: 3 [59520/110534 (54%)]\tAll Loss: 2.0145\tTriple Loss(0): 0.2686\tClassification Loss: 1.4773\n","Train Epoch: 3 [59840/110534 (54%)]\tAll Loss: 2.2136\tTriple Loss(1): 0.3250\tClassification Loss: 1.5636\n","Train Epoch: 3 [60160/110534 (54%)]\tAll Loss: 2.4810\tTriple Loss(0): 0.5369\tClassification Loss: 1.4072\n","Train Epoch: 3 [60480/110534 (55%)]\tAll Loss: 1.6357\tTriple Loss(0): 0.0000\tClassification Loss: 1.6357\n","\n","Test set: Average loss: 1.6252, Accuracy: 549/960 (57%)\n","\n","Train Epoch: 3 [60800/110534 (55%)]\tAll Loss: 2.0973\tTriple Loss(1): 0.1624\tClassification Loss: 1.7725\n","Train Epoch: 3 [61120/110534 (55%)]\tAll Loss: 2.3301\tTriple Loss(1): 0.2503\tClassification Loss: 1.8295\n","Train Epoch: 3 [61440/110534 (56%)]\tAll Loss: 1.4669\tTriple Loss(1): 0.1950\tClassification Loss: 1.0768\n","Train Epoch: 3 [61760/110534 (56%)]\tAll Loss: 2.2899\tTriple Loss(1): 0.2767\tClassification Loss: 1.7364\n","Train Epoch: 3 [62080/110534 (56%)]\tAll Loss: 2.1694\tTriple Loss(1): 0.3168\tClassification Loss: 1.5358\n","Train Epoch: 3 [62400/110534 (56%)]\tAll Loss: 2.1228\tTriple Loss(1): 0.1726\tClassification Loss: 1.7776\n","Train Epoch: 3 [62720/110534 (57%)]\tAll Loss: 2.0449\tTriple Loss(1): 0.2825\tClassification Loss: 1.4800\n","Train Epoch: 3 [63040/110534 (57%)]\tAll Loss: 2.3214\tTriple Loss(1): 0.2305\tClassification Loss: 1.8605\n","Train Epoch: 3 [63360/110534 (57%)]\tAll Loss: 1.5930\tTriple Loss(0): 0.0000\tClassification Loss: 1.5930\n","Train Epoch: 3 [63680/110534 (58%)]\tAll Loss: 1.6754\tTriple Loss(1): 0.0742\tClassification Loss: 1.5271\n","\n","Test set: Average loss: 1.6292, Accuracy: 548/960 (57%)\n","\n","Train Epoch: 3 [64000/110534 (58%)]\tAll Loss: 3.0538\tTriple Loss(1): 0.3916\tClassification Loss: 2.2706\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_2000.pth.tar\n","Train Epoch: 3 [64320/110534 (58%)]\tAll Loss: 1.7263\tTriple Loss(1): 0.2333\tClassification Loss: 1.2598\n","Train Epoch: 3 [64640/110534 (58%)]\tAll Loss: 3.1154\tTriple Loss(1): 0.3322\tClassification Loss: 2.4511\n","Train Epoch: 3 [64960/110534 (59%)]\tAll Loss: 2.0358\tTriple Loss(1): 0.2198\tClassification Loss: 1.5961\n","Train Epoch: 3 [65280/110534 (59%)]\tAll Loss: 2.3008\tTriple Loss(1): 0.2593\tClassification Loss: 1.7821\n","Train Epoch: 3 [65600/110534 (59%)]\tAll Loss: 1.9665\tTriple Loss(1): 0.1795\tClassification Loss: 1.6074\n","Train Epoch: 3 [65920/110534 (60%)]\tAll Loss: 2.3238\tTriple Loss(1): 0.3190\tClassification Loss: 1.6857\n","Train Epoch: 3 [66240/110534 (60%)]\tAll Loss: 1.5704\tTriple Loss(0): 0.0000\tClassification Loss: 1.5704\n","Train Epoch: 3 [66560/110534 (60%)]\tAll Loss: 1.5376\tTriple Loss(1): 0.1062\tClassification Loss: 1.3253\n","Train Epoch: 3 [66880/110534 (60%)]\tAll Loss: 2.0282\tTriple Loss(1): 0.1186\tClassification Loss: 1.7910\n","\n","Test set: Average loss: 1.6257, Accuracy: 549/960 (57%)\n","\n","Train Epoch: 3 [67200/110534 (61%)]\tAll Loss: 2.1903\tTriple Loss(1): 0.2226\tClassification Loss: 1.7452\n","Train Epoch: 3 [67520/110534 (61%)]\tAll Loss: 2.4715\tTriple Loss(1): 0.2599\tClassification Loss: 1.9517\n","Train Epoch: 3 [67840/110534 (61%)]\tAll Loss: 1.9117\tTriple Loss(0): 0.0000\tClassification Loss: 1.9117\n","Train Epoch: 3 [68160/110534 (62%)]\tAll Loss: 1.5613\tTriple Loss(1): 0.0740\tClassification Loss: 1.4133\n","Train Epoch: 3 [68480/110534 (62%)]\tAll Loss: 1.8814\tTriple Loss(1): 0.2353\tClassification Loss: 1.4107\n","Train Epoch: 3 [68800/110534 (62%)]\tAll Loss: 2.3777\tTriple Loss(0): 0.5468\tClassification Loss: 1.2841\n","Train Epoch: 3 [69120/110534 (63%)]\tAll Loss: 11.8949\tTriple Loss(0): 5.1917\tClassification Loss: 1.5115\n","Train Epoch: 3 [69440/110534 (63%)]\tAll Loss: 2.0683\tTriple Loss(1): 0.2568\tClassification Loss: 1.5547\n","Train Epoch: 3 [69760/110534 (63%)]\tAll Loss: 2.0952\tTriple Loss(1): 0.1475\tClassification Loss: 1.8003\n","Train Epoch: 3 [70080/110534 (63%)]\tAll Loss: 1.5824\tTriple Loss(0): 0.0000\tClassification Loss: 1.5824\n","\n","Test set: Average loss: 1.6151, Accuracy: 560/960 (58%)\n","\n","Train Epoch: 3 [70400/110534 (64%)]\tAll Loss: 1.7463\tTriple Loss(1): 0.1076\tClassification Loss: 1.5311\n","Train Epoch: 3 [70720/110534 (64%)]\tAll Loss: 1.3627\tTriple Loss(0): 0.0000\tClassification Loss: 1.3627\n","Train Epoch: 3 [71040/110534 (64%)]\tAll Loss: 1.9133\tTriple Loss(1): 0.1860\tClassification Loss: 1.5414\n","Train Epoch: 3 [71360/110534 (65%)]\tAll Loss: 2.1963\tTriple Loss(1): 0.2752\tClassification Loss: 1.6460\n","Train Epoch: 3 [71680/110534 (65%)]\tAll Loss: 1.8167\tTriple Loss(1): 0.1437\tClassification Loss: 1.5293\n","Train Epoch: 3 [72000/110534 (65%)]\tAll Loss: 2.1869\tTriple Loss(1): 0.2321\tClassification Loss: 1.7227\n","Train Epoch: 3 [72320/110534 (65%)]\tAll Loss: 1.4786\tTriple Loss(0): 0.0000\tClassification Loss: 1.4786\n","Train Epoch: 3 [72640/110534 (66%)]\tAll Loss: 1.8234\tTriple Loss(1): 0.2448\tClassification Loss: 1.3338\n","Train Epoch: 3 [72960/110534 (66%)]\tAll Loss: 1.6790\tTriple Loss(1): 0.1109\tClassification Loss: 1.4571\n","Train Epoch: 3 [73280/110534 (66%)]\tAll Loss: 1.6978\tTriple Loss(1): 0.2081\tClassification Loss: 1.2816\n","\n","Test set: Average loss: 1.6185, Accuracy: 550/960 (57%)\n","\n","Train Epoch: 3 [73600/110534 (67%)]\tAll Loss: 2.8201\tTriple Loss(1): 0.2554\tClassification Loss: 2.3093\n","Train Epoch: 3 [73920/110534 (67%)]\tAll Loss: 2.2153\tTriple Loss(1): 0.1019\tClassification Loss: 2.0114\n","Train Epoch: 3 [74240/110534 (67%)]\tAll Loss: 1.3148\tTriple Loss(0): 0.0000\tClassification Loss: 1.3148\n","Train Epoch: 3 [74560/110534 (67%)]\tAll Loss: 2.0693\tTriple Loss(1): 0.2190\tClassification Loss: 1.6312\n","Train Epoch: 3 [74880/110534 (68%)]\tAll Loss: 1.8192\tTriple Loss(0): 0.0000\tClassification Loss: 1.8192\n","Train Epoch: 3 [75200/110534 (68%)]\tAll Loss: 1.9503\tTriple Loss(1): 0.2487\tClassification Loss: 1.4530\n","Train Epoch: 3 [75520/110534 (68%)]\tAll Loss: 1.8273\tTriple Loss(1): 0.1392\tClassification Loss: 1.5489\n","Train Epoch: 3 [75840/110534 (69%)]\tAll Loss: 1.6608\tTriple Loss(0): 0.0000\tClassification Loss: 1.6608\n","Train Epoch: 3 [76160/110534 (69%)]\tAll Loss: 1.8226\tTriple Loss(1): 0.1005\tClassification Loss: 1.6215\n","Train Epoch: 3 [76480/110534 (69%)]\tAll Loss: 2.0707\tTriple Loss(1): 0.3536\tClassification Loss: 1.3635\n","\n","Test set: Average loss: 1.6147, Accuracy: 554/960 (58%)\n","\n","Train Epoch: 3 [76800/110534 (69%)]\tAll Loss: 2.1622\tTriple Loss(1): 0.3161\tClassification Loss: 1.5301\n","Train Epoch: 3 [77120/110534 (70%)]\tAll Loss: 1.9641\tTriple Loss(1): 0.1861\tClassification Loss: 1.5919\n","Train Epoch: 3 [77440/110534 (70%)]\tAll Loss: 2.0775\tTriple Loss(1): 0.1514\tClassification Loss: 1.7746\n","Train Epoch: 3 [77760/110534 (70%)]\tAll Loss: 1.8946\tTriple Loss(1): 0.2338\tClassification Loss: 1.4271\n","Train Epoch: 3 [78080/110534 (71%)]\tAll Loss: 2.7158\tTriple Loss(1): 0.3252\tClassification Loss: 2.0653\n","Train Epoch: 3 [78400/110534 (71%)]\tAll Loss: 1.8315\tTriple Loss(1): 0.1976\tClassification Loss: 1.4363\n","Train Epoch: 3 [78720/110534 (71%)]\tAll Loss: 1.2295\tTriple Loss(0): 0.0000\tClassification Loss: 1.2295\n","Train Epoch: 3 [79040/110534 (71%)]\tAll Loss: 2.2809\tTriple Loss(1): 0.2479\tClassification Loss: 1.7852\n","Train Epoch: 3 [79360/110534 (72%)]\tAll Loss: 1.9516\tTriple Loss(0): 0.0000\tClassification Loss: 1.9516\n","Train Epoch: 3 [79680/110534 (72%)]\tAll Loss: 1.5644\tTriple Loss(1): 0.0594\tClassification Loss: 1.4456\n","\n","Test set: Average loss: 1.6144, Accuracy: 547/960 (57%)\n","\n","Train Epoch: 3 [80000/110534 (72%)]\tAll Loss: 2.0505\tTriple Loss(1): 0.2476\tClassification Loss: 1.5553\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_2500.pth.tar\n","Train Epoch: 3 [80320/110534 (73%)]\tAll Loss: 1.1478\tTriple Loss(0): 0.0000\tClassification Loss: 1.1478\n","Train Epoch: 3 [80640/110534 (73%)]\tAll Loss: 1.7865\tTriple Loss(1): 0.1950\tClassification Loss: 1.3965\n","Train Epoch: 3 [80960/110534 (73%)]\tAll Loss: 2.0770\tTriple Loss(1): 0.2439\tClassification Loss: 1.5893\n","Train Epoch: 3 [81280/110534 (74%)]\tAll Loss: 1.6787\tTriple Loss(1): 0.1545\tClassification Loss: 1.3697\n","Train Epoch: 3 [81600/110534 (74%)]\tAll Loss: 1.9507\tTriple Loss(1): 0.2588\tClassification Loss: 1.4331\n","Train Epoch: 3 [81920/110534 (74%)]\tAll Loss: 2.1382\tTriple Loss(1): 0.1786\tClassification Loss: 1.7810\n","Train Epoch: 3 [82240/110534 (74%)]\tAll Loss: 1.9644\tTriple Loss(1): 0.1600\tClassification Loss: 1.6444\n","Train Epoch: 3 [82560/110534 (75%)]\tAll Loss: 1.9499\tTriple Loss(1): 0.1929\tClassification Loss: 1.5642\n","Train Epoch: 3 [82880/110534 (75%)]\tAll Loss: 2.2308\tTriple Loss(1): 0.1992\tClassification Loss: 1.8324\n","\n","Test set: Average loss: 1.6172, Accuracy: 550/960 (57%)\n","\n","Train Epoch: 3 [83200/110534 (75%)]\tAll Loss: 2.1292\tTriple Loss(1): 0.2177\tClassification Loss: 1.6938\n","Train Epoch: 3 [83520/110534 (76%)]\tAll Loss: 1.4743\tTriple Loss(0): 0.0000\tClassification Loss: 1.4743\n","Train Epoch: 3 [83840/110534 (76%)]\tAll Loss: 2.4492\tTriple Loss(1): 0.4035\tClassification Loss: 1.6422\n","Train Epoch: 3 [84160/110534 (76%)]\tAll Loss: 1.7572\tTriple Loss(1): 0.2667\tClassification Loss: 1.2238\n","Train Epoch: 3 [84480/110534 (76%)]\tAll Loss: 1.4341\tTriple Loss(0): 0.0000\tClassification Loss: 1.4341\n","Train Epoch: 3 [84800/110534 (77%)]\tAll Loss: 2.0831\tTriple Loss(1): 0.1803\tClassification Loss: 1.7226\n","Train Epoch: 3 [85120/110534 (77%)]\tAll Loss: 2.1895\tTriple Loss(1): 0.4047\tClassification Loss: 1.3800\n","Train Epoch: 3 [85440/110534 (77%)]\tAll Loss: 1.1450\tTriple Loss(0): 0.0000\tClassification Loss: 1.1450\n","Train Epoch: 3 [85760/110534 (78%)]\tAll Loss: 2.0557\tTriple Loss(1): 0.2168\tClassification Loss: 1.6220\n","Train Epoch: 3 [86080/110534 (78%)]\tAll Loss: 1.7505\tTriple Loss(0): 0.0000\tClassification Loss: 1.7505\n","\n","Test set: Average loss: 1.6118, Accuracy: 548/960 (57%)\n","\n","Train Epoch: 3 [86400/110534 (78%)]\tAll Loss: 2.3783\tTriple Loss(1): 0.1696\tClassification Loss: 2.0392\n","Train Epoch: 3 [86720/110534 (78%)]\tAll Loss: 1.2384\tTriple Loss(0): 0.0000\tClassification Loss: 1.2384\n","Train Epoch: 3 [87040/110534 (79%)]\tAll Loss: 1.5438\tTriple Loss(0): 0.0000\tClassification Loss: 1.5438\n","Train Epoch: 3 [87360/110534 (79%)]\tAll Loss: 2.3095\tTriple Loss(1): 0.2727\tClassification Loss: 1.7641\n","Train Epoch: 3 [87680/110534 (79%)]\tAll Loss: 1.9177\tTriple Loss(1): 0.2096\tClassification Loss: 1.4984\n","Train Epoch: 3 [88000/110534 (80%)]\tAll Loss: 2.0831\tTriple Loss(0): 0.0000\tClassification Loss: 2.0831\n","Train Epoch: 3 [88320/110534 (80%)]\tAll Loss: 1.8857\tTriple Loss(1): 0.1446\tClassification Loss: 1.5965\n","Train Epoch: 3 [88640/110534 (80%)]\tAll Loss: 1.9323\tTriple Loss(0): 0.0000\tClassification Loss: 1.9323\n","Train Epoch: 3 [88960/110534 (80%)]\tAll Loss: 2.5407\tTriple Loss(1): 0.3541\tClassification Loss: 1.8324\n","Train Epoch: 3 [89280/110534 (81%)]\tAll Loss: 1.5860\tTriple Loss(1): 0.0843\tClassification Loss: 1.4173\n","\n","Test set: Average loss: 1.6076, Accuracy: 555/960 (58%)\n","\n","Train Epoch: 3 [89600/110534 (81%)]\tAll Loss: 2.0985\tTriple Loss(0): 0.0000\tClassification Loss: 2.0985\n","Train Epoch: 3 [89920/110534 (81%)]\tAll Loss: 2.3212\tTriple Loss(1): 0.2344\tClassification Loss: 1.8523\n","Train Epoch: 3 [90240/110534 (82%)]\tAll Loss: 1.6913\tTriple Loss(1): 0.1502\tClassification Loss: 1.3908\n","Train Epoch: 3 [90560/110534 (82%)]\tAll Loss: 1.7004\tTriple Loss(1): 0.1354\tClassification Loss: 1.4296\n","Train Epoch: 3 [90880/110534 (82%)]\tAll Loss: 2.5980\tTriple Loss(1): 0.4080\tClassification Loss: 1.7819\n","Train Epoch: 3 [91200/110534 (82%)]\tAll Loss: 2.1935\tTriple Loss(1): 0.3464\tClassification Loss: 1.5006\n","Train Epoch: 3 [91520/110534 (83%)]\tAll Loss: 2.7347\tTriple Loss(1): 0.5830\tClassification Loss: 1.5688\n","Train Epoch: 3 [91840/110534 (83%)]\tAll Loss: 2.1010\tTriple Loss(1): 0.2340\tClassification Loss: 1.6331\n","Train Epoch: 3 [92160/110534 (83%)]\tAll Loss: 2.0085\tTriple Loss(0): 0.0000\tClassification Loss: 2.0085\n","Train Epoch: 3 [92480/110534 (84%)]\tAll Loss: 1.9461\tTriple Loss(1): 0.1932\tClassification Loss: 1.5598\n","\n","Test set: Average loss: 1.6155, Accuracy: 551/960 (57%)\n","\n","Train Epoch: 3 [92800/110534 (84%)]\tAll Loss: 1.5008\tTriple Loss(0): 0.0000\tClassification Loss: 1.5008\n","Train Epoch: 3 [93120/110534 (84%)]\tAll Loss: 2.3300\tTriple Loss(1): 0.2869\tClassification Loss: 1.7563\n","Train Epoch: 3 [93440/110534 (85%)]\tAll Loss: 2.2402\tTriple Loss(1): 0.3373\tClassification Loss: 1.5655\n","Train Epoch: 3 [93760/110534 (85%)]\tAll Loss: 1.4289\tTriple Loss(0): 0.0000\tClassification Loss: 1.4289\n","Train Epoch: 3 [94080/110534 (85%)]\tAll Loss: 1.9068\tTriple Loss(1): 0.1801\tClassification Loss: 1.5466\n","Train Epoch: 3 [94400/110534 (85%)]\tAll Loss: 2.2672\tTriple Loss(1): 0.2430\tClassification Loss: 1.7813\n","Train Epoch: 3 [94720/110534 (86%)]\tAll Loss: 1.9643\tTriple Loss(1): 0.1864\tClassification Loss: 1.5915\n","Train Epoch: 3 [95040/110534 (86%)]\tAll Loss: 2.0011\tTriple Loss(1): 0.1701\tClassification Loss: 1.6609\n","Train Epoch: 3 [95360/110534 (86%)]\tAll Loss: 2.4574\tTriple Loss(1): 0.4822\tClassification Loss: 1.4931\n","Train Epoch: 3 [95680/110534 (87%)]\tAll Loss: 2.1804\tTriple Loss(1): 0.2329\tClassification Loss: 1.7146\n","\n","Test set: Average loss: 1.6164, Accuracy: 548/960 (57%)\n","\n","Train Epoch: 3 [96000/110534 (87%)]\tAll Loss: 1.6168\tTriple Loss(1): 0.1377\tClassification Loss: 1.3414\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_3000.pth.tar\n","Train Epoch: 3 [96320/110534 (87%)]\tAll Loss: 1.4745\tTriple Loss(0): 0.0000\tClassification Loss: 1.4745\n","Train Epoch: 3 [96640/110534 (87%)]\tAll Loss: 1.8507\tTriple Loss(1): 0.0667\tClassification Loss: 1.7172\n","Train Epoch: 3 [96960/110534 (88%)]\tAll Loss: 1.8136\tTriple Loss(1): 0.2362\tClassification Loss: 1.3411\n","Train Epoch: 3 [97280/110534 (88%)]\tAll Loss: 1.6458\tTriple Loss(1): 0.1205\tClassification Loss: 1.4048\n","Train Epoch: 3 [97600/110534 (88%)]\tAll Loss: 1.7890\tTriple Loss(0): 0.0763\tClassification Loss: 1.6364\n","Train Epoch: 3 [97920/110534 (89%)]\tAll Loss: 1.2719\tTriple Loss(0): 0.0000\tClassification Loss: 1.2719\n","Train Epoch: 3 [98240/110534 (89%)]\tAll Loss: 1.4392\tTriple Loss(1): 0.0829\tClassification Loss: 1.2735\n","Train Epoch: 3 [98560/110534 (89%)]\tAll Loss: 1.3552\tTriple Loss(0): 0.0000\tClassification Loss: 1.3552\n","Train Epoch: 3 [98880/110534 (89%)]\tAll Loss: 2.1221\tTriple Loss(1): 0.3400\tClassification Loss: 1.4421\n","\n","Test set: Average loss: 1.6239, Accuracy: 546/960 (57%)\n","\n","Train Epoch: 3 [99200/110534 (90%)]\tAll Loss: 2.6382\tTriple Loss(1): 0.5282\tClassification Loss: 1.5817\n","Train Epoch: 3 [99520/110534 (90%)]\tAll Loss: 1.8999\tTriple Loss(1): 0.1222\tClassification Loss: 1.6554\n","Train Epoch: 3 [99840/110534 (90%)]\tAll Loss: 2.0371\tTriple Loss(1): 0.2075\tClassification Loss: 1.6221\n","Train Epoch: 3 [100160/110534 (91%)]\tAll Loss: 2.5638\tTriple Loss(1): 0.3251\tClassification Loss: 1.9136\n","Train Epoch: 3 [100480/110534 (91%)]\tAll Loss: 2.0411\tTriple Loss(1): 0.3208\tClassification Loss: 1.3995\n","Train Epoch: 3 [100800/110534 (91%)]\tAll Loss: 2.6828\tTriple Loss(1): 0.3388\tClassification Loss: 2.0052\n","Train Epoch: 3 [101120/110534 (91%)]\tAll Loss: 1.8462\tTriple Loss(1): 0.1878\tClassification Loss: 1.4707\n","Train Epoch: 3 [101440/110534 (92%)]\tAll Loss: 1.6994\tTriple Loss(1): 0.1926\tClassification Loss: 1.3142\n","Train Epoch: 3 [101760/110534 (92%)]\tAll Loss: 2.0764\tTriple Loss(1): 0.1943\tClassification Loss: 1.6878\n","Train Epoch: 3 [102080/110534 (92%)]\tAll Loss: 2.6985\tTriple Loss(1): 0.3695\tClassification Loss: 1.9595\n","\n","Test set: Average loss: 1.6157, Accuracy: 542/960 (56%)\n","\n","Train Epoch: 3 [102400/110534 (93%)]\tAll Loss: 1.9388\tTriple Loss(1): 0.0979\tClassification Loss: 1.7431\n","Train Epoch: 3 [102720/110534 (93%)]\tAll Loss: 2.2306\tTriple Loss(1): 0.2005\tClassification Loss: 1.8295\n","Train Epoch: 3 [103040/110534 (93%)]\tAll Loss: 2.1076\tTriple Loss(1): 0.2692\tClassification Loss: 1.5693\n","Train Epoch: 3 [103360/110534 (93%)]\tAll Loss: 1.9942\tTriple Loss(1): 0.1480\tClassification Loss: 1.6983\n","Train Epoch: 3 [103680/110534 (94%)]\tAll Loss: 2.4466\tTriple Loss(1): 0.3272\tClassification Loss: 1.7922\n","Train Epoch: 3 [104000/110534 (94%)]\tAll Loss: 2.1344\tTriple Loss(1): 0.3220\tClassification Loss: 1.4903\n","Train Epoch: 3 [104320/110534 (94%)]\tAll Loss: 2.1739\tTriple Loss(0): 0.0000\tClassification Loss: 2.1739\n","Train Epoch: 3 [104640/110534 (95%)]\tAll Loss: 1.9766\tTriple Loss(0): 0.0000\tClassification Loss: 1.9766\n","Train Epoch: 3 [104960/110534 (95%)]\tAll Loss: 1.9880\tTriple Loss(1): 0.2906\tClassification Loss: 1.4069\n","Train Epoch: 3 [105280/110534 (95%)]\tAll Loss: 2.5649\tTriple Loss(1): 0.4854\tClassification Loss: 1.5940\n","\n","Test set: Average loss: 1.6166, Accuracy: 549/960 (57%)\n","\n","Train Epoch: 3 [105600/110534 (96%)]\tAll Loss: 2.0278\tTriple Loss(1): 0.1926\tClassification Loss: 1.6426\n","Train Epoch: 3 [105920/110534 (96%)]\tAll Loss: 2.0284\tTriple Loss(1): 0.1864\tClassification Loss: 1.6557\n","Train Epoch: 3 [106240/110534 (96%)]\tAll Loss: 1.5826\tTriple Loss(1): 0.1594\tClassification Loss: 1.2639\n","Train Epoch: 3 [106560/110534 (96%)]\tAll Loss: 2.1635\tTriple Loss(1): 0.2631\tClassification Loss: 1.6373\n","Train Epoch: 3 [106880/110534 (97%)]\tAll Loss: 2.3258\tTriple Loss(1): 0.1864\tClassification Loss: 1.9529\n","Train Epoch: 3 [107200/110534 (97%)]\tAll Loss: 1.7433\tTriple Loss(0): 0.0000\tClassification Loss: 1.7433\n","Train Epoch: 3 [107520/110534 (97%)]\tAll Loss: 1.7301\tTriple Loss(0): 0.0000\tClassification Loss: 1.7301\n","Train Epoch: 3 [107840/110534 (98%)]\tAll Loss: 1.4623\tTriple Loss(0): 0.0000\tClassification Loss: 1.4623\n","Train Epoch: 3 [108160/110534 (98%)]\tAll Loss: 1.7753\tTriple Loss(1): 0.2587\tClassification Loss: 1.2580\n","Train Epoch: 3 [108480/110534 (98%)]\tAll Loss: 2.4207\tTriple Loss(1): 0.4230\tClassification Loss: 1.5748\n","\n","Test set: Average loss: 1.6027, Accuracy: 553/960 (58%)\n","\n","Train Epoch: 3 [108800/110534 (98%)]\tAll Loss: 1.6664\tTriple Loss(1): 0.1057\tClassification Loss: 1.4551\n","Train Epoch: 3 [109120/110534 (99%)]\tAll Loss: 2.1947\tTriple Loss(1): 0.1654\tClassification Loss: 1.8640\n","Train Epoch: 3 [109440/110534 (99%)]\tAll Loss: 1.6944\tTriple Loss(1): 0.1878\tClassification Loss: 1.3188\n","Train Epoch: 3 [109760/110534 (99%)]\tAll Loss: 1.9214\tTriple Loss(1): 0.1521\tClassification Loss: 1.6173\n","Train Epoch: 3 [110080/110534 (100%)]\tAll Loss: 1.8071\tTriple Loss(1): 0.1050\tClassification Loss: 1.5970\n","Train Epoch: 3 [110400/110534 (100%)]\tAll Loss: 2.4242\tTriple Loss(1): 0.3726\tClassification Loss: 1.6790\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_3_final.pth.tar\n","\n","Test set: Average loss: 1.6079, Accuracy: 550/960 (57%)\n","\n","Train Epoch: 4 [0/110534 (0%)]\tAll Loss: 1.6143\tTriple Loss(0): 0.0000\tClassification Loss: 1.6143\n","Train Epoch: 4 [320/110534 (0%)]\tAll Loss: 1.5282\tTriple Loss(0): 0.0000\tClassification Loss: 1.5282\n","Train Epoch: 4 [640/110534 (1%)]\tAll Loss: 1.4718\tTriple Loss(1): 0.2255\tClassification Loss: 1.0208\n","Train Epoch: 4 [960/110534 (1%)]\tAll Loss: 2.0784\tTriple Loss(0): 0.0000\tClassification Loss: 2.0784\n","Train Epoch: 4 [1280/110534 (1%)]\tAll Loss: 2.0765\tTriple Loss(1): 0.2275\tClassification Loss: 1.6215\n","Train Epoch: 4 [1600/110534 (1%)]\tAll Loss: 2.2020\tTriple Loss(1): 0.1992\tClassification Loss: 1.8037\n","Train Epoch: 4 [1920/110534 (2%)]\tAll Loss: 2.2470\tTriple Loss(1): 0.1846\tClassification Loss: 1.8777\n","Train Epoch: 4 [2240/110534 (2%)]\tAll Loss: 2.0864\tTriple Loss(1): 0.2609\tClassification Loss: 1.5647\n","Train Epoch: 4 [2560/110534 (2%)]\tAll Loss: 1.7710\tTriple Loss(1): 0.0825\tClassification Loss: 1.6060\n","Train Epoch: 4 [2880/110534 (3%)]\tAll Loss: 2.4915\tTriple Loss(1): 0.3874\tClassification Loss: 1.7167\n","\n","Test set: Average loss: 1.6108, Accuracy: 549/960 (57%)\n","\n","Train Epoch: 4 [3200/110534 (3%)]\tAll Loss: 2.2865\tTriple Loss(1): 0.1099\tClassification Loss: 2.0667\n","Train Epoch: 4 [3520/110534 (3%)]\tAll Loss: 1.6985\tTriple Loss(1): 0.2248\tClassification Loss: 1.2490\n","Train Epoch: 4 [3840/110534 (3%)]\tAll Loss: 1.9458\tTriple Loss(1): 0.1119\tClassification Loss: 1.7220\n","Train Epoch: 4 [4160/110534 (4%)]\tAll Loss: 1.9680\tTriple Loss(1): 0.1429\tClassification Loss: 1.6822\n","Train Epoch: 4 [4480/110534 (4%)]\tAll Loss: 1.9162\tTriple Loss(1): 0.2272\tClassification Loss: 1.4618\n","Train Epoch: 4 [4800/110534 (4%)]\tAll Loss: 1.9294\tTriple Loss(1): 0.2557\tClassification Loss: 1.4180\n","Train Epoch: 4 [5120/110534 (5%)]\tAll Loss: 2.0178\tTriple Loss(1): 0.0698\tClassification Loss: 1.8782\n","Train Epoch: 4 [5440/110534 (5%)]\tAll Loss: 2.5363\tTriple Loss(1): 0.5070\tClassification Loss: 1.5224\n","Train Epoch: 4 [5760/110534 (5%)]\tAll Loss: 1.7934\tTriple Loss(1): 0.2674\tClassification Loss: 1.2586\n","Train Epoch: 4 [6080/110534 (5%)]\tAll Loss: 2.6046\tTriple Loss(1): 0.4923\tClassification Loss: 1.6200\n","\n","Test set: Average loss: 1.6040, Accuracy: 551/960 (57%)\n","\n","Train Epoch: 4 [6400/110534 (6%)]\tAll Loss: 2.2226\tTriple Loss(1): 0.3075\tClassification Loss: 1.6076\n","Train Epoch: 4 [6720/110534 (6%)]\tAll Loss: 2.1916\tTriple Loss(1): 0.2620\tClassification Loss: 1.6677\n","Train Epoch: 4 [7040/110534 (6%)]\tAll Loss: 2.1920\tTriple Loss(1): 0.2000\tClassification Loss: 1.7921\n","Train Epoch: 4 [7360/110534 (7%)]\tAll Loss: 1.8703\tTriple Loss(1): 0.2057\tClassification Loss: 1.4590\n","Train Epoch: 4 [7680/110534 (7%)]\tAll Loss: 1.3664\tTriple Loss(1): 0.0864\tClassification Loss: 1.1935\n","Train Epoch: 4 [8000/110534 (7%)]\tAll Loss: 1.9324\tTriple Loss(1): 0.3025\tClassification Loss: 1.3275\n","Train Epoch: 4 [8320/110534 (8%)]\tAll Loss: 1.5880\tTriple Loss(1): 0.0443\tClassification Loss: 1.4994\n","Train Epoch: 4 [8640/110534 (8%)]\tAll Loss: 1.2107\tTriple Loss(0): 0.0000\tClassification Loss: 1.2107\n","Train Epoch: 4 [8960/110534 (8%)]\tAll Loss: 1.2963\tTriple Loss(0): 0.0000\tClassification Loss: 1.2963\n","Train Epoch: 4 [9280/110534 (8%)]\tAll Loss: 1.3921\tTriple Loss(1): 0.0349\tClassification Loss: 1.3223\n","\n","Test set: Average loss: 1.6016, Accuracy: 554/960 (58%)\n","\n","Train Epoch: 4 [9600/110534 (9%)]\tAll Loss: 1.9484\tTriple Loss(1): 0.0618\tClassification Loss: 1.8248\n","Train Epoch: 4 [9920/110534 (9%)]\tAll Loss: 1.6882\tTriple Loss(0): 0.0000\tClassification Loss: 1.6882\n","Train Epoch: 4 [10240/110534 (9%)]\tAll Loss: 2.3149\tTriple Loss(1): 0.2613\tClassification Loss: 1.7924\n","Train Epoch: 4 [10560/110534 (10%)]\tAll Loss: 2.0079\tTriple Loss(1): 0.1805\tClassification Loss: 1.6469\n","Train Epoch: 4 [10880/110534 (10%)]\tAll Loss: 2.1541\tTriple Loss(1): 0.1742\tClassification Loss: 1.8058\n","Train Epoch: 4 [11200/110534 (10%)]\tAll Loss: 1.2090\tTriple Loss(0): 0.0000\tClassification Loss: 1.2090\n","Train Epoch: 4 [11520/110534 (10%)]\tAll Loss: 1.6949\tTriple Loss(1): 0.1355\tClassification Loss: 1.4240\n","Train Epoch: 4 [11840/110534 (11%)]\tAll Loss: 2.2563\tTriple Loss(1): 0.2764\tClassification Loss: 1.7035\n","Train Epoch: 4 [12160/110534 (11%)]\tAll Loss: 1.9195\tTriple Loss(1): 0.1727\tClassification Loss: 1.5741\n","Train Epoch: 4 [12480/110534 (11%)]\tAll Loss: 1.7369\tTriple Loss(0): 0.0000\tClassification Loss: 1.7369\n","\n","Test set: Average loss: 1.6025, Accuracy: 547/960 (57%)\n","\n","Train Epoch: 4 [12800/110534 (12%)]\tAll Loss: 1.9491\tTriple Loss(1): 0.2992\tClassification Loss: 1.3506\n","Train Epoch: 4 [13120/110534 (12%)]\tAll Loss: 1.6719\tTriple Loss(1): 0.1450\tClassification Loss: 1.3819\n","Train Epoch: 4 [13440/110534 (12%)]\tAll Loss: 2.2059\tTriple Loss(1): 0.1689\tClassification Loss: 1.8681\n","Train Epoch: 4 [13760/110534 (12%)]\tAll Loss: 1.9861\tTriple Loss(1): 0.1073\tClassification Loss: 1.7715\n","Train Epoch: 4 [14080/110534 (13%)]\tAll Loss: 1.8783\tTriple Loss(0): 0.0000\tClassification Loss: 1.8783\n","Train Epoch: 4 [14400/110534 (13%)]\tAll Loss: 2.7916\tTriple Loss(1): 0.3747\tClassification Loss: 2.0423\n","Train Epoch: 4 [14720/110534 (13%)]\tAll Loss: 1.9012\tTriple Loss(1): 0.1365\tClassification Loss: 1.6282\n","Train Epoch: 4 [15040/110534 (14%)]\tAll Loss: 2.0235\tTriple Loss(1): 0.2773\tClassification Loss: 1.4689\n","Train Epoch: 4 [15360/110534 (14%)]\tAll Loss: 2.2557\tTriple Loss(1): 0.3413\tClassification Loss: 1.5732\n","Train Epoch: 4 [15680/110534 (14%)]\tAll Loss: 1.6727\tTriple Loss(1): 0.2108\tClassification Loss: 1.2511\n","\n","Test set: Average loss: 1.6021, Accuracy: 544/960 (57%)\n","\n","Train Epoch: 4 [16000/110534 (14%)]\tAll Loss: 1.9459\tTriple Loss(1): 0.1248\tClassification Loss: 1.6963\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_500.pth.tar\n","Train Epoch: 4 [16320/110534 (15%)]\tAll Loss: 1.2675\tTriple Loss(0): 0.0000\tClassification Loss: 1.2675\n","Train Epoch: 4 [16640/110534 (15%)]\tAll Loss: 1.9627\tTriple Loss(1): 0.1616\tClassification Loss: 1.6395\n","Train Epoch: 4 [16960/110534 (15%)]\tAll Loss: 2.0852\tTriple Loss(1): 0.1504\tClassification Loss: 1.7844\n","Train Epoch: 4 [17280/110534 (16%)]\tAll Loss: 1.7734\tTriple Loss(1): 0.1713\tClassification Loss: 1.4308\n","Train Epoch: 4 [17600/110534 (16%)]\tAll Loss: 2.0336\tTriple Loss(1): 0.1905\tClassification Loss: 1.6527\n","Train Epoch: 4 [17920/110534 (16%)]\tAll Loss: 2.0750\tTriple Loss(1): 0.1942\tClassification Loss: 1.6865\n","Train Epoch: 4 [18240/110534 (16%)]\tAll Loss: 7.0800\tTriple Loss(0): 2.5153\tClassification Loss: 2.0494\n","Train Epoch: 4 [18560/110534 (17%)]\tAll Loss: 1.5806\tTriple Loss(0): 0.0000\tClassification Loss: 1.5806\n","Train Epoch: 4 [18880/110534 (17%)]\tAll Loss: 2.3235\tTriple Loss(1): 0.2706\tClassification Loss: 1.7824\n","\n","Test set: Average loss: 1.6059, Accuracy: 556/960 (58%)\n","\n","Train Epoch: 4 [19200/110534 (17%)]\tAll Loss: 1.9615\tTriple Loss(1): 0.2957\tClassification Loss: 1.3700\n","Train Epoch: 4 [19520/110534 (18%)]\tAll Loss: 1.5681\tTriple Loss(0): 0.0000\tClassification Loss: 1.5681\n","Train Epoch: 4 [19840/110534 (18%)]\tAll Loss: 2.7177\tTriple Loss(0): 0.5612\tClassification Loss: 1.5954\n","Train Epoch: 4 [20160/110534 (18%)]\tAll Loss: 1.3343\tTriple Loss(0): 0.0000\tClassification Loss: 1.3343\n","Train Epoch: 4 [20480/110534 (19%)]\tAll Loss: 2.0395\tTriple Loss(1): 0.2844\tClassification Loss: 1.4708\n","Train Epoch: 4 [20800/110534 (19%)]\tAll Loss: 2.4581\tTriple Loss(1): 0.2440\tClassification Loss: 1.9700\n","Train Epoch: 4 [21120/110534 (19%)]\tAll Loss: 2.0580\tTriple Loss(1): 0.2701\tClassification Loss: 1.5178\n","Train Epoch: 4 [21440/110534 (19%)]\tAll Loss: 1.7876\tTriple Loss(1): 0.1790\tClassification Loss: 1.4296\n","Train Epoch: 4 [21760/110534 (20%)]\tAll Loss: 2.6684\tTriple Loss(1): 0.3029\tClassification Loss: 2.0626\n","Train Epoch: 4 [22080/110534 (20%)]\tAll Loss: 1.9059\tTriple Loss(1): 0.1170\tClassification Loss: 1.6719\n","\n","Test set: Average loss: 1.6078, Accuracy: 546/960 (57%)\n","\n","Train Epoch: 4 [22400/110534 (20%)]\tAll Loss: 1.5224\tTriple Loss(0): 0.0000\tClassification Loss: 1.5224\n","Train Epoch: 4 [22720/110534 (21%)]\tAll Loss: 1.9223\tTriple Loss(1): 0.1637\tClassification Loss: 1.5950\n","Train Epoch: 4 [23040/110534 (21%)]\tAll Loss: 2.0793\tTriple Loss(1): 0.1404\tClassification Loss: 1.7986\n","Train Epoch: 4 [23360/110534 (21%)]\tAll Loss: 1.8507\tTriple Loss(1): 0.2032\tClassification Loss: 1.4443\n","Train Epoch: 4 [23680/110534 (21%)]\tAll Loss: 2.2691\tTriple Loss(1): 0.2072\tClassification Loss: 1.8546\n","Train Epoch: 4 [24000/110534 (22%)]\tAll Loss: 1.9754\tTriple Loss(1): 0.1977\tClassification Loss: 1.5801\n","Train Epoch: 4 [24320/110534 (22%)]\tAll Loss: 2.3338\tTriple Loss(1): 0.3313\tClassification Loss: 1.6712\n","Train Epoch: 4 [24640/110534 (22%)]\tAll Loss: 2.4249\tTriple Loss(1): 0.2195\tClassification Loss: 1.9858\n","Train Epoch: 4 [24960/110534 (23%)]\tAll Loss: 1.8664\tTriple Loss(1): 0.2435\tClassification Loss: 1.3795\n","Train Epoch: 4 [25280/110534 (23%)]\tAll Loss: 1.6723\tTriple Loss(1): 0.0686\tClassification Loss: 1.5351\n","\n","Test set: Average loss: 1.6048, Accuracy: 555/960 (58%)\n","\n","Train Epoch: 4 [25600/110534 (23%)]\tAll Loss: 1.5907\tTriple Loss(0): 0.0000\tClassification Loss: 1.5907\n","Train Epoch: 4 [25920/110534 (23%)]\tAll Loss: 2.3169\tTriple Loss(1): 0.2936\tClassification Loss: 1.7297\n","Train Epoch: 4 [26240/110534 (24%)]\tAll Loss: 2.2649\tTriple Loss(1): 0.3589\tClassification Loss: 1.5471\n","Train Epoch: 4 [26560/110534 (24%)]\tAll Loss: 2.1117\tTriple Loss(1): 0.1522\tClassification Loss: 1.8073\n","Train Epoch: 4 [26880/110534 (24%)]\tAll Loss: 2.2916\tTriple Loss(1): 0.2714\tClassification Loss: 1.7487\n","Train Epoch: 4 [27200/110534 (25%)]\tAll Loss: 2.2312\tTriple Loss(1): 0.1760\tClassification Loss: 1.8793\n","Train Epoch: 4 [27520/110534 (25%)]\tAll Loss: 1.7053\tTriple Loss(1): 0.0000\tClassification Loss: 1.7053\n","Train Epoch: 4 [27840/110534 (25%)]\tAll Loss: 1.5078\tTriple Loss(0): 0.0000\tClassification Loss: 1.5078\n","Train Epoch: 4 [28160/110534 (25%)]\tAll Loss: 2.1821\tTriple Loss(1): 0.1941\tClassification Loss: 1.7939\n","Train Epoch: 4 [28480/110534 (26%)]\tAll Loss: 1.9387\tTriple Loss(1): 0.2361\tClassification Loss: 1.4665\n","\n","Test set: Average loss: 1.6056, Accuracy: 553/960 (58%)\n","\n","Train Epoch: 4 [28800/110534 (26%)]\tAll Loss: 1.4205\tTriple Loss(0): 0.0000\tClassification Loss: 1.4205\n","Train Epoch: 4 [29120/110534 (26%)]\tAll Loss: 2.0927\tTriple Loss(1): 0.2764\tClassification Loss: 1.5399\n","Train Epoch: 4 [29440/110534 (27%)]\tAll Loss: 1.3972\tTriple Loss(0): 0.0000\tClassification Loss: 1.3972\n","Train Epoch: 4 [29760/110534 (27%)]\tAll Loss: 1.7598\tTriple Loss(1): 0.2205\tClassification Loss: 1.3188\n","Train Epoch: 4 [30080/110534 (27%)]\tAll Loss: 1.7648\tTriple Loss(1): 0.1728\tClassification Loss: 1.4191\n","Train Epoch: 4 [30400/110534 (27%)]\tAll Loss: 2.0758\tTriple Loss(1): 0.1662\tClassification Loss: 1.7433\n","Train Epoch: 4 [30720/110534 (28%)]\tAll Loss: 1.6051\tTriple Loss(1): 0.1708\tClassification Loss: 1.2636\n","Train Epoch: 4 [31040/110534 (28%)]\tAll Loss: 2.6178\tTriple Loss(1): 0.3544\tClassification Loss: 1.9090\n","Train Epoch: 4 [31360/110534 (28%)]\tAll Loss: 1.7841\tTriple Loss(1): 0.0783\tClassification Loss: 1.6275\n","Train Epoch: 4 [31680/110534 (29%)]\tAll Loss: 2.1278\tTriple Loss(1): 0.2509\tClassification Loss: 1.6260\n","\n","Test set: Average loss: 1.5984, Accuracy: 560/960 (58%)\n","\n","Train Epoch: 4 [32000/110534 (29%)]\tAll Loss: 2.3783\tTriple Loss(1): 0.3408\tClassification Loss: 1.6968\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_1000.pth.tar\n","Train Epoch: 4 [32320/110534 (29%)]\tAll Loss: 1.5942\tTriple Loss(1): 0.1384\tClassification Loss: 1.3174\n","Train Epoch: 4 [32640/110534 (30%)]\tAll Loss: 1.6929\tTriple Loss(0): 0.0000\tClassification Loss: 1.6929\n","Train Epoch: 4 [32960/110534 (30%)]\tAll Loss: 2.1175\tTriple Loss(1): 0.1803\tClassification Loss: 1.7569\n","Train Epoch: 4 [33280/110534 (30%)]\tAll Loss: 2.5035\tTriple Loss(1): 0.2684\tClassification Loss: 1.9666\n","Train Epoch: 4 [33600/110534 (30%)]\tAll Loss: 1.8624\tTriple Loss(1): 0.1163\tClassification Loss: 1.6297\n","Train Epoch: 4 [33920/110534 (31%)]\tAll Loss: 1.5929\tTriple Loss(0): 0.0000\tClassification Loss: 1.5929\n","Train Epoch: 4 [34240/110534 (31%)]\tAll Loss: 1.9849\tTriple Loss(1): 0.2240\tClassification Loss: 1.5369\n","Train Epoch: 4 [34560/110534 (31%)]\tAll Loss: 2.0687\tTriple Loss(1): 0.2806\tClassification Loss: 1.5074\n","Train Epoch: 4 [34880/110534 (32%)]\tAll Loss: 1.7505\tTriple Loss(1): 0.2626\tClassification Loss: 1.2252\n","\n","Test set: Average loss: 1.6050, Accuracy: 557/960 (58%)\n","\n","Train Epoch: 4 [35200/110534 (32%)]\tAll Loss: 2.4097\tTriple Loss(1): 0.2287\tClassification Loss: 1.9523\n","Train Epoch: 4 [35520/110534 (32%)]\tAll Loss: 2.0083\tTriple Loss(1): 0.1816\tClassification Loss: 1.6451\n","Train Epoch: 4 [35840/110534 (32%)]\tAll Loss: 1.6150\tTriple Loss(1): 0.1367\tClassification Loss: 1.3416\n","Train Epoch: 4 [36160/110534 (33%)]\tAll Loss: 2.1299\tTriple Loss(1): 0.1693\tClassification Loss: 1.7913\n","Train Epoch: 4 [36480/110534 (33%)]\tAll Loss: 2.1198\tTriple Loss(1): 0.1773\tClassification Loss: 1.7653\n","Train Epoch: 4 [36800/110534 (33%)]\tAll Loss: 2.0883\tTriple Loss(1): 0.2867\tClassification Loss: 1.5149\n","Train Epoch: 4 [37120/110534 (34%)]\tAll Loss: 2.3692\tTriple Loss(1): 0.2538\tClassification Loss: 1.8615\n","Train Epoch: 4 [37440/110534 (34%)]\tAll Loss: 2.1092\tTriple Loss(1): 0.2229\tClassification Loss: 1.6635\n","Train Epoch: 4 [37760/110534 (34%)]\tAll Loss: 1.7955\tTriple Loss(1): 0.1184\tClassification Loss: 1.5586\n","Train Epoch: 4 [38080/110534 (34%)]\tAll Loss: 2.2771\tTriple Loss(0): 0.2525\tClassification Loss: 1.7720\n","\n","Test set: Average loss: 1.5971, Accuracy: 553/960 (58%)\n","\n","Train Epoch: 4 [38400/110534 (35%)]\tAll Loss: 1.9470\tTriple Loss(1): 0.2871\tClassification Loss: 1.3728\n","Train Epoch: 4 [38720/110534 (35%)]\tAll Loss: 1.7556\tTriple Loss(0): 0.0000\tClassification Loss: 1.7556\n","Train Epoch: 4 [39040/110534 (35%)]\tAll Loss: 1.7056\tTriple Loss(1): 0.1890\tClassification Loss: 1.3276\n","Train Epoch: 4 [39360/110534 (36%)]\tAll Loss: 1.5900\tTriple Loss(1): 0.0680\tClassification Loss: 1.4541\n","Train Epoch: 4 [39680/110534 (36%)]\tAll Loss: 2.4338\tTriple Loss(1): 0.2951\tClassification Loss: 1.8436\n","Train Epoch: 4 [40000/110534 (36%)]\tAll Loss: 1.2722\tTriple Loss(0): 0.0000\tClassification Loss: 1.2722\n","Train Epoch: 4 [40320/110534 (36%)]\tAll Loss: 2.4052\tTriple Loss(1): 0.2999\tClassification Loss: 1.8054\n","Train Epoch: 4 [40640/110534 (37%)]\tAll Loss: 1.9097\tTriple Loss(1): 0.1354\tClassification Loss: 1.6389\n","Train Epoch: 4 [40960/110534 (37%)]\tAll Loss: 1.4005\tTriple Loss(0): 0.0000\tClassification Loss: 1.4005\n","Train Epoch: 4 [41280/110534 (37%)]\tAll Loss: 2.0128\tTriple Loss(1): 0.2322\tClassification Loss: 1.5485\n","\n","Test set: Average loss: 1.5955, Accuracy: 557/960 (58%)\n","\n","Train Epoch: 4 [41600/110534 (38%)]\tAll Loss: 1.6653\tTriple Loss(0): 0.0000\tClassification Loss: 1.6653\n","Train Epoch: 4 [41920/110534 (38%)]\tAll Loss: 2.0519\tTriple Loss(1): 0.2597\tClassification Loss: 1.5326\n","Train Epoch: 4 [42240/110534 (38%)]\tAll Loss: 1.9780\tTriple Loss(1): 0.1133\tClassification Loss: 1.7514\n","Train Epoch: 4 [42560/110534 (38%)]\tAll Loss: 1.9826\tTriple Loss(1): 0.2425\tClassification Loss: 1.4977\n","Train Epoch: 4 [42880/110534 (39%)]\tAll Loss: 2.1550\tTriple Loss(1): 0.2615\tClassification Loss: 1.6320\n","Train Epoch: 4 [43200/110534 (39%)]\tAll Loss: 2.3601\tTriple Loss(1): 0.3899\tClassification Loss: 1.5803\n","Train Epoch: 4 [43520/110534 (39%)]\tAll Loss: 1.3831\tTriple Loss(1): 0.0393\tClassification Loss: 1.3044\n","Train Epoch: 4 [43840/110534 (40%)]\tAll Loss: 2.1792\tTriple Loss(1): 0.2076\tClassification Loss: 1.7639\n","Train Epoch: 4 [44160/110534 (40%)]\tAll Loss: 2.4917\tTriple Loss(1): 0.3037\tClassification Loss: 1.8843\n","Train Epoch: 4 [44480/110534 (40%)]\tAll Loss: 2.0384\tTriple Loss(1): 0.2393\tClassification Loss: 1.5597\n","\n","Test set: Average loss: 1.5973, Accuracy: 556/960 (58%)\n","\n","Train Epoch: 4 [44800/110534 (41%)]\tAll Loss: 1.1836\tTriple Loss(1): 0.0557\tClassification Loss: 1.0722\n","Train Epoch: 4 [45120/110534 (41%)]\tAll Loss: 1.8973\tTriple Loss(1): 0.1777\tClassification Loss: 1.5420\n","Train Epoch: 4 [45440/110534 (41%)]\tAll Loss: 1.8518\tTriple Loss(1): 0.1733\tClassification Loss: 1.5051\n","Train Epoch: 4 [45760/110534 (41%)]\tAll Loss: 1.8246\tTriple Loss(1): 0.1285\tClassification Loss: 1.5676\n","Train Epoch: 4 [46080/110534 (42%)]\tAll Loss: 1.8610\tTriple Loss(1): 0.3239\tClassification Loss: 1.2131\n","Train Epoch: 4 [46400/110534 (42%)]\tAll Loss: 2.9003\tTriple Loss(1): 0.4369\tClassification Loss: 2.0265\n","Train Epoch: 4 [46720/110534 (42%)]\tAll Loss: 2.4275\tTriple Loss(1): 0.2438\tClassification Loss: 1.9398\n","Train Epoch: 4 [47040/110534 (43%)]\tAll Loss: 2.2211\tTriple Loss(1): 0.3706\tClassification Loss: 1.4798\n","Train Epoch: 4 [47360/110534 (43%)]\tAll Loss: 2.1354\tTriple Loss(1): 0.2548\tClassification Loss: 1.6257\n","Train Epoch: 4 [47680/110534 (43%)]\tAll Loss: 1.1829\tTriple Loss(0): 0.0000\tClassification Loss: 1.1829\n","\n","Test set: Average loss: 1.6000, Accuracy: 551/960 (57%)\n","\n","Train Epoch: 4 [48000/110534 (43%)]\tAll Loss: 2.3523\tTriple Loss(1): 0.2275\tClassification Loss: 1.8974\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_1500.pth.tar\n","Train Epoch: 4 [48320/110534 (44%)]\tAll Loss: 2.6035\tTriple Loss(1): 0.3631\tClassification Loss: 1.8773\n","Train Epoch: 4 [48640/110534 (44%)]\tAll Loss: 2.3342\tTriple Loss(1): 0.2337\tClassification Loss: 1.8669\n","Train Epoch: 4 [48960/110534 (44%)]\tAll Loss: 2.1643\tTriple Loss(1): 0.3470\tClassification Loss: 1.4702\n","Train Epoch: 4 [49280/110534 (45%)]\tAll Loss: 2.5824\tTriple Loss(1): 0.5587\tClassification Loss: 1.4649\n","Train Epoch: 4 [49600/110534 (45%)]\tAll Loss: 1.7900\tTriple Loss(0): 0.0000\tClassification Loss: 1.7900\n","Train Epoch: 4 [49920/110534 (45%)]\tAll Loss: 1.6180\tTriple Loss(1): 0.1099\tClassification Loss: 1.3982\n","Train Epoch: 4 [50240/110534 (45%)]\tAll Loss: 1.8281\tTriple Loss(1): 0.2966\tClassification Loss: 1.2350\n","Train Epoch: 4 [50560/110534 (46%)]\tAll Loss: 2.1639\tTriple Loss(1): 0.1529\tClassification Loss: 1.8580\n","Train Epoch: 4 [50880/110534 (46%)]\tAll Loss: 2.8016\tTriple Loss(1): 0.4540\tClassification Loss: 1.8936\n","\n","Test set: Average loss: 1.5952, Accuracy: 552/960 (58%)\n","\n","Train Epoch: 4 [51200/110534 (46%)]\tAll Loss: 2.0792\tTriple Loss(1): 0.2656\tClassification Loss: 1.5480\n","Train Epoch: 4 [51520/110534 (47%)]\tAll Loss: 1.3915\tTriple Loss(0): 0.0000\tClassification Loss: 1.3915\n","Train Epoch: 4 [51840/110534 (47%)]\tAll Loss: 1.6864\tTriple Loss(0): 0.0000\tClassification Loss: 1.6864\n","Train Epoch: 4 [52160/110534 (47%)]\tAll Loss: 2.3159\tTriple Loss(1): 0.2844\tClassification Loss: 1.7472\n","Train Epoch: 4 [52480/110534 (47%)]\tAll Loss: 2.4306\tTriple Loss(1): 0.1389\tClassification Loss: 2.1529\n","Train Epoch: 4 [52800/110534 (48%)]\tAll Loss: 2.0399\tTriple Loss(1): 0.2355\tClassification Loss: 1.5690\n","Train Epoch: 4 [53120/110534 (48%)]\tAll Loss: 2.1957\tTriple Loss(1): 0.2986\tClassification Loss: 1.5985\n","Train Epoch: 4 [53440/110534 (48%)]\tAll Loss: 2.3623\tTriple Loss(1): 0.2389\tClassification Loss: 1.8846\n","Train Epoch: 4 [53760/110534 (49%)]\tAll Loss: 2.0617\tTriple Loss(1): 0.1761\tClassification Loss: 1.7096\n","Train Epoch: 4 [54080/110534 (49%)]\tAll Loss: 2.5485\tTriple Loss(1): 0.2507\tClassification Loss: 2.0471\n","\n","Test set: Average loss: 1.5985, Accuracy: 554/960 (58%)\n","\n","Train Epoch: 4 [54400/110534 (49%)]\tAll Loss: 2.7689\tTriple Loss(1): 0.5512\tClassification Loss: 1.6665\n","Train Epoch: 4 [54720/110534 (49%)]\tAll Loss: 2.0710\tTriple Loss(1): 0.1850\tClassification Loss: 1.7009\n","Train Epoch: 4 [55040/110534 (50%)]\tAll Loss: 2.5161\tTriple Loss(1): 0.2355\tClassification Loss: 2.0451\n","Train Epoch: 4 [55360/110534 (50%)]\tAll Loss: 1.4399\tTriple Loss(0): 0.0435\tClassification Loss: 1.3529\n","Train Epoch: 4 [55680/110534 (50%)]\tAll Loss: 2.3011\tTriple Loss(1): 0.2409\tClassification Loss: 1.8192\n","Train Epoch: 4 [56000/110534 (51%)]\tAll Loss: 2.2018\tTriple Loss(0): 0.0000\tClassification Loss: 2.2018\n","Train Epoch: 4 [56320/110534 (51%)]\tAll Loss: 1.8208\tTriple Loss(0): 0.0000\tClassification Loss: 1.8208\n","Train Epoch: 4 [56640/110534 (51%)]\tAll Loss: 1.0498\tTriple Loss(0): 0.0000\tClassification Loss: 1.0498\n","Train Epoch: 4 [56960/110534 (52%)]\tAll Loss: 2.2406\tTriple Loss(1): 0.2633\tClassification Loss: 1.7141\n","Train Epoch: 4 [57280/110534 (52%)]\tAll Loss: 2.1007\tTriple Loss(1): 0.1892\tClassification Loss: 1.7223\n","\n","Test set: Average loss: 1.5972, Accuracy: 554/960 (58%)\n","\n","Train Epoch: 4 [57600/110534 (52%)]\tAll Loss: 1.8066\tTriple Loss(0): 0.0000\tClassification Loss: 1.8066\n","Train Epoch: 4 [57920/110534 (52%)]\tAll Loss: 1.9905\tTriple Loss(1): 0.1581\tClassification Loss: 1.6743\n","Train Epoch: 4 [58240/110534 (53%)]\tAll Loss: 1.4896\tTriple Loss(1): 0.1581\tClassification Loss: 1.1733\n","Train Epoch: 4 [58560/110534 (53%)]\tAll Loss: 1.2871\tTriple Loss(1): 0.0000\tClassification Loss: 1.2871\n","Train Epoch: 4 [58880/110534 (53%)]\tAll Loss: 2.4145\tTriple Loss(1): 0.3666\tClassification Loss: 1.6814\n","Train Epoch: 4 [59200/110534 (54%)]\tAll Loss: 2.0576\tTriple Loss(1): 0.0994\tClassification Loss: 1.8588\n","Train Epoch: 4 [59520/110534 (54%)]\tAll Loss: 1.8965\tTriple Loss(1): 0.1339\tClassification Loss: 1.6287\n","Train Epoch: 4 [59840/110534 (54%)]\tAll Loss: 1.9590\tTriple Loss(1): 0.2384\tClassification Loss: 1.4822\n","Train Epoch: 4 [60160/110534 (54%)]\tAll Loss: 1.5201\tTriple Loss(0): 0.0000\tClassification Loss: 1.5201\n","Train Epoch: 4 [60480/110534 (55%)]\tAll Loss: 1.5668\tTriple Loss(1): 0.0986\tClassification Loss: 1.3697\n","\n","Test set: Average loss: 1.5963, Accuracy: 554/960 (58%)\n","\n","Train Epoch: 4 [60800/110534 (55%)]\tAll Loss: 2.0602\tTriple Loss(1): 0.1946\tClassification Loss: 1.6709\n","Train Epoch: 4 [61120/110534 (55%)]\tAll Loss: 2.1042\tTriple Loss(1): 0.2008\tClassification Loss: 1.7026\n","Train Epoch: 4 [61440/110534 (56%)]\tAll Loss: 1.6207\tTriple Loss(1): 0.2812\tClassification Loss: 1.0583\n","Train Epoch: 4 [61760/110534 (56%)]\tAll Loss: 2.1675\tTriple Loss(1): 0.2654\tClassification Loss: 1.6368\n","Train Epoch: 4 [62080/110534 (56%)]\tAll Loss: 2.1382\tTriple Loss(1): 0.3114\tClassification Loss: 1.5154\n","Train Epoch: 4 [62400/110534 (56%)]\tAll Loss: 1.2154\tTriple Loss(0): 0.0000\tClassification Loss: 1.2154\n","Train Epoch: 4 [62720/110534 (57%)]\tAll Loss: 1.6270\tTriple Loss(1): 0.1103\tClassification Loss: 1.4063\n","Train Epoch: 4 [63040/110534 (57%)]\tAll Loss: 2.0711\tTriple Loss(1): 0.1132\tClassification Loss: 1.8447\n","Train Epoch: 4 [63360/110534 (57%)]\tAll Loss: 1.8845\tTriple Loss(1): 0.2035\tClassification Loss: 1.4775\n","Train Epoch: 4 [63680/110534 (58%)]\tAll Loss: 1.7647\tTriple Loss(1): 0.1691\tClassification Loss: 1.4265\n","\n","Test set: Average loss: 1.6047, Accuracy: 547/960 (57%)\n","\n","Train Epoch: 4 [64000/110534 (58%)]\tAll Loss: 2.3576\tTriple Loss(1): 0.0760\tClassification Loss: 2.2056\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_2000.pth.tar\n","Train Epoch: 4 [64320/110534 (58%)]\tAll Loss: 1.6223\tTriple Loss(1): 0.1551\tClassification Loss: 1.3121\n","Train Epoch: 4 [64640/110534 (58%)]\tAll Loss: 2.2186\tTriple Loss(0): 0.0000\tClassification Loss: 2.2186\n","Train Epoch: 4 [64960/110534 (59%)]\tAll Loss: 2.2444\tTriple Loss(1): 0.3298\tClassification Loss: 1.5847\n","Train Epoch: 4 [65280/110534 (59%)]\tAll Loss: 2.2175\tTriple Loss(1): 0.2350\tClassification Loss: 1.7475\n","Train Epoch: 4 [65600/110534 (59%)]\tAll Loss: 2.2488\tTriple Loss(1): 0.2247\tClassification Loss: 1.7994\n","Train Epoch: 4 [65920/110534 (60%)]\tAll Loss: 1.5568\tTriple Loss(0): 0.0000\tClassification Loss: 1.5568\n","Train Epoch: 4 [66240/110534 (60%)]\tAll Loss: 1.7678\tTriple Loss(1): 0.0361\tClassification Loss: 1.6956\n","Train Epoch: 4 [66560/110534 (60%)]\tAll Loss: 1.7393\tTriple Loss(1): 0.1750\tClassification Loss: 1.3892\n","Train Epoch: 4 [66880/110534 (60%)]\tAll Loss: 2.3561\tTriple Loss(1): 0.3850\tClassification Loss: 1.5861\n","\n","Test set: Average loss: 1.6003, Accuracy: 546/960 (57%)\n","\n","Train Epoch: 4 [67200/110534 (61%)]\tAll Loss: 2.1307\tTriple Loss(1): 0.1551\tClassification Loss: 1.8205\n","Train Epoch: 4 [67520/110534 (61%)]\tAll Loss: 1.8261\tTriple Loss(0): 0.0000\tClassification Loss: 1.8261\n","Train Epoch: 4 [67840/110534 (61%)]\tAll Loss: 2.3735\tTriple Loss(1): 0.2610\tClassification Loss: 1.8516\n","Train Epoch: 4 [68160/110534 (62%)]\tAll Loss: 1.4197\tTriple Loss(0): 0.0000\tClassification Loss: 1.4197\n","Train Epoch: 4 [68480/110534 (62%)]\tAll Loss: 2.1250\tTriple Loss(1): 0.3298\tClassification Loss: 1.4655\n","Train Epoch: 4 [68800/110534 (62%)]\tAll Loss: 1.8819\tTriple Loss(1): 0.2419\tClassification Loss: 1.3981\n","Train Epoch: 4 [69120/110534 (63%)]\tAll Loss: 1.9601\tTriple Loss(1): 0.1386\tClassification Loss: 1.6828\n","Train Epoch: 4 [69440/110534 (63%)]\tAll Loss: 1.4799\tTriple Loss(0): 0.0000\tClassification Loss: 1.4799\n","Train Epoch: 4 [69760/110534 (63%)]\tAll Loss: 1.9940\tTriple Loss(1): 0.2697\tClassification Loss: 1.4545\n","Train Epoch: 4 [70080/110534 (63%)]\tAll Loss: 1.8492\tTriple Loss(1): 0.1692\tClassification Loss: 1.5109\n","\n","Test set: Average loss: 1.5942, Accuracy: 563/960 (59%)\n","\n","Train Epoch: 4 [70400/110534 (64%)]\tAll Loss: 1.7876\tTriple Loss(1): 0.1457\tClassification Loss: 1.4963\n","Train Epoch: 4 [70720/110534 (64%)]\tAll Loss: 1.9347\tTriple Loss(1): 0.2729\tClassification Loss: 1.3889\n","Train Epoch: 4 [71040/110534 (64%)]\tAll Loss: 1.8869\tTriple Loss(1): 0.2414\tClassification Loss: 1.4041\n","Train Epoch: 4 [71360/110534 (65%)]\tAll Loss: 2.2123\tTriple Loss(1): 0.2217\tClassification Loss: 1.7688\n","Train Epoch: 4 [71680/110534 (65%)]\tAll Loss: 2.1073\tTriple Loss(1): 0.2413\tClassification Loss: 1.6246\n","Train Epoch: 4 [72000/110534 (65%)]\tAll Loss: 2.4729\tTriple Loss(1): 0.3830\tClassification Loss: 1.7069\n","Train Epoch: 4 [72320/110534 (65%)]\tAll Loss: 1.5412\tTriple Loss(0): 0.0000\tClassification Loss: 1.5412\n","Train Epoch: 4 [72640/110534 (66%)]\tAll Loss: 1.5324\tTriple Loss(1): 0.1142\tClassification Loss: 1.3040\n","Train Epoch: 4 [72960/110534 (66%)]\tAll Loss: 1.5052\tTriple Loss(0): 0.0000\tClassification Loss: 1.5052\n","Train Epoch: 4 [73280/110534 (66%)]\tAll Loss: 1.6882\tTriple Loss(1): 0.2256\tClassification Loss: 1.2370\n","\n","Test set: Average loss: 1.5875, Accuracy: 548/960 (57%)\n","\n","Train Epoch: 4 [73600/110534 (67%)]\tAll Loss: 2.4388\tTriple Loss(1): 0.1108\tClassification Loss: 2.2172\n","Train Epoch: 4 [73920/110534 (67%)]\tAll Loss: 2.2566\tTriple Loss(1): 0.2039\tClassification Loss: 1.8487\n","Train Epoch: 4 [74240/110534 (67%)]\tAll Loss: 2.0129\tTriple Loss(1): 0.3629\tClassification Loss: 1.2870\n","Train Epoch: 4 [74560/110534 (67%)]\tAll Loss: 1.9548\tTriple Loss(1): 0.1028\tClassification Loss: 1.7491\n","Train Epoch: 4 [74880/110534 (68%)]\tAll Loss: 1.8811\tTriple Loss(1): 0.1631\tClassification Loss: 1.5549\n","Train Epoch: 4 [75200/110534 (68%)]\tAll Loss: 1.4489\tTriple Loss(0): 0.0000\tClassification Loss: 1.4489\n","Train Epoch: 4 [75520/110534 (68%)]\tAll Loss: 1.9243\tTriple Loss(1): 0.1764\tClassification Loss: 1.5715\n","Train Epoch: 4 [75840/110534 (69%)]\tAll Loss: 2.3496\tTriple Loss(1): 0.4115\tClassification Loss: 1.5265\n","Train Epoch: 4 [76160/110534 (69%)]\tAll Loss: 1.9182\tTriple Loss(1): 0.1355\tClassification Loss: 1.6472\n","Train Epoch: 4 [76480/110534 (69%)]\tAll Loss: 1.6719\tTriple Loss(1): 0.1429\tClassification Loss: 1.3862\n","\n","Test set: Average loss: 1.5884, Accuracy: 552/960 (58%)\n","\n","Train Epoch: 4 [76800/110534 (69%)]\tAll Loss: 1.4688\tTriple Loss(0): 0.0000\tClassification Loss: 1.4688\n","Train Epoch: 4 [77120/110534 (70%)]\tAll Loss: 1.7325\tTriple Loss(1): 0.0892\tClassification Loss: 1.5541\n","Train Epoch: 4 [77440/110534 (70%)]\tAll Loss: 2.1859\tTriple Loss(1): 0.3290\tClassification Loss: 1.5278\n","Train Epoch: 4 [77760/110534 (70%)]\tAll Loss: 1.6206\tTriple Loss(0): 0.0000\tClassification Loss: 1.6206\n","Train Epoch: 4 [78080/110534 (71%)]\tAll Loss: 2.4688\tTriple Loss(1): 0.1314\tClassification Loss: 2.2060\n","Train Epoch: 4 [78400/110534 (71%)]\tAll Loss: 2.0627\tTriple Loss(1): 0.2360\tClassification Loss: 1.5907\n","Train Epoch: 4 [78720/110534 (71%)]\tAll Loss: 1.6236\tTriple Loss(1): 0.2676\tClassification Loss: 1.0885\n","Train Epoch: 4 [79040/110534 (71%)]\tAll Loss: 6.2384\tTriple Loss(0): 2.2640\tClassification Loss: 1.7103\n","Train Epoch: 4 [79360/110534 (72%)]\tAll Loss: 2.5946\tTriple Loss(1): 0.3844\tClassification Loss: 1.8257\n","Train Epoch: 4 [79680/110534 (72%)]\tAll Loss: 2.3705\tTriple Loss(1): 0.4390\tClassification Loss: 1.4924\n","\n","Test set: Average loss: 1.5919, Accuracy: 546/960 (57%)\n","\n","Train Epoch: 4 [80000/110534 (72%)]\tAll Loss: 1.6160\tTriple Loss(1): 0.1650\tClassification Loss: 1.2859\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_2500.pth.tar\n","Train Epoch: 4 [80320/110534 (73%)]\tAll Loss: 1.5677\tTriple Loss(1): 0.2212\tClassification Loss: 1.1252\n","Train Epoch: 4 [80640/110534 (73%)]\tAll Loss: 1.7319\tTriple Loss(1): 0.2649\tClassification Loss: 1.2021\n","Train Epoch: 4 [80960/110534 (73%)]\tAll Loss: 2.0456\tTriple Loss(1): 0.3152\tClassification Loss: 1.4152\n","Train Epoch: 4 [81280/110534 (74%)]\tAll Loss: 1.9366\tTriple Loss(1): 0.2964\tClassification Loss: 1.3437\n","Train Epoch: 4 [81600/110534 (74%)]\tAll Loss: 1.6872\tTriple Loss(1): 0.1420\tClassification Loss: 1.4032\n","Train Epoch: 4 [81920/110534 (74%)]\tAll Loss: 2.2540\tTriple Loss(1): 0.3059\tClassification Loss: 1.6423\n","Train Epoch: 4 [82240/110534 (74%)]\tAll Loss: 2.6418\tTriple Loss(1): 0.3864\tClassification Loss: 1.8690\n","Train Epoch: 4 [82560/110534 (75%)]\tAll Loss: 2.2861\tTriple Loss(1): 0.3289\tClassification Loss: 1.6283\n","Train Epoch: 4 [82880/110534 (75%)]\tAll Loss: 2.8237\tTriple Loss(1): 0.3248\tClassification Loss: 2.1741\n","\n","Test set: Average loss: 1.5875, Accuracy: 553/960 (58%)\n","\n","Train Epoch: 4 [83200/110534 (75%)]\tAll Loss: 1.8082\tTriple Loss(1): 0.1407\tClassification Loss: 1.5268\n","Train Epoch: 4 [83520/110534 (76%)]\tAll Loss: 1.5433\tTriple Loss(0): 0.0252\tClassification Loss: 1.4929\n","Train Epoch: 4 [83840/110534 (76%)]\tAll Loss: 1.3094\tTriple Loss(1): 0.0280\tClassification Loss: 1.2534\n","Train Epoch: 4 [84160/110534 (76%)]\tAll Loss: 1.7579\tTriple Loss(1): 0.3026\tClassification Loss: 1.1527\n","Train Epoch: 4 [84480/110534 (76%)]\tAll Loss: 1.6448\tTriple Loss(1): 0.1885\tClassification Loss: 1.2678\n","Train Epoch: 4 [84800/110534 (77%)]\tAll Loss: 2.3269\tTriple Loss(1): 0.3589\tClassification Loss: 1.6091\n","Train Epoch: 4 [85120/110534 (77%)]\tAll Loss: 1.4429\tTriple Loss(0): 0.0000\tClassification Loss: 1.4429\n","Train Epoch: 4 [85440/110534 (77%)]\tAll Loss: 1.8710\tTriple Loss(1): 0.3793\tClassification Loss: 1.1124\n","Train Epoch: 4 [85760/110534 (78%)]\tAll Loss: 1.6768\tTriple Loss(0): 0.0000\tClassification Loss: 1.6768\n","Train Epoch: 4 [86080/110534 (78%)]\tAll Loss: 1.9893\tTriple Loss(0): 0.0000\tClassification Loss: 1.9893\n","\n","Test set: Average loss: 1.5859, Accuracy: 559/960 (58%)\n","\n","Train Epoch: 4 [86400/110534 (78%)]\tAll Loss: 2.3442\tTriple Loss(1): 0.1932\tClassification Loss: 1.9577\n","Train Epoch: 4 [86720/110534 (78%)]\tAll Loss: 1.7570\tTriple Loss(1): 0.1755\tClassification Loss: 1.4060\n","Train Epoch: 4 [87040/110534 (79%)]\tAll Loss: 2.0247\tTriple Loss(1): 0.2774\tClassification Loss: 1.4700\n","Train Epoch: 4 [87360/110534 (79%)]\tAll Loss: 2.2441\tTriple Loss(1): 0.2272\tClassification Loss: 1.7898\n","Train Epoch: 4 [87680/110534 (79%)]\tAll Loss: 2.0789\tTriple Loss(1): 0.1759\tClassification Loss: 1.7271\n","Train Epoch: 4 [88000/110534 (80%)]\tAll Loss: 2.9043\tTriple Loss(1): 0.2816\tClassification Loss: 2.3411\n","Train Epoch: 4 [88320/110534 (80%)]\tAll Loss: 1.6383\tTriple Loss(1): 0.1182\tClassification Loss: 1.4020\n","Train Epoch: 4 [88640/110534 (80%)]\tAll Loss: 2.0327\tTriple Loss(0): 0.0000\tClassification Loss: 2.0327\n","Train Epoch: 4 [88960/110534 (80%)]\tAll Loss: 2.8804\tTriple Loss(1): 0.5624\tClassification Loss: 1.7556\n","Train Epoch: 4 [89280/110534 (81%)]\tAll Loss: 2.4753\tTriple Loss(1): 0.4196\tClassification Loss: 1.6360\n","\n","Test set: Average loss: 1.5858, Accuracy: 553/960 (58%)\n","\n","Train Epoch: 4 [89600/110534 (81%)]\tAll Loss: 2.2284\tTriple Loss(0): 0.0000\tClassification Loss: 2.2284\n","Train Epoch: 4 [89920/110534 (81%)]\tAll Loss: 2.3901\tTriple Loss(1): 0.3145\tClassification Loss: 1.7610\n","Train Epoch: 4 [90240/110534 (82%)]\tAll Loss: 2.0021\tTriple Loss(1): 0.3006\tClassification Loss: 1.4008\n","Train Epoch: 4 [90560/110534 (82%)]\tAll Loss: 1.6123\tTriple Loss(1): 0.1997\tClassification Loss: 1.2130\n","Train Epoch: 4 [90880/110534 (82%)]\tAll Loss: 2.2769\tTriple Loss(1): 0.3266\tClassification Loss: 1.6237\n","Train Epoch: 4 [91200/110534 (82%)]\tAll Loss: 1.7026\tTriple Loss(1): 0.1548\tClassification Loss: 1.3931\n","Train Epoch: 4 [91520/110534 (83%)]\tAll Loss: 2.6605\tTriple Loss(1): 0.5365\tClassification Loss: 1.5876\n","Train Epoch: 4 [91840/110534 (83%)]\tAll Loss: 1.4709\tTriple Loss(0): 0.0000\tClassification Loss: 1.4709\n","Train Epoch: 4 [92160/110534 (83%)]\tAll Loss: 2.6662\tTriple Loss(1): 0.3617\tClassification Loss: 1.9427\n","Train Epoch: 4 [92480/110534 (84%)]\tAll Loss: 1.7738\tTriple Loss(1): 0.0993\tClassification Loss: 1.5752\n","\n","Test set: Average loss: 1.5925, Accuracy: 553/960 (58%)\n","\n","Train Epoch: 4 [92800/110534 (84%)]\tAll Loss: 1.9677\tTriple Loss(1): 0.1546\tClassification Loss: 1.6585\n","Train Epoch: 4 [93120/110534 (84%)]\tAll Loss: 1.7432\tTriple Loss(0): 0.0000\tClassification Loss: 1.7432\n","Train Epoch: 4 [93440/110534 (85%)]\tAll Loss: 1.8013\tTriple Loss(1): 0.1865\tClassification Loss: 1.4282\n","Train Epoch: 4 [93760/110534 (85%)]\tAll Loss: 5.7776\tTriple Loss(0): 2.1513\tClassification Loss: 1.4751\n","Train Epoch: 4 [94080/110534 (85%)]\tAll Loss: 6.7598\tTriple Loss(0): 2.6934\tClassification Loss: 1.3730\n","Train Epoch: 4 [94400/110534 (85%)]\tAll Loss: 1.9653\tTriple Loss(1): 0.1789\tClassification Loss: 1.6075\n","Train Epoch: 4 [94720/110534 (86%)]\tAll Loss: 2.0206\tTriple Loss(1): 0.1857\tClassification Loss: 1.6493\n","Train Epoch: 4 [95040/110534 (86%)]\tAll Loss: 1.8099\tTriple Loss(1): 0.0704\tClassification Loss: 1.6692\n","Train Epoch: 4 [95360/110534 (86%)]\tAll Loss: 2.0932\tTriple Loss(1): 0.2776\tClassification Loss: 1.5380\n","Train Epoch: 4 [95680/110534 (87%)]\tAll Loss: 2.0644\tTriple Loss(1): 0.2134\tClassification Loss: 1.6375\n","\n","Test set: Average loss: 1.5857, Accuracy: 549/960 (57%)\n","\n","Train Epoch: 4 [96000/110534 (87%)]\tAll Loss: 1.9258\tTriple Loss(1): 0.2760\tClassification Loss: 1.3739\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_3000.pth.tar\n","Train Epoch: 4 [96320/110534 (87%)]\tAll Loss: 1.7938\tTriple Loss(1): 0.1750\tClassification Loss: 1.4438\n","Train Epoch: 4 [96640/110534 (87%)]\tAll Loss: 1.7434\tTriple Loss(0): 0.0000\tClassification Loss: 1.7434\n","Train Epoch: 4 [96960/110534 (88%)]\tAll Loss: 1.9789\tTriple Loss(1): 0.2907\tClassification Loss: 1.3976\n","Train Epoch: 4 [97280/110534 (88%)]\tAll Loss: 1.5676\tTriple Loss(1): 0.1746\tClassification Loss: 1.2184\n","Train Epoch: 4 [97600/110534 (88%)]\tAll Loss: 2.1359\tTriple Loss(1): 0.2092\tClassification Loss: 1.7174\n","Train Epoch: 4 [97920/110534 (89%)]\tAll Loss: 1.1293\tTriple Loss(0): 0.0000\tClassification Loss: 1.1293\n","Train Epoch: 4 [98240/110534 (89%)]\tAll Loss: 1.6372\tTriple Loss(1): 0.2127\tClassification Loss: 1.2119\n","Train Epoch: 4 [98560/110534 (89%)]\tAll Loss: 1.1898\tTriple Loss(0): 0.0000\tClassification Loss: 1.1898\n","Train Epoch: 4 [98880/110534 (89%)]\tAll Loss: 1.4673\tTriple Loss(0): 0.0000\tClassification Loss: 1.4673\n","\n","Test set: Average loss: 1.6001, Accuracy: 555/960 (58%)\n","\n","Train Epoch: 4 [99200/110534 (90%)]\tAll Loss: 1.9585\tTriple Loss(1): 0.2448\tClassification Loss: 1.4690\n","Train Epoch: 4 [99520/110534 (90%)]\tAll Loss: 1.8197\tTriple Loss(1): 0.1254\tClassification Loss: 1.5689\n","Train Epoch: 4 [99840/110534 (90%)]\tAll Loss: 2.0553\tTriple Loss(1): 0.1998\tClassification Loss: 1.6556\n","Train Epoch: 4 [100160/110534 (91%)]\tAll Loss: 4.3528\tTriple Loss(0): 1.2153\tClassification Loss: 1.9223\n","Train Epoch: 4 [100480/110534 (91%)]\tAll Loss: 2.0049\tTriple Loss(1): 0.2379\tClassification Loss: 1.5290\n","Train Epoch: 4 [100800/110534 (91%)]\tAll Loss: 3.4454\tTriple Loss(0): 0.8193\tClassification Loss: 1.8068\n","Train Epoch: 4 [101120/110534 (91%)]\tAll Loss: 1.8647\tTriple Loss(1): 0.2451\tClassification Loss: 1.3744\n","Train Epoch: 4 [101440/110534 (92%)]\tAll Loss: 2.0374\tTriple Loss(1): 0.3677\tClassification Loss: 1.3021\n","Train Epoch: 4 [101760/110534 (92%)]\tAll Loss: 1.7292\tTriple Loss(1): 0.0663\tClassification Loss: 1.5965\n","Train Epoch: 4 [102080/110534 (92%)]\tAll Loss: 2.8579\tTriple Loss(1): 0.3550\tClassification Loss: 2.1479\n","\n","Test set: Average loss: 1.5898, Accuracy: 548/960 (57%)\n","\n","Train Epoch: 4 [102400/110534 (93%)]\tAll Loss: 1.7546\tTriple Loss(1): 0.1591\tClassification Loss: 1.4363\n","Train Epoch: 4 [102720/110534 (93%)]\tAll Loss: 2.2690\tTriple Loss(1): 0.3175\tClassification Loss: 1.6341\n","Train Epoch: 4 [103040/110534 (93%)]\tAll Loss: 2.0679\tTriple Loss(1): 0.2930\tClassification Loss: 1.4818\n","Train Epoch: 4 [103360/110534 (93%)]\tAll Loss: 1.9058\tTriple Loss(0): 0.0000\tClassification Loss: 1.9058\n","Train Epoch: 4 [103680/110534 (94%)]\tAll Loss: 2.3772\tTriple Loss(1): 0.3019\tClassification Loss: 1.7735\n","Train Epoch: 4 [104000/110534 (94%)]\tAll Loss: 1.7711\tTriple Loss(1): 0.1266\tClassification Loss: 1.5179\n","Train Epoch: 4 [104320/110534 (94%)]\tAll Loss: 1.9167\tTriple Loss(0): 0.0000\tClassification Loss: 1.9167\n","Train Epoch: 4 [104640/110534 (95%)]\tAll Loss: 2.2616\tTriple Loss(1): 0.2138\tClassification Loss: 1.8341\n","Train Epoch: 4 [104960/110534 (95%)]\tAll Loss: 1.5719\tTriple Loss(0): 0.0000\tClassification Loss: 1.5719\n","Train Epoch: 4 [105280/110534 (95%)]\tAll Loss: 1.9879\tTriple Loss(1): 0.1800\tClassification Loss: 1.6280\n","\n","Test set: Average loss: 1.5918, Accuracy: 560/960 (58%)\n","\n","Train Epoch: 4 [105600/110534 (96%)]\tAll Loss: 2.0039\tTriple Loss(1): 0.1176\tClassification Loss: 1.7687\n","Train Epoch: 4 [105920/110534 (96%)]\tAll Loss: 1.7262\tTriple Loss(0): 0.0000\tClassification Loss: 1.7262\n","Train Epoch: 4 [106240/110534 (96%)]\tAll Loss: 1.6088\tTriple Loss(1): 0.1576\tClassification Loss: 1.2937\n","Train Epoch: 4 [106560/110534 (96%)]\tAll Loss: 1.8804\tTriple Loss(1): 0.1281\tClassification Loss: 1.6243\n","Train Epoch: 4 [106880/110534 (97%)]\tAll Loss: 2.9208\tTriple Loss(1): 0.4500\tClassification Loss: 2.0208\n","Train Epoch: 4 [107200/110534 (97%)]\tAll Loss: 1.8484\tTriple Loss(0): 0.0000\tClassification Loss: 1.8484\n","Train Epoch: 4 [107520/110534 (97%)]\tAll Loss: 1.9060\tTriple Loss(1): 0.1496\tClassification Loss: 1.6068\n","Train Epoch: 4 [107840/110534 (98%)]\tAll Loss: 1.5530\tTriple Loss(0): 0.0000\tClassification Loss: 1.5530\n","Train Epoch: 4 [108160/110534 (98%)]\tAll Loss: 1.5517\tTriple Loss(1): 0.1518\tClassification Loss: 1.2481\n","Train Epoch: 4 [108480/110534 (98%)]\tAll Loss: 1.8281\tTriple Loss(1): 0.0946\tClassification Loss: 1.6390\n","\n","Test set: Average loss: 1.5840, Accuracy: 557/960 (58%)\n","\n","Train Epoch: 4 [108800/110534 (98%)]\tAll Loss: 1.6886\tTriple Loss(0): 0.0000\tClassification Loss: 1.6886\n","Train Epoch: 4 [109120/110534 (99%)]\tAll Loss: 2.2969\tTriple Loss(1): 0.2016\tClassification Loss: 1.8937\n","Train Epoch: 4 [109440/110534 (99%)]\tAll Loss: 1.8607\tTriple Loss(1): 0.2129\tClassification Loss: 1.4349\n","Train Epoch: 4 [109760/110534 (99%)]\tAll Loss: 1.5134\tTriple Loss(0): 0.0000\tClassification Loss: 1.5134\n","Train Epoch: 4 [110080/110534 (100%)]\tAll Loss: 2.1339\tTriple Loss(1): 0.2114\tClassification Loss: 1.7110\n","Train Epoch: 4 [110400/110534 (100%)]\tAll Loss: 1.9492\tTriple Loss(1): 0.1944\tClassification Loss: 1.5605\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_4_final.pth.tar\n","\n","Test set: Average loss: 1.5863, Accuracy: 556/960 (58%)\n","\n","Train Epoch: 5 [0/110534 (0%)]\tAll Loss: 1.9346\tTriple Loss(1): 0.2073\tClassification Loss: 1.5200\n","Train Epoch: 5 [320/110534 (0%)]\tAll Loss: 2.3173\tTriple Loss(1): 0.4272\tClassification Loss: 1.4629\n","Train Epoch: 5 [640/110534 (1%)]\tAll Loss: 1.3295\tTriple Loss(1): 0.1510\tClassification Loss: 1.0275\n","Train Epoch: 5 [960/110534 (1%)]\tAll Loss: 2.3740\tTriple Loss(1): 0.2690\tClassification Loss: 1.8360\n","Train Epoch: 5 [1280/110534 (1%)]\tAll Loss: 1.6855\tTriple Loss(1): 0.1196\tClassification Loss: 1.4463\n","Train Epoch: 5 [1600/110534 (1%)]\tAll Loss: 2.0910\tTriple Loss(1): 0.1966\tClassification Loss: 1.6978\n","Train Epoch: 5 [1920/110534 (2%)]\tAll Loss: 1.8110\tTriple Loss(1): 0.0815\tClassification Loss: 1.6479\n","Train Epoch: 5 [2240/110534 (2%)]\tAll Loss: 1.8682\tTriple Loss(1): 0.1483\tClassification Loss: 1.5715\n","Train Epoch: 5 [2560/110534 (2%)]\tAll Loss: 1.8474\tTriple Loss(1): 0.1536\tClassification Loss: 1.5401\n","Train Epoch: 5 [2880/110534 (3%)]\tAll Loss: 2.1520\tTriple Loss(1): 0.0997\tClassification Loss: 1.9525\n","\n","Test set: Average loss: 1.5915, Accuracy: 557/960 (58%)\n","\n","Train Epoch: 5 [3200/110534 (3%)]\tAll Loss: 2.2608\tTriple Loss(1): 0.0764\tClassification Loss: 2.1080\n","Train Epoch: 5 [3520/110534 (3%)]\tAll Loss: 1.2783\tTriple Loss(0): 0.0000\tClassification Loss: 1.2783\n","Train Epoch: 5 [3840/110534 (3%)]\tAll Loss: 2.3105\tTriple Loss(1): 0.3141\tClassification Loss: 1.6823\n","Train Epoch: 5 [4160/110534 (4%)]\tAll Loss: 1.9516\tTriple Loss(1): 0.1785\tClassification Loss: 1.5946\n","Train Epoch: 5 [4480/110534 (4%)]\tAll Loss: 1.9282\tTriple Loss(1): 0.2533\tClassification Loss: 1.4215\n","Train Epoch: 5 [4800/110534 (4%)]\tAll Loss: 1.6674\tTriple Loss(1): 0.1065\tClassification Loss: 1.4543\n","Train Epoch: 5 [5120/110534 (5%)]\tAll Loss: 2.0875\tTriple Loss(1): 0.1981\tClassification Loss: 1.6913\n","Train Epoch: 5 [5440/110534 (5%)]\tAll Loss: 2.0870\tTriple Loss(1): 0.1956\tClassification Loss: 1.6959\n","Train Epoch: 5 [5760/110534 (5%)]\tAll Loss: 1.8451\tTriple Loss(1): 0.3835\tClassification Loss: 1.0781\n","Train Epoch: 5 [6080/110534 (5%)]\tAll Loss: 1.4283\tTriple Loss(0): 0.0000\tClassification Loss: 1.4283\n","\n","Test set: Average loss: 1.5851, Accuracy: 552/960 (58%)\n","\n","Train Epoch: 5 [6400/110534 (6%)]\tAll Loss: 2.6505\tTriple Loss(1): 0.4293\tClassification Loss: 1.7919\n","Train Epoch: 5 [6720/110534 (6%)]\tAll Loss: 1.6735\tTriple Loss(0): 0.0000\tClassification Loss: 1.6735\n","Train Epoch: 5 [7040/110534 (6%)]\tAll Loss: 2.4707\tTriple Loss(1): 0.2980\tClassification Loss: 1.8746\n","Train Epoch: 5 [7360/110534 (7%)]\tAll Loss: 2.0783\tTriple Loss(1): 0.3011\tClassification Loss: 1.4761\n","Train Epoch: 5 [7680/110534 (7%)]\tAll Loss: 1.7756\tTriple Loss(1): 0.2950\tClassification Loss: 1.1856\n","Train Epoch: 5 [8000/110534 (7%)]\tAll Loss: 1.5714\tTriple Loss(1): 0.1494\tClassification Loss: 1.2726\n","Train Epoch: 5 [8320/110534 (8%)]\tAll Loss: 2.2170\tTriple Loss(1): 0.3513\tClassification Loss: 1.5143\n","Train Epoch: 5 [8640/110534 (8%)]\tAll Loss: 1.6244\tTriple Loss(1): 0.1581\tClassification Loss: 1.3083\n","Train Epoch: 5 [8960/110534 (8%)]\tAll Loss: 13.5483\tTriple Loss(0): 6.0888\tClassification Loss: 1.3707\n","Train Epoch: 5 [9280/110534 (8%)]\tAll Loss: 1.2703\tTriple Loss(0): 0.0000\tClassification Loss: 1.2703\n","\n","Test set: Average loss: 1.5826, Accuracy: 551/960 (57%)\n","\n","Train Epoch: 5 [9600/110534 (9%)]\tAll Loss: 2.4587\tTriple Loss(1): 0.2710\tClassification Loss: 1.9167\n","Train Epoch: 5 [9920/110534 (9%)]\tAll Loss: 1.6368\tTriple Loss(1): 0.0935\tClassification Loss: 1.4497\n","Train Epoch: 5 [10240/110534 (9%)]\tAll Loss: 1.8395\tTriple Loss(0): 0.0000\tClassification Loss: 1.8395\n","Train Epoch: 5 [10560/110534 (10%)]\tAll Loss: 1.7452\tTriple Loss(0): 0.0000\tClassification Loss: 1.7452\n","Train Epoch: 5 [10880/110534 (10%)]\tAll Loss: 1.9553\tTriple Loss(1): 0.1227\tClassification Loss: 1.7098\n","Train Epoch: 5 [11200/110534 (10%)]\tAll Loss: 1.5803\tTriple Loss(1): 0.1332\tClassification Loss: 1.3139\n","Train Epoch: 5 [11520/110534 (10%)]\tAll Loss: 2.0895\tTriple Loss(1): 0.2813\tClassification Loss: 1.5270\n","Train Epoch: 5 [11840/110534 (11%)]\tAll Loss: 2.0519\tTriple Loss(1): 0.2374\tClassification Loss: 1.5772\n","Train Epoch: 5 [12160/110534 (11%)]\tAll Loss: 2.3497\tTriple Loss(1): 0.3123\tClassification Loss: 1.7251\n","Train Epoch: 5 [12480/110534 (11%)]\tAll Loss: 2.0096\tTriple Loss(1): 0.1812\tClassification Loss: 1.6472\n","\n","Test set: Average loss: 1.5827, Accuracy: 553/960 (58%)\n","\n","Train Epoch: 5 [12800/110534 (12%)]\tAll Loss: 1.4241\tTriple Loss(0): 0.0000\tClassification Loss: 1.4241\n","Train Epoch: 5 [13120/110534 (12%)]\tAll Loss: 1.3893\tTriple Loss(0): 0.0000\tClassification Loss: 1.3893\n","Train Epoch: 5 [13440/110534 (12%)]\tAll Loss: 2.5202\tTriple Loss(1): 0.1951\tClassification Loss: 2.1300\n","Train Epoch: 5 [13760/110534 (12%)]\tAll Loss: 3.0111\tTriple Loss(1): 0.6506\tClassification Loss: 1.7100\n","Train Epoch: 5 [14080/110534 (13%)]\tAll Loss: 1.9223\tTriple Loss(1): 0.1785\tClassification Loss: 1.5652\n","Train Epoch: 5 [14400/110534 (13%)]\tAll Loss: 2.2244\tTriple Loss(1): 0.0635\tClassification Loss: 2.0973\n","Train Epoch: 5 [14720/110534 (13%)]\tAll Loss: 2.0267\tTriple Loss(1): 0.1697\tClassification Loss: 1.6873\n","Train Epoch: 5 [15040/110534 (14%)]\tAll Loss: 1.7934\tTriple Loss(1): 0.1364\tClassification Loss: 1.5206\n","Train Epoch: 5 [15360/110534 (14%)]\tAll Loss: 1.7923\tTriple Loss(1): 0.1004\tClassification Loss: 1.5915\n","Train Epoch: 5 [15680/110534 (14%)]\tAll Loss: 1.8293\tTriple Loss(1): 0.2257\tClassification Loss: 1.3779\n","\n","Test set: Average loss: 1.5820, Accuracy: 551/960 (57%)\n","\n","Train Epoch: 5 [16000/110534 (14%)]\tAll Loss: 3.5146\tTriple Loss(0): 0.9210\tClassification Loss: 1.6725\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_5_500.pth.tar\n","Train Epoch: 5 [16320/110534 (15%)]\tAll Loss: 1.8803\tTriple Loss(1): 0.2743\tClassification Loss: 1.3317\n","Train Epoch: 5 [16640/110534 (15%)]\tAll Loss: 2.0335\tTriple Loss(1): 0.1485\tClassification Loss: 1.7366\n","Train Epoch: 5 [16960/110534 (15%)]\tAll Loss: 2.3462\tTriple Loss(1): 0.1616\tClassification Loss: 2.0230\n","Train Epoch: 5 [17280/110534 (16%)]\tAll Loss: 1.7425\tTriple Loss(1): 0.1579\tClassification Loss: 1.4266\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"WQN1zCcsuzua","colab_type":"code","outputId":"56702ff0-f0a8-40bb-f7f0-d2a4bba654e0","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# From scratch. FREEZE = False. LR=0.01\n","! python train.py"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n","  \"please use transforms.Resize instead.\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:704: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n","  \"please use transforms.RandomResizedCrop instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n","train.py:132: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(data, volatile=True), Variable(target)\n","\n","Test set: Average loss: 3.4774, Accuracy: 58/960 (6%)\n","\n","Train Epoch: 1 [0/110534 (0%)]\tAll Loss: 6.3927\tTriple Loss(0): 1.5768\tClassification Loss: 3.2391\n","Train Epoch: 1 [320/110534 (0%)]\tAll Loss: 7.2953\tTriple Loss(0): 1.1788\tClassification Loss: 4.9378\n","Train Epoch: 1 [640/110534 (1%)]\tAll Loss: 4.4997\tTriple Loss(1): 0.8896\tClassification Loss: 2.7204\n","Train Epoch: 1 [960/110534 (1%)]\tAll Loss: 2.4944\tTriple Loss(0): 0.0000\tClassification Loss: 2.4944\n","Train Epoch: 1 [1280/110534 (1%)]\tAll Loss: 4.5276\tTriple Loss(1): 0.9104\tClassification Loss: 2.7068\n","Train Epoch: 1 [1600/110534 (1%)]\tAll Loss: 4.2570\tTriple Loss(1): 0.7107\tClassification Loss: 2.8356\n","Train Epoch: 1 [1920/110534 (2%)]\tAll Loss: 5.3400\tTriple Loss(0): 1.5198\tClassification Loss: 2.3005\n","Train Epoch: 1 [2240/110534 (2%)]\tAll Loss: 2.5340\tTriple Loss(0): 0.0000\tClassification Loss: 2.5340\n","Train Epoch: 1 [2560/110534 (2%)]\tAll Loss: 4.3180\tTriple Loss(1): 0.9147\tClassification Loss: 2.4887\n","Train Epoch: 1 [2880/110534 (3%)]\tAll Loss: 4.2571\tTriple Loss(1): 0.9409\tClassification Loss: 2.3753\n","\n","Test set: Average loss: 2.6772, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [3200/110534 (3%)]\tAll Loss: 4.1243\tTriple Loss(1): 0.7547\tClassification Loss: 2.6149\n","Train Epoch: 1 [3520/110534 (3%)]\tAll Loss: 4.4523\tTriple Loss(1): 0.9022\tClassification Loss: 2.6479\n","Train Epoch: 1 [3840/110534 (3%)]\tAll Loss: 4.3290\tTriple Loss(1): 0.9194\tClassification Loss: 2.4903\n","Train Epoch: 1 [4160/110534 (4%)]\tAll Loss: 6.0292\tTriple Loss(0): 1.6875\tClassification Loss: 2.6541\n","Train Epoch: 1 [4480/110534 (4%)]\tAll Loss: 5.3994\tTriple Loss(0): 1.4073\tClassification Loss: 2.5848\n","Train Epoch: 1 [4800/110534 (4%)]\tAll Loss: 4.2473\tTriple Loss(1): 0.9426\tClassification Loss: 2.3621\n","Train Epoch: 1 [5120/110534 (5%)]\tAll Loss: 4.4257\tTriple Loss(1): 0.9199\tClassification Loss: 2.5860\n","Train Epoch: 1 [5440/110534 (5%)]\tAll Loss: 5.1151\tTriple Loss(1): 0.9369\tClassification Loss: 3.2413\n","Train Epoch: 1 [5760/110534 (5%)]\tAll Loss: 4.1016\tTriple Loss(1): 0.7977\tClassification Loss: 2.5062\n","Train Epoch: 1 [6080/110534 (5%)]\tAll Loss: 3.8709\tTriple Loss(1): 0.7892\tClassification Loss: 2.2924\n","\n","Test set: Average loss: 2.6767, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [6400/110534 (6%)]\tAll Loss: 4.0436\tTriple Loss(1): 0.7719\tClassification Loss: 2.4999\n","Train Epoch: 1 [6720/110534 (6%)]\tAll Loss: 3.9948\tTriple Loss(1): 0.7379\tClassification Loss: 2.5191\n","Train Epoch: 1 [7040/110534 (6%)]\tAll Loss: 3.9228\tTriple Loss(1): 0.8700\tClassification Loss: 2.1828\n","Train Epoch: 1 [7360/110534 (7%)]\tAll Loss: 4.3355\tTriple Loss(0): 0.8877\tClassification Loss: 2.5601\n","Train Epoch: 1 [7680/110534 (7%)]\tAll Loss: 4.0670\tTriple Loss(1): 0.8836\tClassification Loss: 2.2997\n","Train Epoch: 1 [8000/110534 (7%)]\tAll Loss: 4.2738\tTriple Loss(1): 0.9201\tClassification Loss: 2.4335\n","Train Epoch: 1 [8320/110534 (8%)]\tAll Loss: 4.7312\tTriple Loss(0): 1.0168\tClassification Loss: 2.6977\n","Train Epoch: 1 [8640/110534 (8%)]\tAll Loss: 4.3484\tTriple Loss(1): 0.9238\tClassification Loss: 2.5008\n","Train Epoch: 1 [8960/110534 (8%)]\tAll Loss: 4.0268\tTriple Loss(1): 0.7372\tClassification Loss: 2.5524\n","Train Epoch: 1 [9280/110534 (8%)]\tAll Loss: 7.4774\tTriple Loss(0): 2.4656\tClassification Loss: 2.5462\n","\n","Test set: Average loss: 2.8447, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [9600/110534 (9%)]\tAll Loss: 4.7577\tTriple Loss(1): 0.9924\tClassification Loss: 2.7729\n","Train Epoch: 1 [9920/110534 (9%)]\tAll Loss: 4.8479\tTriple Loss(0): 1.1232\tClassification Loss: 2.6015\n","Train Epoch: 1 [10240/110534 (9%)]\tAll Loss: 4.1653\tTriple Loss(1): 0.8588\tClassification Loss: 2.4477\n","Train Epoch: 1 [10560/110534 (10%)]\tAll Loss: 4.1695\tTriple Loss(1): 0.7784\tClassification Loss: 2.6127\n","Train Epoch: 1 [10880/110534 (10%)]\tAll Loss: 3.3668\tTriple Loss(1): 0.6423\tClassification Loss: 2.0822\n","Train Epoch: 1 [11200/110534 (10%)]\tAll Loss: 4.0505\tTriple Loss(1): 0.7542\tClassification Loss: 2.5421\n","Train Epoch: 1 [11520/110534 (10%)]\tAll Loss: 4.2915\tTriple Loss(1): 0.8644\tClassification Loss: 2.5628\n","Train Epoch: 1 [11840/110534 (11%)]\tAll Loss: 4.5104\tTriple Loss(1): 0.9290\tClassification Loss: 2.6525\n","Train Epoch: 1 [12160/110534 (11%)]\tAll Loss: 4.0403\tTriple Loss(1): 0.8604\tClassification Loss: 2.3194\n","Train Epoch: 1 [12480/110534 (11%)]\tAll Loss: 4.4361\tTriple Loss(1): 0.9970\tClassification Loss: 2.4421\n","\n","Test set: Average loss: 2.6686, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [12800/110534 (12%)]\tAll Loss: 4.2094\tTriple Loss(1): 0.8399\tClassification Loss: 2.5296\n","Train Epoch: 1 [13120/110534 (12%)]\tAll Loss: 4.4838\tTriple Loss(1): 1.0555\tClassification Loss: 2.3728\n","Train Epoch: 1 [13440/110534 (12%)]\tAll Loss: 4.5072\tTriple Loss(1): 0.9801\tClassification Loss: 2.5470\n","Train Epoch: 1 [13760/110534 (12%)]\tAll Loss: 3.4201\tTriple Loss(0): 0.4881\tClassification Loss: 2.4438\n","Train Epoch: 1 [14080/110534 (13%)]\tAll Loss: 4.2018\tTriple Loss(1): 0.8508\tClassification Loss: 2.5002\n","Train Epoch: 1 [14400/110534 (13%)]\tAll Loss: 4.2747\tTriple Loss(1): 0.9568\tClassification Loss: 2.3611\n","Train Epoch: 1 [14720/110534 (13%)]\tAll Loss: 8.5445\tTriple Loss(0): 3.1487\tClassification Loss: 2.2470\n","Train Epoch: 1 [15040/110534 (14%)]\tAll Loss: 4.1940\tTriple Loss(1): 0.9116\tClassification Loss: 2.3708\n","Train Epoch: 1 [15360/110534 (14%)]\tAll Loss: 4.3293\tTriple Loss(1): 0.8798\tClassification Loss: 2.5697\n","Train Epoch: 1 [15680/110534 (14%)]\tAll Loss: 4.8355\tTriple Loss(0): 1.0902\tClassification Loss: 2.6550\n","\n","Test set: Average loss: 2.6613, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [16000/110534 (14%)]\tAll Loss: 4.3279\tTriple Loss(1): 0.9446\tClassification Loss: 2.4386\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_500.pth.tar\n","Train Epoch: 1 [16320/110534 (15%)]\tAll Loss: 4.5907\tTriple Loss(1): 0.9076\tClassification Loss: 2.7754\n","Train Epoch: 1 [16640/110534 (15%)]\tAll Loss: 5.5341\tTriple Loss(0): 1.4708\tClassification Loss: 2.5926\n","Train Epoch: 1 [16960/110534 (15%)]\tAll Loss: 2.4883\tTriple Loss(0): 0.0000\tClassification Loss: 2.4883\n","Train Epoch: 1 [17280/110534 (16%)]\tAll Loss: 4.1963\tTriple Loss(1): 0.8973\tClassification Loss: 2.4017\n","Train Epoch: 1 [17600/110534 (16%)]\tAll Loss: 3.9172\tTriple Loss(1): 0.6945\tClassification Loss: 2.5282\n","Train Epoch: 1 [17920/110534 (16%)]\tAll Loss: 4.1890\tTriple Loss(1): 0.8385\tClassification Loss: 2.5119\n","Train Epoch: 1 [18240/110534 (16%)]\tAll Loss: 4.4191\tTriple Loss(1): 0.9389\tClassification Loss: 2.5413\n","Train Epoch: 1 [18560/110534 (17%)]\tAll Loss: 4.6381\tTriple Loss(1): 0.9491\tClassification Loss: 2.7399\n","Train Epoch: 1 [18880/110534 (17%)]\tAll Loss: 4.3851\tTriple Loss(1): 0.8349\tClassification Loss: 2.7153\n","\n","Test set: Average loss: 2.6528, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [19200/110534 (17%)]\tAll Loss: 4.4306\tTriple Loss(1): 0.9319\tClassification Loss: 2.5668\n","Train Epoch: 1 [19520/110534 (18%)]\tAll Loss: 2.5804\tTriple Loss(0): 0.0000\tClassification Loss: 2.5804\n","Train Epoch: 1 [19840/110534 (18%)]\tAll Loss: 4.1453\tTriple Loss(1): 0.8753\tClassification Loss: 2.3946\n","Train Epoch: 1 [20160/110534 (18%)]\tAll Loss: 4.0131\tTriple Loss(1): 0.8551\tClassification Loss: 2.3028\n","Train Epoch: 1 [20480/110534 (19%)]\tAll Loss: 4.2994\tTriple Loss(1): 0.8849\tClassification Loss: 2.5296\n","Train Epoch: 1 [20800/110534 (19%)]\tAll Loss: 4.4528\tTriple Loss(1): 0.9643\tClassification Loss: 2.5241\n","Train Epoch: 1 [21120/110534 (19%)]\tAll Loss: 4.5310\tTriple Loss(1): 0.8650\tClassification Loss: 2.8009\n","Train Epoch: 1 [21440/110534 (19%)]\tAll Loss: 4.4944\tTriple Loss(1): 0.9457\tClassification Loss: 2.6029\n","Train Epoch: 1 [21760/110534 (20%)]\tAll Loss: 4.1482\tTriple Loss(1): 0.7665\tClassification Loss: 2.6153\n","Train Epoch: 1 [22080/110534 (20%)]\tAll Loss: 4.1255\tTriple Loss(1): 0.7416\tClassification Loss: 2.6424\n","\n","Test set: Average loss: 2.6748, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [22400/110534 (20%)]\tAll Loss: 4.1520\tTriple Loss(1): 0.7861\tClassification Loss: 2.5798\n","Train Epoch: 1 [22720/110534 (21%)]\tAll Loss: 4.3277\tTriple Loss(1): 0.8814\tClassification Loss: 2.5649\n","Train Epoch: 1 [23040/110534 (21%)]\tAll Loss: 4.1481\tTriple Loss(1): 0.8958\tClassification Loss: 2.3566\n","Train Epoch: 1 [23360/110534 (21%)]\tAll Loss: 3.9202\tTriple Loss(1): 0.7689\tClassification Loss: 2.3824\n","Train Epoch: 1 [23680/110534 (21%)]\tAll Loss: 8.2987\tTriple Loss(0): 2.8458\tClassification Loss: 2.6071\n","Train Epoch: 1 [24000/110534 (22%)]\tAll Loss: 4.5714\tTriple Loss(1): 0.9867\tClassification Loss: 2.5980\n","Train Epoch: 1 [24320/110534 (22%)]\tAll Loss: 4.1334\tTriple Loss(1): 0.7681\tClassification Loss: 2.5972\n","Train Epoch: 1 [24640/110534 (22%)]\tAll Loss: 3.8031\tTriple Loss(1): 0.7846\tClassification Loss: 2.2338\n","Train Epoch: 1 [24960/110534 (23%)]\tAll Loss: 3.7348\tTriple Loss(1): 0.6410\tClassification Loss: 2.4527\n","Train Epoch: 1 [25280/110534 (23%)]\tAll Loss: 4.1063\tTriple Loss(1): 0.8097\tClassification Loss: 2.4869\n","\n","Test set: Average loss: 2.6601, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [25600/110534 (23%)]\tAll Loss: 3.9465\tTriple Loss(1): 0.7276\tClassification Loss: 2.4914\n","Train Epoch: 1 [25920/110534 (23%)]\tAll Loss: 5.7636\tTriple Loss(0): 1.5520\tClassification Loss: 2.6596\n","Train Epoch: 1 [26240/110534 (24%)]\tAll Loss: 3.8178\tTriple Loss(1): 0.8391\tClassification Loss: 2.1396\n","Train Epoch: 1 [26560/110534 (24%)]\tAll Loss: 3.7539\tTriple Loss(1): 0.5853\tClassification Loss: 2.5832\n","Train Epoch: 1 [26880/110534 (24%)]\tAll Loss: 3.8732\tTriple Loss(1): 0.6777\tClassification Loss: 2.5177\n","Train Epoch: 1 [27200/110534 (25%)]\tAll Loss: 4.8000\tTriple Loss(0): 1.1752\tClassification Loss: 2.4496\n","Train Epoch: 1 [27520/110534 (25%)]\tAll Loss: 4.1132\tTriple Loss(1): 0.7470\tClassification Loss: 2.6192\n","Train Epoch: 1 [27840/110534 (25%)]\tAll Loss: 4.1018\tTriple Loss(1): 0.8068\tClassification Loss: 2.4883\n","Train Epoch: 1 [28160/110534 (25%)]\tAll Loss: 7.0002\tTriple Loss(0): 2.2984\tClassification Loss: 2.4034\n","Train Epoch: 1 [28480/110534 (26%)]\tAll Loss: 3.9736\tTriple Loss(1): 0.7858\tClassification Loss: 2.4020\n","\n","Test set: Average loss: 2.6639, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [28800/110534 (26%)]\tAll Loss: 3.7584\tTriple Loss(1): 0.6495\tClassification Loss: 2.4594\n","Train Epoch: 1 [29120/110534 (26%)]\tAll Loss: 4.0681\tTriple Loss(1): 0.6573\tClassification Loss: 2.7535\n","Train Epoch: 1 [29440/110534 (27%)]\tAll Loss: 4.4222\tTriple Loss(1): 0.8457\tClassification Loss: 2.7307\n","Train Epoch: 1 [29760/110534 (27%)]\tAll Loss: 4.9471\tTriple Loss(0): 0.9854\tClassification Loss: 2.9762\n","Train Epoch: 1 [30080/110534 (27%)]\tAll Loss: 3.7203\tTriple Loss(1): 0.6304\tClassification Loss: 2.4595\n","Train Epoch: 1 [30400/110534 (27%)]\tAll Loss: 3.6259\tTriple Loss(1): 0.5765\tClassification Loss: 2.4729\n","Train Epoch: 1 [30720/110534 (28%)]\tAll Loss: 4.2980\tTriple Loss(1): 0.8256\tClassification Loss: 2.6469\n","Train Epoch: 1 [31040/110534 (28%)]\tAll Loss: 3.6839\tTriple Loss(1): 0.6456\tClassification Loss: 2.3928\n","Train Epoch: 1 [31360/110534 (28%)]\tAll Loss: 4.0375\tTriple Loss(1): 0.8145\tClassification Loss: 2.4084\n","Train Epoch: 1 [31680/110534 (29%)]\tAll Loss: 4.4286\tTriple Loss(1): 0.8084\tClassification Loss: 2.8117\n","\n","Test set: Average loss: 2.6668, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [32000/110534 (29%)]\tAll Loss: 3.9235\tTriple Loss(1): 0.6228\tClassification Loss: 2.6779\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_1000.pth.tar\n","Train Epoch: 1 [32320/110534 (29%)]\tAll Loss: 4.0697\tTriple Loss(1): 0.8481\tClassification Loss: 2.3736\n","Train Epoch: 1 [32640/110534 (30%)]\tAll Loss: 3.7210\tTriple Loss(1): 0.6839\tClassification Loss: 2.3531\n","Train Epoch: 1 [32960/110534 (30%)]\tAll Loss: 3.4258\tTriple Loss(0): 0.4476\tClassification Loss: 2.5306\n","Train Epoch: 1 [33280/110534 (30%)]\tAll Loss: 4.2453\tTriple Loss(1): 0.8276\tClassification Loss: 2.5901\n","Train Epoch: 1 [33600/110534 (30%)]\tAll Loss: 3.7739\tTriple Loss(1): 0.7161\tClassification Loss: 2.3417\n","Train Epoch: 1 [33920/110534 (31%)]\tAll Loss: 3.8801\tTriple Loss(1): 0.6531\tClassification Loss: 2.5739\n","Train Epoch: 1 [34240/110534 (31%)]\tAll Loss: 3.6513\tTriple Loss(1): 0.7075\tClassification Loss: 2.2363\n","Train Epoch: 1 [34560/110534 (31%)]\tAll Loss: 3.6162\tTriple Loss(0): 0.5754\tClassification Loss: 2.4654\n","Train Epoch: 1 [34880/110534 (32%)]\tAll Loss: 4.5402\tTriple Loss(0): 0.9832\tClassification Loss: 2.5738\n","\n","Test set: Average loss: 2.6561, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [35200/110534 (32%)]\tAll Loss: 3.9922\tTriple Loss(1): 0.6541\tClassification Loss: 2.6840\n","Train Epoch: 1 [35520/110534 (32%)]\tAll Loss: 3.9051\tTriple Loss(0): 0.7399\tClassification Loss: 2.4252\n","Train Epoch: 1 [35840/110534 (32%)]\tAll Loss: 4.0492\tTriple Loss(1): 0.8603\tClassification Loss: 2.3287\n","Train Epoch: 1 [36160/110534 (33%)]\tAll Loss: 4.4272\tTriple Loss(0): 0.9735\tClassification Loss: 2.4803\n","Train Epoch: 1 [36480/110534 (33%)]\tAll Loss: 4.1439\tTriple Loss(1): 0.8121\tClassification Loss: 2.5197\n","Train Epoch: 1 [36800/110534 (33%)]\tAll Loss: 2.7120\tTriple Loss(0): 0.0000\tClassification Loss: 2.7120\n","Train Epoch: 1 [37120/110534 (34%)]\tAll Loss: 4.2137\tTriple Loss(1): 0.7058\tClassification Loss: 2.8021\n","Train Epoch: 1 [37440/110534 (34%)]\tAll Loss: 4.2286\tTriple Loss(1): 0.7150\tClassification Loss: 2.7986\n","Train Epoch: 1 [37760/110534 (34%)]\tAll Loss: 3.9963\tTriple Loss(1): 0.6717\tClassification Loss: 2.6529\n","Train Epoch: 1 [38080/110534 (34%)]\tAll Loss: 4.2253\tTriple Loss(1): 0.8379\tClassification Loss: 2.5496\n","\n","Test set: Average loss: 2.6629, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [38400/110534 (35%)]\tAll Loss: 3.7411\tTriple Loss(1): 0.6992\tClassification Loss: 2.3426\n","Train Epoch: 1 [38720/110534 (35%)]\tAll Loss: 3.3764\tTriple Loss(1): 0.4097\tClassification Loss: 2.5570\n","Train Epoch: 1 [39040/110534 (35%)]\tAll Loss: 4.1509\tTriple Loss(1): 0.8638\tClassification Loss: 2.4232\n","Train Epoch: 1 [39360/110534 (36%)]\tAll Loss: 4.2628\tTriple Loss(1): 0.9046\tClassification Loss: 2.4536\n","Train Epoch: 1 [39680/110534 (36%)]\tAll Loss: 4.2391\tTriple Loss(1): 0.7547\tClassification Loss: 2.7298\n","Train Epoch: 1 [40000/110534 (36%)]\tAll Loss: 2.6321\tTriple Loss(0): 0.0000\tClassification Loss: 2.6321\n","Train Epoch: 1 [40320/110534 (36%)]\tAll Loss: 3.8283\tTriple Loss(1): 0.7154\tClassification Loss: 2.3975\n","Train Epoch: 1 [40640/110534 (37%)]\tAll Loss: 3.8307\tTriple Loss(1): 0.7573\tClassification Loss: 2.3162\n","Train Epoch: 1 [40960/110534 (37%)]\tAll Loss: 3.9246\tTriple Loss(1): 0.7688\tClassification Loss: 2.3869\n","Train Epoch: 1 [41280/110534 (37%)]\tAll Loss: 4.4439\tTriple Loss(1): 0.8794\tClassification Loss: 2.6850\n","\n","Test set: Average loss: 2.7437, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [41600/110534 (38%)]\tAll Loss: 3.8282\tTriple Loss(1): 0.7124\tClassification Loss: 2.4033\n","Train Epoch: 1 [41920/110534 (38%)]\tAll Loss: 3.9216\tTriple Loss(1): 0.7975\tClassification Loss: 2.3266\n","Train Epoch: 1 [42240/110534 (38%)]\tAll Loss: 3.9059\tTriple Loss(1): 0.7218\tClassification Loss: 2.4622\n","Train Epoch: 1 [42560/110534 (38%)]\tAll Loss: 4.2621\tTriple Loss(1): 0.8165\tClassification Loss: 2.6290\n","Train Epoch: 1 [42880/110534 (39%)]\tAll Loss: 3.9384\tTriple Loss(1): 0.6800\tClassification Loss: 2.5784\n","Train Epoch: 1 [43200/110534 (39%)]\tAll Loss: 4.0537\tTriple Loss(1): 0.7590\tClassification Loss: 2.5357\n","Train Epoch: 1 [43520/110534 (39%)]\tAll Loss: 3.8508\tTriple Loss(1): 0.6374\tClassification Loss: 2.5761\n","Train Epoch: 1 [43840/110534 (40%)]\tAll Loss: 3.9722\tTriple Loss(1): 0.7587\tClassification Loss: 2.4548\n","Train Epoch: 1 [44160/110534 (40%)]\tAll Loss: 4.4477\tTriple Loss(1): 0.9011\tClassification Loss: 2.6455\n","Train Epoch: 1 [44480/110534 (40%)]\tAll Loss: 3.9055\tTriple Loss(1): 0.8302\tClassification Loss: 2.2450\n","\n","Test set: Average loss: 2.6481, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [44800/110534 (41%)]\tAll Loss: 4.1332\tTriple Loss(1): 0.7882\tClassification Loss: 2.5567\n","Train Epoch: 1 [45120/110534 (41%)]\tAll Loss: 3.7593\tTriple Loss(1): 0.7328\tClassification Loss: 2.2937\n","Train Epoch: 1 [45440/110534 (41%)]\tAll Loss: 3.8391\tTriple Loss(1): 0.7869\tClassification Loss: 2.2652\n","Train Epoch: 1 [45760/110534 (41%)]\tAll Loss: 4.1812\tTriple Loss(1): 0.8330\tClassification Loss: 2.5152\n","Train Epoch: 1 [46080/110534 (42%)]\tAll Loss: 4.0878\tTriple Loss(1): 0.8500\tClassification Loss: 2.3877\n","Train Epoch: 1 [46400/110534 (42%)]\tAll Loss: 4.2643\tTriple Loss(1): 0.9003\tClassification Loss: 2.4637\n","Train Epoch: 1 [46720/110534 (42%)]\tAll Loss: 3.5854\tTriple Loss(1): 0.7438\tClassification Loss: 2.0979\n","Train Epoch: 1 [47040/110534 (43%)]\tAll Loss: 3.7833\tTriple Loss(1): 0.6576\tClassification Loss: 2.4681\n","Train Epoch: 1 [47360/110534 (43%)]\tAll Loss: 3.9693\tTriple Loss(1): 0.8942\tClassification Loss: 2.1810\n","Train Epoch: 1 [47680/110534 (43%)]\tAll Loss: 3.7576\tTriple Loss(1): 0.5797\tClassification Loss: 2.5983\n","\n","Test set: Average loss: 2.6315, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [48000/110534 (43%)]\tAll Loss: 8.5209\tTriple Loss(0): 3.1098\tClassification Loss: 2.3013\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_1500.pth.tar\n","Train Epoch: 1 [48320/110534 (44%)]\tAll Loss: 4.0806\tTriple Loss(1): 0.7510\tClassification Loss: 2.5787\n","Train Epoch: 1 [48640/110534 (44%)]\tAll Loss: 3.6743\tTriple Loss(1): 0.6843\tClassification Loss: 2.3057\n","Train Epoch: 1 [48960/110534 (44%)]\tAll Loss: 4.1766\tTriple Loss(1): 0.9917\tClassification Loss: 2.1933\n","Train Epoch: 1 [49280/110534 (45%)]\tAll Loss: 4.1863\tTriple Loss(1): 0.8764\tClassification Loss: 2.4335\n","Train Epoch: 1 [49600/110534 (45%)]\tAll Loss: 4.3330\tTriple Loss(1): 0.7780\tClassification Loss: 2.7771\n","Train Epoch: 1 [49920/110534 (45%)]\tAll Loss: 4.0618\tTriple Loss(1): 0.7741\tClassification Loss: 2.5135\n","Train Epoch: 1 [50240/110534 (45%)]\tAll Loss: 3.6110\tTriple Loss(1): 0.5645\tClassification Loss: 2.4819\n","Train Epoch: 1 [50560/110534 (46%)]\tAll Loss: 4.0616\tTriple Loss(1): 0.8380\tClassification Loss: 2.3856\n","Train Epoch: 1 [50880/110534 (46%)]\tAll Loss: 4.4979\tTriple Loss(0): 0.9976\tClassification Loss: 2.5027\n","\n","Test set: Average loss: 2.6499, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [51200/110534 (46%)]\tAll Loss: 4.2981\tTriple Loss(1): 0.8312\tClassification Loss: 2.6357\n","Train Epoch: 1 [51520/110534 (47%)]\tAll Loss: 12.6927\tTriple Loss(0): 5.1161\tClassification Loss: 2.4605\n","Train Epoch: 1 [51840/110534 (47%)]\tAll Loss: 4.2003\tTriple Loss(1): 0.8325\tClassification Loss: 2.5353\n","Train Epoch: 1 [52160/110534 (47%)]\tAll Loss: 3.9495\tTriple Loss(1): 0.7075\tClassification Loss: 2.5345\n","Train Epoch: 1 [52480/110534 (47%)]\tAll Loss: 4.1363\tTriple Loss(1): 0.8198\tClassification Loss: 2.4968\n","Train Epoch: 1 [52800/110534 (48%)]\tAll Loss: 3.8626\tTriple Loss(1): 0.7561\tClassification Loss: 2.3505\n","Train Epoch: 1 [53120/110534 (48%)]\tAll Loss: 4.5070\tTriple Loss(1): 1.0835\tClassification Loss: 2.3401\n","Train Epoch: 1 [53440/110534 (48%)]\tAll Loss: 3.9245\tTriple Loss(1): 0.7964\tClassification Loss: 2.3317\n","Train Epoch: 1 [53760/110534 (49%)]\tAll Loss: 3.6909\tTriple Loss(1): 0.5869\tClassification Loss: 2.5171\n","Train Epoch: 1 [54080/110534 (49%)]\tAll Loss: 3.9717\tTriple Loss(1): 0.8395\tClassification Loss: 2.2927\n","\n","Test set: Average loss: 2.7334, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [54400/110534 (49%)]\tAll Loss: 5.2474\tTriple Loss(0): 1.4444\tClassification Loss: 2.3587\n","Train Epoch: 1 [54720/110534 (49%)]\tAll Loss: 4.4407\tTriple Loss(1): 0.8215\tClassification Loss: 2.7978\n","Train Epoch: 1 [55040/110534 (50%)]\tAll Loss: 3.7474\tTriple Loss(1): 0.6906\tClassification Loss: 2.3662\n","Train Epoch: 1 [55360/110534 (50%)]\tAll Loss: 4.1838\tTriple Loss(0): 0.7159\tClassification Loss: 2.7520\n","Train Epoch: 1 [55680/110534 (50%)]\tAll Loss: 2.4238\tTriple Loss(0): 0.0000\tClassification Loss: 2.4238\n","Train Epoch: 1 [56000/110534 (51%)]\tAll Loss: 3.2547\tTriple Loss(1): 0.4922\tClassification Loss: 2.2703\n","Train Epoch: 1 [56320/110534 (51%)]\tAll Loss: 4.0683\tTriple Loss(1): 0.8153\tClassification Loss: 2.4377\n","Train Epoch: 1 [56640/110534 (51%)]\tAll Loss: 4.3793\tTriple Loss(1): 0.8783\tClassification Loss: 2.6226\n","Train Epoch: 1 [56960/110534 (52%)]\tAll Loss: 4.1905\tTriple Loss(1): 0.7679\tClassification Loss: 2.6548\n","Train Epoch: 1 [57280/110534 (52%)]\tAll Loss: 3.9424\tTriple Loss(1): 0.8441\tClassification Loss: 2.2543\n","\n","Test set: Average loss: 2.6730, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [57600/110534 (52%)]\tAll Loss: 3.8567\tTriple Loss(1): 0.7495\tClassification Loss: 2.3577\n","Train Epoch: 1 [57920/110534 (52%)]\tAll Loss: 3.5804\tTriple Loss(1): 0.6242\tClassification Loss: 2.3320\n","Train Epoch: 1 [58240/110534 (53%)]\tAll Loss: 6.8750\tTriple Loss(0): 2.1449\tClassification Loss: 2.5852\n","Train Epoch: 1 [58560/110534 (53%)]\tAll Loss: 6.0188\tTriple Loss(0): 1.8324\tClassification Loss: 2.3540\n","Train Epoch: 1 [58880/110534 (53%)]\tAll Loss: 3.9976\tTriple Loss(1): 0.8488\tClassification Loss: 2.2999\n","Train Epoch: 1 [59200/110534 (54%)]\tAll Loss: 4.1748\tTriple Loss(1): 0.7894\tClassification Loss: 2.5960\n","Train Epoch: 1 [59520/110534 (54%)]\tAll Loss: 3.9409\tTriple Loss(1): 0.7467\tClassification Loss: 2.4474\n","Train Epoch: 1 [59840/110534 (54%)]\tAll Loss: 4.1466\tTriple Loss(1): 0.7673\tClassification Loss: 2.6120\n","Train Epoch: 1 [60160/110534 (54%)]\tAll Loss: 9.7092\tTriple Loss(0): 3.5342\tClassification Loss: 2.6408\n","Train Epoch: 1 [60480/110534 (55%)]\tAll Loss: 4.5505\tTriple Loss(1): 0.9712\tClassification Loss: 2.6081\n","\n","Test set: Average loss: 2.7275, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [60800/110534 (55%)]\tAll Loss: 4.0273\tTriple Loss(1): 0.6688\tClassification Loss: 2.6897\n","Train Epoch: 1 [61120/110534 (55%)]\tAll Loss: 2.5612\tTriple Loss(0): 0.0000\tClassification Loss: 2.5612\n","Train Epoch: 1 [61440/110534 (56%)]\tAll Loss: 6.3996\tTriple Loss(0): 1.9380\tClassification Loss: 2.5237\n","Train Epoch: 1 [61760/110534 (56%)]\tAll Loss: 3.9192\tTriple Loss(1): 0.5812\tClassification Loss: 2.7568\n","Train Epoch: 1 [62080/110534 (56%)]\tAll Loss: 4.1631\tTriple Loss(1): 0.8311\tClassification Loss: 2.5008\n","Train Epoch: 1 [62400/110534 (56%)]\tAll Loss: 3.9093\tTriple Loss(1): 0.6345\tClassification Loss: 2.6403\n","Train Epoch: 1 [62720/110534 (57%)]\tAll Loss: 3.7834\tTriple Loss(1): 0.6801\tClassification Loss: 2.4232\n","Train Epoch: 1 [63040/110534 (57%)]\tAll Loss: 7.7268\tTriple Loss(0): 2.5676\tClassification Loss: 2.5916\n","Train Epoch: 1 [63360/110534 (57%)]\tAll Loss: 3.9846\tTriple Loss(1): 0.7991\tClassification Loss: 2.3863\n","Train Epoch: 1 [63680/110534 (58%)]\tAll Loss: 6.2936\tTriple Loss(0): 2.0286\tClassification Loss: 2.2364\n","\n","Test set: Average loss: 2.6635, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [64000/110534 (58%)]\tAll Loss: 3.9434\tTriple Loss(1): 0.7060\tClassification Loss: 2.5315\n","Model saved to //content/drive/My Drive/Deep Fashion Retrieval/base/models/model_1_2000.pth.tar\n","Train Epoch: 1 [64320/110534 (58%)]\tAll Loss: 4.6048\tTriple Loss(0): 1.0081\tClassification Loss: 2.5886\n","Train Epoch: 1 [64640/110534 (58%)]\tAll Loss: 3.8410\tTriple Loss(1): 0.7692\tClassification Loss: 2.3027\n","Train Epoch: 1 [64960/110534 (59%)]\tAll Loss: 4.0947\tTriple Loss(1): 0.7699\tClassification Loss: 2.5548\n","Train Epoch: 1 [65280/110534 (59%)]\tAll Loss: 3.8123\tTriple Loss(1): 0.6383\tClassification Loss: 2.5357\n","Train Epoch: 1 [65600/110534 (59%)]\tAll Loss: 6.1475\tTriple Loss(0): 1.8844\tClassification Loss: 2.3786\n","Train Epoch: 1 [65920/110534 (60%)]\tAll Loss: 4.0878\tTriple Loss(1): 0.8280\tClassification Loss: 2.4319\n","Train Epoch: 1 [66240/110534 (60%)]\tAll Loss: 3.9971\tTriple Loss(1): 0.8480\tClassification Loss: 2.3010\n","Train Epoch: 1 [66560/110534 (60%)]\tAll Loss: 4.1392\tTriple Loss(1): 0.9196\tClassification Loss: 2.3000\n","Train Epoch: 1 [66880/110534 (60%)]\tAll Loss: 3.9952\tTriple Loss(1): 0.6688\tClassification Loss: 2.6576\n","\n","Test set: Average loss: 2.7002, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [67200/110534 (61%)]\tAll Loss: 4.0636\tTriple Loss(1): 0.8800\tClassification Loss: 2.3036\n","Train Epoch: 1 [67520/110534 (61%)]\tAll Loss: 4.3451\tTriple Loss(1): 0.8504\tClassification Loss: 2.6442\n","Train Epoch: 1 [67840/110534 (61%)]\tAll Loss: 3.9375\tTriple Loss(1): 0.6755\tClassification Loss: 2.5865\n","Train Epoch: 1 [68160/110534 (62%)]\tAll Loss: 4.3661\tTriple Loss(1): 0.8514\tClassification Loss: 2.6634\n","Train Epoch: 1 [68480/110534 (62%)]\tAll Loss: 4.2320\tTriple Loss(1): 0.8666\tClassification Loss: 2.4987\n","Train Epoch: 1 [68800/110534 (62%)]\tAll Loss: 3.7889\tTriple Loss(1): 0.7018\tClassification Loss: 2.3853\n","Train Epoch: 1 [69120/110534 (63%)]\tAll Loss: 4.2493\tTriple Loss(1): 0.8332\tClassification Loss: 2.5828\n","Train Epoch: 1 [69440/110534 (63%)]\tAll Loss: 4.3882\tTriple Loss(1): 0.9122\tClassification Loss: 2.5638\n","Train Epoch: 1 [69760/110534 (63%)]\tAll Loss: 4.1909\tTriple Loss(1): 0.7531\tClassification Loss: 2.6846\n","Train Epoch: 1 [70080/110534 (63%)]\tAll Loss: 3.6834\tTriple Loss(1): 0.7582\tClassification Loss: 2.1669\n","\n","Test set: Average loss: 2.6748, Accuracy: 244/960 (25%)\n","\n","Train Epoch: 1 [70400/110534 (64%)]\tAll Loss: 3.7322\tTriple Loss(1): 0.8063\tClassification Loss: 2.1195\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"TEDtAX9gj2Tq","colab_type":"text"},"source":["# Rough"]},{"cell_type":"code","metadata":{"id":"VQMr5jgwNBsT","colab_type":"code","colab":{}},"source":["! python train.py"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c4w2MEm5U8R3","colab_type":"code","outputId":"d781e6f7-ce04-4faf-eabe-11dfc63f1f84","executionInfo":{"status":"ok","timestamp":1586704220059,"user_tz":-300,"elapsed":726830,"user":{"displayName":"Muhammad Ali","photoUrl":"","userId":"15673831022739340207"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["from config import *\n","from plotcm import plot_confusion_matrix\n","from train import get_conf_matrix\n","import matplotlib.pyplot as plt\n","\n","names = ['Anorak', 'Blazer', 'Bomber', 'Button-Down', 'Cardigan', \n","              'Flannel', 'Henley', 'Hoodie', 'Jacket', 'Jersey', 'Parka', \n","              'Peacoat', 'Sweater', 'Tank', 'Tee', 'Top', 'Turtleneck', \n","              'Chinos','Jeans', 'Joggers', 'Shorts', 'Sweatpants', \n","              'Sweatshorts', 'Trunks', 'Coat', 'Robe']\n","\n","cm = get_conf_matrix()\n","print(cm.shape)\n","\n","# plt.figure(figsize=(10,10))\n","# plot_confusion_matrix(cm, names)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n","  \"please use transforms.Resize instead.\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n","  \"please use transforms.Resize instead.\")\n","/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:704: UserWarning: The use of the transforms.RandomSizedCrop transform is deprecated, please use transforms.RandomResizedCrop instead.\n","  \"please use transforms.RandomResizedCrop instead.\")\n"],"name":"stderr"},{"output_type":"stream","text":["Loading model model_5_1500.pth.tar\n","dataset: <data.Fashion_attr_prediction object at 0x7fcee6ff0f28>\n","dataset len: 42305\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["batch_idx: 0\n"],"name":"stdout"},{"output_type":"stream","text":["/content/drive/My Drive/Deep Fashion Retrieval/deep-fashion-retrieval/train.py:171: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n","  data, target = Variable(data, volatile=True), Variable(target)\n"],"name":"stderr"},{"output_type":"stream","text":["batch_idx: 5\n","batch_idx: 10\n","batch_idx: 15\n","batch_idx: 20\n","batch_idx: 25\n","batch_idx: 30\n","batch_idx: 35\n","batch_idx: 40\n","batch_idx: 45\n","batch_idx: 50\n","batch_idx: 55\n","batch_idx: 60\n","batch_idx: 65\n","batch_idx: 70\n","batch_idx: 75\n","batch_idx: 80\n","batch_idx: 85\n","batch_idx: 90\n","batch_idx: 95\n","batch_idx: 100\n","batch_idx: 105\n","batch_idx: 110\n","batch_idx: 115\n","batch_idx: 120\n","batch_idx: 125\n","batch_idx: 130\n","batch_idx: 135\n","batch_idx: 140\n","batch_idx: 145\n","batch_idx: 150\n","batch_idx: 155\n","batch_idx: 160\n","batch_idx: 165\n","batch_idx: 170\n","batch_idx: 175\n","batch_idx: 180\n","batch_idx: 185\n","batch_idx: 190\n","batch_idx: 195\n","batch_idx: 200\n","batch_idx: 205\n","batch_idx: 210\n","batch_idx: 215\n","batch_idx: 220\n","batch_idx: 225\n","batch_idx: 230\n","batch_idx: 235\n","batch_idx: 240\n","batch_idx: 245\n","batch_idx: 250\n","batch_idx: 255\n","batch_idx: 260\n","batch_idx: 265\n","batch_idx: 270\n","batch_idx: 275\n","batch_idx: 280\n","batch_idx: 285\n","batch_idx: 290\n","batch_idx: 295\n","batch_idx: 300\n","batch_idx: 305\n","batch_idx: 310\n","batch_idx: 315\n","batch_idx: 320\n","batch_idx: 325\n","batch_idx: 330\n","batch_idx: 335\n","batch_idx: 340\n","batch_idx: 345\n","batch_idx: 350\n","batch_idx: 355\n","batch_idx: 360\n","batch_idx: 365\n","batch_idx: 370\n","batch_idx: 375\n","batch_idx: 380\n","batch_idx: 385\n","batch_idx: 390\n","batch_idx: 395\n","batch_idx: 400\n","batch_idx: 405\n","batch_idx: 410\n","batch_idx: 415\n","batch_idx: 420\n","batch_idx: 425\n","batch_idx: 430\n","batch_idx: 435\n","batch_idx: 440\n","batch_idx: 445\n","batch_idx: 450\n","batch_idx: 455\n","batch_idx: 460\n","batch_idx: 465\n","batch_idx: 470\n","batch_idx: 475\n","batch_idx: 480\n","batch_idx: 485\n","batch_idx: 490\n","batch_idx: 495\n","batch_idx: 500\n","batch_idx: 505\n","batch_idx: 510\n","batch_idx: 515\n","batch_idx: 520\n","batch_idx: 525\n","batch_idx: 530\n","batch_idx: 535\n","batch_idx: 540\n","batch_idx: 545\n","batch_idx: 550\n","batch_idx: 555\n","batch_idx: 560\n","batch_idx: 565\n","batch_idx: 570\n","batch_idx: 575\n","batch_idx: 580\n","batch_idx: 585\n","batch_idx: 590\n","batch_idx: 595\n","batch_idx: 600\n","batch_idx: 605\n","batch_idx: 610\n","batch_idx: 615\n","batch_idx: 620\n","batch_idx: 625\n","batch_idx: 630\n","batch_idx: 635\n","batch_idx: 640\n","batch_idx: 645\n","batch_idx: 650\n","batch_idx: 655\n","batch_idx: 660\n","batch_idx: 665\n","batch_idx: 670\n","batch_idx: 675\n","batch_idx: 680\n","batch_idx: 685\n","batch_idx: 690\n","batch_idx: 695\n","batch_idx: 700\n","batch_idx: 705\n","batch_idx: 710\n","batch_idx: 715\n","batch_idx: 720\n","batch_idx: 725\n","batch_idx: 730\n","batch_idx: 735\n","batch_idx: 740\n","batch_idx: 745\n","batch_idx: 750\n","batch_idx: 755\n","batch_idx: 760\n","batch_idx: 765\n","batch_idx: 770\n","batch_idx: 775\n","batch_idx: 780\n","batch_idx: 785\n","batch_idx: 790\n","batch_idx: 795\n","batch_idx: 800\n","batch_idx: 805\n","batch_idx: 810\n","batch_idx: 815\n","batch_idx: 820\n","batch_idx: 825\n","batch_idx: 830\n","batch_idx: 835\n","batch_idx: 840\n","batch_idx: 845\n","batch_idx: 850\n","batch_idx: 855\n","batch_idx: 860\n","batch_idx: 865\n","batch_idx: 870\n","batch_idx: 875\n","batch_idx: 880\n","batch_idx: 885\n","batch_idx: 890\n","batch_idx: 895\n","batch_idx: 900\n","batch_idx: 905\n","batch_idx: 910\n","batch_idx: 915\n","batch_idx: 920\n","batch_idx: 925\n","batch_idx: 930\n","batch_idx: 935\n","batch_idx: 940\n","batch_idx: 945\n","batch_idx: 950\n","batch_idx: 955\n","batch_idx: 960\n","batch_idx: 965\n","batch_idx: 970\n","batch_idx: 975\n","batch_idx: 980\n","batch_idx: 985\n","batch_idx: 990\n","batch_idx: 995\n","batch_idx: 1000\n","batch_idx: 1005\n","batch_idx: 1010\n","batch_idx: 1015\n","batch_idx: 1020\n","batch_idx: 1025\n","batch_idx: 1030\n","batch_idx: 1035\n","batch_idx: 1040\n","batch_idx: 1045\n","batch_idx: 1050\n","batch_idx: 1055\n","batch_idx: 1060\n","batch_idx: 1065\n","batch_idx: 1070\n","batch_idx: 1075\n","batch_idx: 1080\n","batch_idx: 1085\n","batch_idx: 1090\n","batch_idx: 1095\n","batch_idx: 1100\n","batch_idx: 1105\n","batch_idx: 1110\n","batch_idx: 1115\n","batch_idx: 1120\n","batch_idx: 1125\n","batch_idx: 1130\n","batch_idx: 1135\n","batch_idx: 1140\n","batch_idx: 1145\n","batch_idx: 1150\n","batch_idx: 1155\n","batch_idx: 1160\n","batch_idx: 1165\n","batch_idx: 1170\n","batch_idx: 1175\n","batch_idx: 1180\n","batch_idx: 1185\n","batch_idx: 1190\n","batch_idx: 1195\n","batch_idx: 1200\n","batch_idx: 1205\n","batch_idx: 1210\n","batch_idx: 1215\n","batch_idx: 1220\n","batch_idx: 1225\n","batch_idx: 1230\n","batch_idx: 1235\n","batch_idx: 1240\n","batch_idx: 1245\n","batch_idx: 1250\n","batch_idx: 1255\n","batch_idx: 1260\n","batch_idx: 1265\n","batch_idx: 1270\n","batch_idx: 1275\n","batch_idx: 1280\n","batch_idx: 1285\n","batch_idx: 1290\n","batch_idx: 1295\n","batch_idx: 1300\n","batch_idx: 1305\n","batch_idx: 1310\n","batch_idx: 1315\n","batch_idx: 1320\n","batch_idx: 1325\n","batch_idx: 1330\n","batch_idx: 1335\n","batch_idx: 1340\n","batch_idx: 1345\n","batch_idx: 1350\n","batch_idx: 1355\n","batch_idx: 1360\n","batch_idx: 1365\n","batch_idx: 1370\n","batch_idx: 1375\n","batch_idx: 1380\n","batch_idx: 1385\n","batch_idx: 1390\n","batch_idx: 1395\n","batch_idx: 1400\n","batch_idx: 1405\n","batch_idx: 1410\n","batch_idx: 1415\n","batch_idx: 1420\n","batch_idx: 1425\n","batch_idx: 1430\n","batch_idx: 1435\n","batch_idx: 1440\n","batch_idx: 1445\n","batch_idx: 1450\n","batch_idx: 1455\n","batch_idx: 1460\n","batch_idx: 1465\n","batch_idx: 1470\n","batch_idx: 1475\n","batch_idx: 1480\n","batch_idx: 1485\n","batch_idx: 1490\n","batch_idx: 1495\n","batch_idx: 1500\n","batch_idx: 1505\n","batch_idx: 1510\n","batch_idx: 1515\n","batch_idx: 1520\n","batch_idx: 1525\n","batch_idx: 1530\n","batch_idx: 1535\n","batch_idx: 1540\n","batch_idx: 1545\n","batch_idx: 1550\n","batch_idx: 1555\n","batch_idx: 1560\n","batch_idx: 1565\n","batch_idx: 1570\n","batch_idx: 1575\n","batch_idx: 1580\n","batch_idx: 1585\n","batch_idx: 1590\n","batch_idx: 1595\n","batch_idx: 1600\n","batch_idx: 1605\n","batch_idx: 1610\n","batch_idx: 1615\n","batch_idx: 1620\n","batch_idx: 1625\n","batch_idx: 1630\n","batch_idx: 1635\n","batch_idx: 1640\n","batch_idx: 1645\n","batch_idx: 1650\n","batch_idx: 1655\n","batch_idx: 1660\n","batch_idx: 1665\n","batch_idx: 1670\n","batch_idx: 1675\n","batch_idx: 1680\n","batch_idx: 1685\n","batch_idx: 1690\n","batch_idx: 1695\n","batch_idx: 1700\n","batch_idx: 1705\n","batch_idx: 1710\n","batch_idx: 1715\n","batch_idx: 1720\n","batch_idx: 1725\n","batch_idx: 1730\n","batch_idx: 1735\n","batch_idx: 1740\n","batch_idx: 1745\n","batch_idx: 1750\n","batch_idx: 1755\n","batch_idx: 1760\n","batch_idx: 1765\n","batch_idx: 1770\n","batch_idx: 1775\n","batch_idx: 1780\n","batch_idx: 1785\n","batch_idx: 1790\n","batch_idx: 1795\n","batch_idx: 1800\n","batch_idx: 1805\n","batch_idx: 1810\n","batch_idx: 1815\n","batch_idx: 1820\n","batch_idx: 1825\n","batch_idx: 1830\n","batch_idx: 1835\n","batch_idx: 1840\n","batch_idx: 1845\n","batch_idx: 1850\n","batch_idx: 1855\n","batch_idx: 1860\n","batch_idx: 1865\n","batch_idx: 1870\n","batch_idx: 1875\n","batch_idx: 1880\n","batch_idx: 1885\n","batch_idx: 1890\n","batch_idx: 1895\n","batch_idx: 1900\n","batch_idx: 1905\n","batch_idx: 1910\n","batch_idx: 1915\n","batch_idx: 1920\n","batch_idx: 1925\n","batch_idx: 1930\n","batch_idx: 1935\n","batch_idx: 1940\n","batch_idx: 1945\n","batch_idx: 1950\n","batch_idx: 1955\n","batch_idx: 1960\n","batch_idx: 1965\n","batch_idx: 1970\n","batch_idx: 1975\n","batch_idx: 1980\n","batch_idx: 1985\n","batch_idx: 1990\n","batch_idx: 1995\n","batch_idx: 2000\n","batch_idx: 2005\n","batch_idx: 2010\n","batch_idx: 2015\n","batch_idx: 2020\n","batch_idx: 2025\n","batch_idx: 2030\n","batch_idx: 2035\n","batch_idx: 2040\n","batch_idx: 2045\n","batch_idx: 2050\n","batch_idx: 2055\n","batch_idx: 2060\n","batch_idx: 2065\n","batch_idx: 2070\n","batch_idx: 2075\n","batch_idx: 2080\n","batch_idx: 2085\n","batch_idx: 2090\n","batch_idx: 2095\n","batch_idx: 2100\n","batch_idx: 2105\n","batch_idx: 2110\n","batch_idx: 2115\n","batch_idx: 2120\n","batch_idx: 2125\n","batch_idx: 2130\n","batch_idx: 2135\n","batch_idx: 2140\n","batch_idx: 2145\n","batch_idx: 2150\n","batch_idx: 2155\n","batch_idx: 2160\n","batch_idx: 2165\n","batch_idx: 2170\n","batch_idx: 2175\n","batch_idx: 2180\n","batch_idx: 2185\n","batch_idx: 2190\n","batch_idx: 2195\n","batch_idx: 2200\n","batch_idx: 2205\n","batch_idx: 2210\n","batch_idx: 2215\n","batch_idx: 2220\n","batch_idx: 2225\n","batch_idx: 2230\n","batch_idx: 2235\n","batch_idx: 2240\n","batch_idx: 2245\n","batch_idx: 2250\n","batch_idx: 2255\n","batch_idx: 2260\n","batch_idx: 2265\n","batch_idx: 2270\n","batch_idx: 2275\n","batch_idx: 2280\n","batch_idx: 2285\n","batch_idx: 2290\n","batch_idx: 2295\n","batch_idx: 2300\n","batch_idx: 2305\n","batch_idx: 2310\n","batch_idx: 2315\n","batch_idx: 2320\n","batch_idx: 2325\n","batch_idx: 2330\n","batch_idx: 2335\n","batch_idx: 2340\n","batch_idx: 2345\n","batch_idx: 2350\n","batch_idx: 2355\n","batch_idx: 2360\n","batch_idx: 2365\n","batch_idx: 2370\n","batch_idx: 2375\n","batch_idx: 2380\n","batch_idx: 2385\n","batch_idx: 2390\n","batch_idx: 2395\n","batch_idx: 2400\n","batch_idx: 2405\n","batch_idx: 2410\n","batch_idx: 2415\n","batch_idx: 2420\n","batch_idx: 2425\n","batch_idx: 2430\n","batch_idx: 2435\n","batch_idx: 2440\n","batch_idx: 2445\n","batch_idx: 2450\n","batch_idx: 2455\n","batch_idx: 2460\n","batch_idx: 2465\n","batch_idx: 2470\n","batch_idx: 2475\n","batch_idx: 2480\n","batch_idx: 2485\n","batch_idx: 2490\n","batch_idx: 2495\n","batch_idx: 2500\n","batch_idx: 2505\n","batch_idx: 2510\n","batch_idx: 2515\n","batch_idx: 2520\n","batch_idx: 2525\n","batch_idx: 2530\n","batch_idx: 2535\n","batch_idx: 2540\n","batch_idx: 2545\n","batch_idx: 2550\n","batch_idx: 2555\n","batch_idx: 2560\n","batch_idx: 2565\n","batch_idx: 2570\n","batch_idx: 2575\n","batch_idx: 2580\n","batch_idx: 2585\n","batch_idx: 2590\n","batch_idx: 2595\n","batch_idx: 2600\n","batch_idx: 2605\n","batch_idx: 2610\n","batch_idx: 2615\n","batch_idx: 2620\n","batch_idx: 2625\n","batch_idx: 2630\n","batch_idx: 2635\n","batch_idx: 2640\n","\n","Test set: Average loss: 1.4276, Accuracy: 22872/42320 (54%)\n","\n","(26, 26)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HLtMI6lYCGip","colab_type":"code","outputId":"4a176329-4b70-43c8-d737-a215fc2d3bbb","executionInfo":{"status":"ok","timestamp":1586706743938,"user_tz":-300,"elapsed":4738,"user":{"displayName":"Muhammad Ali","photoUrl":"","userId":"15673831022739340207"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["plt.figure(figsize=(12,12))\n","plot_confusion_matrix(cm, names)\n","plt.savefig('conf_matrix.png')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Confusion matrix, without normalization\n","[[   0    0    0    0    1    0    0    1   35    0    0    0    0    0\n","     1    0    0    0    0    0    0    0    0    0    1    0]\n"," [   0  590    0    0  294    1    0    0  691    0    0    0   19   37\n","   317   15    0    0   26    6   79    0    0    0   12    0]\n"," [   0    1    0    0    3    0    0    0   64    0    0    0    0    0\n","     8    0    0    0    1    0    2    0    0    0    1    0]\n"," [   0    0    0    3    5    2    0    0   21    0    0    0    2    4\n","    39    1    0    0    2    0    6    0    0    0    2    0]\n"," [   0   73    0    0 1756    0    0    5  521    0    0    0  261   76\n","   772   50    0    0   31   18  128    1    0    0   13    0]\n"," [   0    0    0    2    8    5    0    1   10    0    0    0    3    1\n","    58    3    0    0    0    2    7    0    0    0    0    0]\n"," [   0    5    0    0    9    0    0    8   15    0    0    0    9    1\n","   138    1    0    0    2    4    3    0    0    0    0    0]\n"," [   0    2    0    0   89    0    0  188  211    0    0    0   72   24\n","   506    6    0    0    6    9   25    0    0    0    0    0]\n"," [   0  147    0    0  322    0    0   19 1792    0    3    0   21   35\n","   391   29    0    0   37   16   73    0    0    0   34    0]\n"," [   0    1    0    0    2    0    0    1   14    0    0    0    0   20\n","   174    0    0    0    0    2    0    0    0    0    0    0]\n"," [   0    2    0    0   11    0    0    0  136    0    6    0    2    0\n","     5    0    0    0    1    0    3    0    0    0   19    0]\n"," [   0    4    0    0    3    0    0    0   19    0    0    0    0    0\n","     2    0    0    0    0    0    1    0    0    0    5    0]\n"," [   0   19    0    0  671    0    0   15  196    0    0    0 1140   81\n","  1308   42    0    0   33    9   89    0    0    0    3    0]\n"," [   0    8    0    0   46    0    0    1   36    0    0    0   18 2499\n","  1295   40    0    0   20   14  246    0    0    0    2    0]\n"," [   0   22    0    3  160    0    0   17  222    0    0    0  111  665\n","  8501   64    0    0   75   37  353    0    0    0    4    0]\n"," [   0   11    0    0  144    0    0    1  102    1    0    0   73  553\n","  1492  124    0    0   21   37  246    1    0    0    2    0]\n"," [   0    0    0    0    7    0    0    1    1    0    0    0   14    1\n","    17    2    0    0    0    0    4    0    0    0    0    0]\n"," [   0    0    0    0    1    0    0    0    2    0    0    0    0    1\n","    11    0    0   19   29   65   24    0    0    0    1    0]\n"," [   0    1    0    0   22    0    0    0   61    0    0    0   10   13\n","   208    2    0    2 1368  204   54    5    0    0    0    0]\n"," [   0    5    0    0   23    0    0    0   40    0    0    0    4   14\n","   170    6    0    5  184  617   79    9    0    0    0    0]\n"," [   0    9    0    0   49    0    0    0   94    0    0    0   22  295\n","   772   21    0    1   77   86 4037    4    0    2    2    0]\n"," [   0    3    0    0   13    0    0    1   19    0    0    0    2    6\n","   130    0    0    8  106  419   62   55    0    0    0    0]\n"," [   0    0    0    0    8    0    0    0    4    0    0    0    4    2\n","    61    0    0    0    6   69  163    6    2    0    0    0]\n"," [   0    0    0    0    0    0    0    0    0    0    0    0    0    1\n","     5    0    0    0    0    0   87    0    0    6    0    0]\n"," [   0   28    0    0   90    0    0    0  251    0    1    0    2    2\n","    30    2    0    0    2    1    8    0    0    0  164    0]\n"," [   0    1    0    1   10    0    0    0    5    0    0    0    0    5\n","    12    5    0    0    0    0    3    0    0    0    1    0]]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x864 with 2 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"16Q2YK_uS-jI","colab_type":"code","colab":{}},"source":["os.path.exists('//content/drive/My Drive/Deep Fashion Retrieval/base/img/Leaf_Print-Sleeve_Tee/img_00000031.jpg')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BftcVBtT4Rn5","colab_type":"code","colab":{}},"source":["function ClickConnect(){\n","console.log(\"Working\"); \n","document.querySelector(\"colab-toolbar-button#connect\").click() \n","}\n","setInterval(ClickConnect,60000)"],"execution_count":0,"outputs":[]}]}